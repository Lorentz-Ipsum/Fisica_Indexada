\documentclass{article}
\usepackage{yhmath}
\usepackage[utf8]{inputenc}
\usepackage{ upgreek }
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{ amssymb }
\usepackage{ tipa }
\usepackage{svg}
\usepackage{amsmath}
\setlength{\parindent}{0cm}
\usepackage[spanish]{babel}
\usepackage{vmargin}
\usepackage{subfig}
\usepackage{cancel}
\usepackage{ dsfont }
\newcommand{\rn}[1][n]{\mathds{R}^{#1}}
\usepackage{pdfpages}
\usepackage{braket}
\usepackage{ gensymb }
\usepackage{wrapfig}
\usepackage{subcaption}
\newcommand{\commentedbox}[2]{%
  \mbox{
    \begin{tabular}[t]{@{}c@{}}
    $\boxed{\displaystyle#1}$\\
    #2
    \end{tabular}%
  }%
}
\usepackage{scalerel,stackengine}
\stackMath
\newcommand\reallywidehat[1]{%
\savestack{\tmpbox}{\stretchto{%
  \scaleto{%
    \scalerel*[\widthof{\ensuremath{#1}}]{\kern-.6pt\bigwedge\kern-.6pt}%
    {\rule[-\textheight/2]{1ex}{\textheight}}%WIDTH-LIMITED BIG WEDGE
  }{\textheight}%
}{0.5ex}}%
\stackon[1pt]{#1}{\tmpbox}%
}
\title{Notas de simetrías y teoría de grupos}
\author{Álvaro Santiago Serrano }
\date{Septiembre de 2019}

\begin{document}

\maketitle

\section{Introducción a las simetrías en física}
\subsection{La importancia de las simetrías en física}

Llamamos simetrías de un sistema físico a aquellas propiedades del sistema que se preservan bajo alguna transformación. Las simetrías van de la mano a leyes de conservación (ver teorema de Noether) de las ecuaciones que describen estos sistemas.

Hasta el siglo XX se estudiaban las simetrías mediante las ecuaciones de los sistemas pero a partir de Einstein y la revolución que supuso la relatividad se cambia el foco y nos empezamos a fijar directamente en las simetrías (y no en las ecuaciones), derivando más tarde las ecuaciones de los sistemas asociadas a tales simetrías.

\subsection{Relación entre simetría y teoría de grupos}

La herramienta fundamental para el estudio de las simetrías es la teoría de grupos. Las transformaciones matemáticas asociadas a cada simetría poseen las propiedades de los grupos algebraicos. Por tanto, para el estudio de las simetrías es necesario conocer teoría de grupos.

\smallskip
Así, por ejemplo, las transformaciones de simetría de la relatividad especial conforman el grupo de Poincaré (traslaciones espacio-temporales, rotaciones espaciales y rotaciones espacio-temporales).

\subsection{Tipos de simetrías}

\subsubsection{Locales y globales}
Decimos que una simetría es local si las transformaciones de simetría cambian de punto a punto del espacio-tiempo ($\mathcal{X}(\Vec{x},t)$). Las globales por tanto son aquellas invariantes punto a punto. Son importantes en teorías gauge como la electromagnétcia.

\subsubsection{Discretas y continuas}

Una simetría es discreta si su ley de transformación no es continua (como las simetrías de los cristales o las rotaciones de los poliedros), un ejemplo de gran importancia es la simetría CPT de la cuántica de campos. Las simetrías continuas son aquellas que sí pueden ser descritas por una transformación continua (por ejemplo la simetría de rotación del círculo).

\smallskip
En dinámica clásica las consecuencias de las simetrías continuas se hacen evidentes a través del principio de mínima acción. Una simetría de un sistema clásico deja invariante la acción bajo su aplicación sobre la misma y por tanto no modifica las ecuaciones del movimiento.

\subsection{Simetría en mecánica cuántica}

En mecánica cuántica las transformaciones de simetría son lineales. El principio de superposición  provoca que las simetrías tengan profundas consecuencias.

Sea $\ket{\uppsi}$ el estado del sistema y $\mathcal{R}$ un operador lineal unitario que representa una transformación.

$$\mathcal{R}:\ket{\uppsi}\longrightarrow \mathcal{R}\ket{\uppsi}\in \mathcal{H}$$

El estado $\mathcal{R}\ket{\uppsi}$ representa también un estado del sistema si $\mathcal{R}$ es una transformación unitaria (es decir que preserva la norma). Por ejemplo, una representación del singlete de rotaciones sería, si $\mathcal{R}=R$ representa el operador rotación $\ket{\phi}$ el estado del sistema y $\ket{\uppsi}$ un estado que puede ser mezcla:

$$\ket{\phi} = \sum _R R\ket{\uppsi} \hspace{1cm} \text{Invariancia bajo rotaciones}$$

$$R\ket{\phi}=\sum _{R'}R'R\ket{\uppsi} \hspace{1cm} \text{Como R es grupo, RR'=R'' -otra rotación-}$$

$$R\ket{\phi}= \sum _{R''}R''\ket{\uppsi}=\sum _R R\ket{\uppsi}=\ket{\phi} \hspace{1cm} \text{El estado $\ket{\phi}$ es invariante bajo rotaciones}$$

La teoría de representaciones de grupos discretos y continuos es muy importante en mecánica cuántica para derivar las consecuencias de las simetrías. Las leyes de selección que gobiernan los espectros atómicos son, por ejemplo, consecuencia de la simetría de rotaciones.

Las simetrías también ayudaron a predecir la existencia de los fermiones y los bosones, partículas con distinta transformación frente al cambio de partículas idénticas.

\subsection{Transformaciones globales}
La aplicación de una transformacion de simetría global da lugar a una situación física diferente en la que las observaciones son invariantes bajo dichas transformaciones.

$$\Vec{x}\longrightarrow \Vec{x}+\Vec{v}t$$

Siendo $\Vec{v}t\neq \Vec{v}t(\Vec{x})$ una transformación global que no depende del punto del e-t.


\subsection{Simetrías gauge}
La aplicación de la transformación gauge solo cambia la descripción de una misma situación física. La primera vez que aparece es en electrodinámica, donde los campos EM pueden presentarse en un potencial cuadrivector $A_\mu=(\phi, \Vec{A})$ que se transforma según:

$$A_\mu \longrightarrow A_\mu +\partial _\mu \phi (x)$$

Donde $\partial _\mu \phi (x)=\frac{\partial \phi (x)}{\partial x_\mu}$.

Transformación que deja completamente invariantes los campos $\Vec{E}$ y $\Vec{B}$. A partir de los años 70 las teorías gauge van a tener una posición central en el estudio de las teorías fundamentales de la naturaleza. Son la base del modelo estándar de partículas, afectando por ejemplo a la dinámica y fuerza de partículas o forzando la existencia de otras tales como el famoso bosón de Higgs (se dice que la simetría dicta la interacción).

\newpage
\subsection{Ejemplo de aplicación de las simetrías a un caso sencillo}

Sea una partícula en una red periódica unidimensional, su hamiltoniano es del tipo $H=\frac{p^2}{2m}+\phi (x)$ con $\phi (x)$ un potencial periódico con periodicidad $\phi (x)=\phi (x+nb) \hspace{0.1cm} \forall n\in \mathcal{Z}$.

\smallskip
$\bullet$ Por tener esta periodicidad existe simetría de traslación en la red (pues tanto la parte cinética como la potencial tienen la simetría).

\begin{equation}
    x\longrightarrow x+nb
    \label{traslacion}
\end{equation}

\smallskip
$\bullet$ Dos sistemas relacionados por (\ref{traslacion}) tienen el mismo comportamiento.

\smallskip
$\bullet$ Dos observadores relacionados por (\ref{traslacion}) ven las mismas propiedades del sistema (punto de vista activo/pasivo).

En mecánica cuántica tenemos un estado $\ket{\uppsi}$ en $\mathcal{H}=L^2(\mathds{R},dx)$ que se traslada según el operador unitario $\mathcal{T}=T$ que cumple:

$$\mathcal{T}: \ket{\uppsi}\longrightarrow T\ket{\uppsi}=\ket{\uppsi '}$$

$\mathcal{T}$ es una transformación lineal que deja \textbf{invariantes los observables}. Es decir, si $\mathcal{O}$ es un observable:

$$O\ket{\uppsi}=O\ket{\uppsi '}$$

Dado que los obserbables se expresan como $\braket{\phi|\uppsi}$, el operador T debe preservar este producto escalar. T debe ser un operador \textbf{unitario autoadjunto}.

El conjunto de simetrías de la red está representado por el conjunto de operadores unitarios en $\mathcal{H}$. Es decir, forma una representación de transformaciones de simetría del hamiltoniano. Vemos pues que deben transformar también los observables. Hallemos el valor esperado del operador unitario A:

$$\braket{\phi |A|\uppsi}=\braket{\phi |T^{-1}TAT^{-1}T|\uppsi}=\braket{\phi '|TAT^{-1}|\uppsi '}=\braket{\phi '|A'|\uppsi'}$$

Luego vemos que bajo la acción de T:

$$A\longrightarrow TAT^{-1}=A'$$

Volviendo ahora a nuestro sistema, sea $\ket{x}$ un estado del sistema en representación de posición, el operador traslación:

$$T\ket{x}=\ket{x+nb}=\ket{x'}$$

Es evidente ver que H es invariante bajo traslación:

$$THT^{-1}=T\left( \frac{P^2}{2m}+\phi (x)\right)T^{-1}=T \frac{P^2}{2m}T^{-1}+\phi (x+nb)$$

Dado que $\phi (x)=\phi (x+nb)$ solo debemos comprobar que el operador momento lineal en mecánica cuántica se traslade adecuadamente. Sea $P^2=\partial ^2_x\frac{1}{2m}$ la expresión de este operador en la base de posiciones, tenemos:

$$T\frac{\partial ^2}{\partial x^2}\frac{1}{2m}T^{-1}\ket{x}=T\frac{\partial ^2}{\partial x^2}\frac{1}{2m}\ket{x-nb}=\frac{T}{2m}\left ( \frac{\partial ^2}{\partial x^2}\ket{x-nb}\right)$$

Dado que $\frac{\partial ^2}{\partial x^2}\ket{x}=\frac{\partial ^2}{\partial x^2}\ket{x-nb}$, decimos que la parte cinética es invariante bajo traslaciones [H,T]=0 y la base de posiciones que hemos utilizado sería una buena base de autoestados para el hamiltoniano.

\newpage
\subsubsection{Representación de operadores de traslación discreta}
La simetría que presenta este sistema es la del grupo de operadores de traslación discreta. Cumplen una serie de propiedades:

\begin{itemize}

\item T(n)T(m)=T(n+m)

\item T(0)=$\mathds{1}$

\item T(-n)=$T(n)^{-1}$

\end{itemize}

La primera propiedad es la de grupo abeliano y la segunda la de unitario. ¿Cómo implementamos entonces esta simetría?

Escogemos una base de autoestados de T(n) (que conmuten para todo n) que sea base para todo n. Es decir:

$$T(n) \ket{u(\xi)}=t_n(\xi)\ket{u(\xi)}$$

La base $\ket{u(\xi)}$ tiene unos autovalores $t_n(\xi)$ que guardan información sobre n y $\xi$. Estos autovalores tienen una serie de propiedades:

\begin{itemize}
    \item $t_n (\xi) t_m(\xi) = t_{n+m}(\xi)$
    \item $t_0(\xi)=1$
    \item $t_{-n}(\xi)=\frac{1}{t_n(\xi)}$
    \item $t_n(\xi)t_m(\xi)=t_m(\xi)t_n(\xi)$
    \item $|t_n(\xi)|^2=1$
\end{itemize}

Luego, los $t_n(\xi)$ adecuados son $ t_n(\xi)=e^{-i\phi _n(\xi)}$ con $\phi (\xi)$ real e impar. Luego $\phi (\xi)=nf(\xi)$ para cualquier $f(\xi)$ real e impar, por ejemplo $f(\xi)=\xi$. Quedan los autovalores:

$$t_n(\xi)=e^{-i\xi n}$$

Se dice que el cojunto de autovalores $\lbrace t_n(\xi) \rbrace$ proporcionan una representación para el grupo de operadores de traslación discreta de dimensión d: $\mathcal{T}^d$.

\subsubsection{Consecuencias de la simetría}
Dado que $t_n(\xi)$ es periódica de periodo $2\pi$  tomamos el intervalo $-\pi <k<\pi$. Sea $k=\xi/b$ en el intervalo $-\pi/b<k<\pi/b$ buscamos los $t_n(k)$ que sean autoestados de T y H al mismo tiempo (es decir que definan adecuadamente las traslaciones y la energía de las partículas al mismo tiempo).

\smallskip
Sea $U_{E,k}(x)=\braket{x|U(E,k)}$ la función de onda en representacion de posiciones.

$$x=nb+y \hspace{0.2cm} \forall y \in \frac{-b}{2}\leq y \leq \frac{b}{2}$$

$$\ket{x}=T(n)\ket{y}; \hspace{0.2cm}\bra{x}=\bra{y}T^+(n)$$
$$\overbrace{\braket{x|U(E,k)}}^{U_{E,k}(x)}=\bra{y}T^+(n)\ket{U(E,k)}=\bra{y}T(-n)\ket{U(E,k)}=\braket{y|U(E,k)}e^{iknb}=U_{E,k}(y)e^{ik(x-y)}$$

Lo que acabamos de demostrar, que $e^{-ikx}U(x)=e^{-iky}U(y)$, implica que podemos estudiar una única celda de la red y tendríamos igualmente toda la información de la misma. Se define la función de Bloch como $V_{E,k}(x)=U_{E,k}(x)e^{-ikx}$ , periódica en x de periodo b con k la llamada variable del vector de ondas. La función de Bloch, si recordamos de sólido, nos permite estudiar todo el cristal a partir de uno de sus electrones pues la red posee una simetría de traslación igual a la de la función (al igual que aquí en nuestro ejemplo).
\newpage

Por tanto, usando la función de onda obtenida en la ecuación de Schrödinger del sistema encontraríamos la solución (sin más que introducir una condición de contorno periódica). Para cualquier k del intervalo admitido se puede encontrar un conjunto de autovalores y autofunciones en forma explícita. En general se obtienen niveles de energía discretos (el Ppio. de exclusión de Pauli limitará luego -o no- el número de partículas por nivel).

\smallskip
Este ejemplo también ilustra la relación entre la teoría de representación de grupos y el análisis armónico (siendo el ejemplo del de Fourier un caso partícular). En nuestro ejemplo las funciones de representación son la base de ondas planas de las sries de Fourier, satisfacen:

$$1.- \int ^\pi _{-\pi} d\xi e^{in\xi} e^{im\xi} =2\pi \delta _{m,n}$$
$$2.- \sum _{n=1}^N e^{in\xi}e^{in\xi '}=\delta (\xi-\xi ')$$

Las llamadas relaciones de ortonormalidad y compeltitud. Dada por tanto una función cualquiera $f(\xi)$ con $\xi \in [-\pi,\pi)$ se puede escribir $f(\xi)=\sum _{n=1}^N f_ne^{-in\xi}$ en base de ondas planas con coeficientes $f_n=\frac{1}{2\pi}\int^\pi _{-\pi}d\xi e^{in\xi}f(\xi)$.






\newpage

\section{Elementos generales de teoría de grupos}
\subsection{Defininición y ejemplos}
Un grupo es un conjunto de elemntos $G=\lbrace g\rbrace$ junto con una ley de composición interna (la llamaremos multiplicación).

$$G\times G\rightarrow G$$
$$(g_1,g_2)\rightarrow g_1g_2 \in G$$

Debe tener las siguientes propiedades:

\begin{enumerate}
    \item Asociativa, $ (g_1g_2)g_3=g_1(g_2g_3)$
    \item Existencia del elemento unidad, $\exists e / \hspace{0.2cm} eg=g\hspace{0.2cm} \forall g\in G$
    \item Existencia de inversa, $\exists g^{-1} / \hspace{0.2cm} g^{-1}g=e \hspace{0.2cm} \forall g\in G$
\end{enumerate}

Puede probarse facilmente que esas propiedades existen también cuando se multiplica por la derecha en vez de por la izquierda.

\smallskip
Definimos el \textbf{grupo abeliano} como aquel grupo que presenta la "propiedad distributiva", $g_ig_j=g_jg_i \hspace{0.2cm} \forall g_ig_j \in G$.

\smallskip
Definimos el \textbf{orden} como el número de elementos de un grupo, denominado por $|G|$.

\smallskip
El teorema de reordenamiento nos asegura que $gG=\lbrace gg_i\rbrace =G$ pues g es un elemento de G y la multiplicación de elementos de un grupo no nos saca del grupo.

\subsection{Ejemplos de grupos}

\subsubsection{Grupos finitos}

\begin{itemize}
    \item Los enteros bajo suma de módulo n $\mathcal{Z}_n$ (grupo cíclico de orden n). Si su suma se sale del conjunto se restan tantos $|n|$ como haga falta para que viva en el conjunto. Veamos por ejemplo la tabla de multiplicar del grupo $\mathcal{Z}_3$:

    $$\begin{tabular}[b]{ c | c c c }

+ & 0 & 1 & 2 \\
\hline
0  & 0 & 1 & 2 \\

1 & 1 & 2 & 0 \\

2 & 2 & 0 & 1

\end{tabular}
$$

    \item Los grupos de invariancia rotacional discreta y de reflexión (grupos puntuales de redes regulares discretas).
    \item Grupo de permutaciones ($S_n$) de n elementos o grupo simétrico. Tiene orden $n!$. Grupo no abeliano.
\end{itemize}

\subsubsection{Grupos infinitos discretos}

\begin{itemize}
    \item Los enteros bajo suma $\mathcal{Z}$.
    \item Los reales bajo multiplicación.
    \item Los grupos de traslación en redes regulares discretas.
\end{itemize}

\subsubsection{Grupos continuos compactos}
\begin{itemize}
    \item Grupo ortogonal O(n), grupo de matices de orden n (n$\times$n) que satisfacen $O^+(n)=O^{-1}(n)$. Son el grupo de rotaciones y reflexiones en $\mathds{R}^n$.
    \item Grupo unitario U(n) de matrices n $\times$ n que satisfacen $U^+U=\mathds{1}$.
    \item SO(n) y SU(n) son subgrupos con det=1 (\textit{ Nota: los grupos con det=-1 no tienen ley de composición interna}).
\end{itemize}

\subsubsection{Grupos continuos no compactos}

\begin{itemize}
    \item $\mathds{R}$, $\mathds{Q}$ y $\mathds{C}$ bajo suma y multiplicación (sin el cero).
    \item El grupo lineal GL(n,$\mathds{K}$) de amtrices n $\times$ n con coeficientes en el cuerpo $\mathds{K}=\mathds{R},\mathds{C}$ y det $\neq$ 0 y su subgrupo, el grupo espacial lineal de las matrices con determinante 1 (SL(n,$\mathds{K}$)).
    \item El grupo euclídeo de transformaciones del tipo $\Vec{x}\rightarrow O\Vec{x}+\Vec{b}$ con $O\in O(n)$ y $\Vec{b}$ un vector constante (E(n)).
    \item El grupo de Poincaré (de isometrías del espacio de Minkowski) y el grupo de Lorentz (subgrupo de isometrías que dejan el origen fijo).
\end{itemize}

\subsection{Subgrupos}

Se llama subgrupo al conjunto de elementos $H \subset G$ que es a su vez un grupo con la misma operación interna que G.

\smallskip
$\hspace{0.5cm }$ \textbf{Teorema:} H es subgrupo de G si para cualesquiera 2 elementos de H cumple que $h'h^{-1}\in H$ y $h'h\in H$.

\subsubsection{Definiciones}

\begin{itemize}
    \item Se dice que un subgrupo es \textbf{propio} si es distinto a G y al elemento identidad.
    \item Definimos el \textbf{centro} de G, Z(G), como el conjunto de elementos que cumplen $h\in G$ / $hg=gh \hspace{0.2cm} \forall g \in G$. Sera no-propio si no es abeliano (pues si no se cumple trivialmente para cualquier subgrupo de G).
\end{itemize}

\subsection{Clases de conjugación}

Un elemento $g_1$ de un grupo G se dice conjugado de otro $g_2$ si existe un $h\in G \hspace{0.2cm} / \hspace{0.1cm} g_1=hg_2h^{-1} \leftrightarrow g_1h=hg_2$.

\begin{itemize}
    \item Si $g_1$ es conjugado a $g_2$ entonces $g_2$ es conjugado a $g_1$.
    \item Si $g_1,g_2$ son conjugados a $G_3$ y todos pertenecen al mismo grupo G $g_1$ y $g_2$ también son conjugados entre sí.
\end{itemize}

Una \textbf{clase de conjugación} es un conjunto de elementos mutuamente conjugados.

\subsubsection{Popiedades}

\begin{enumerate}
    \item Cada $g\in G$ pertenece a alguna clase de conjugación.
    \item Ningún g puede pertenecer a dos clases distintas.
    \item La identidad forma una clase consigo misma.
    \item Si G es abeliano entonces cada $g\in G$ forma una clase consigo misma.
\end{enumerate}

\subsection{Subgrupos normales}
Un subgrupo H de un grupo G es normal (también llamado invariante bajo conjugación) si $ghg^{-1}\in H \hspace{0.2cm} \forall h\in H$ y $\forall g\in G$. Se denota por $H\lhd G$.

$\hspace{0.5cm}$\textbf{Teorema:} $H\subset G$ es un grupo normal de G si H es una unión de clases de conjugación de G.

$$\text{Si} \hspace{0.1cm} N\subset H \subset G \hspace{0.1cm}\text{son subgrupos y} N\lhd G \hspace{0.1cm} \text{entonces}\hspace{0.1cm} N\lhd H $$

\begin{itemize}
    \item Se dice que un subgrupo es \textbf{simple} si no tiene subgrupos normales propios (excluyendo a la identidad y al propio grupo que son triviales).
    \item Un grupo es semi simple si no tiene subgrupos normales abelianos propios. Evidentemente, simple implica semi simple.
\end{itemize}

\subsection{Cosets}

Sea $H=\left \lbrace h_I\right \rbrace$ un subgrupo de G. Se define el coset por la izquierda de H asociado a $g\in H : gH= \left \lbrace gh_i\right \rbrace$ y el coset por la derecha de H asociado a $g\in H : Hg= \left \lbrace h_i g\right \rbrace$.

\smallskip
\textbf{Propiedades} (Valen para los cosets por la derecha o por la izquierda):

\smallskip
$\bullet$ gH coincide con H si y solo si $g\in H$.

\smallskip
$\bullet$ Si $g\notin H$ entonces $e\notin gH$ no es subgrupo de G.

\smallskip
$\bullet$ Cada elemento de G pertenece a algún coset.

\smallskip
$\bullet$ Si $g'\in gH$, entonces $g'H=gH$.

\smallskip
$\bullet$ Dos cosets $g_1H, g_2H$ o bien son idénticos o bien son disjuntos.

\smallskip
A partir de esta propiedades se ve que si H es subgrupo de G, entonces G es una unión disjunta de cosets asociados a H.

\smallskip
$\bullet$ Si G es un grupo finito de H es subgrupo de G, el orden de H es divisor del orden de G ($|G|/|H|\in \mathcal{Z}$, Teorema de Lagrange). A esa división se le llama índice y es el número de cosets diferentes de H.

 \subsection{Grupo cociente}
 Un subgrupo H de un grupo G es normal ($H\lhd G$) si y solo si los cosets por la derecha H coinciden con los cosets por la izquierda.

 Podemos definir el producto de cosets (por la izquierda) de un subgrupo normal $H\in G$ como:

 $$g_1H*g_2H=(g_1\cdot g_2)H; \hspace{0.2cm} g_1,g_2 \in G$$

 No es obvio que $(g_1\cdot g_2)$ sea consistente pues hay elementos de G diferentes para los que sus cosets son los mismos. Necesitamos una definición que nos da la relación de consistencia:

 $$(g_1'g_2')H=(g_1g_2)H$$

 Es decir, si los cosets coinciden debe coincidir también la composición. Por sencillez tomemos $g_1=e$, entonces $eH=H=hH \hspace{0.2cm} \forall h\in H$. Ahora, con el anterior producto $(eg)H=eH*gH=hH*gH=(hg)H=gg^{-1}hgH$.

 \smallskip
 Necesitamos que $g^{-1}hgH=H \leftrightarrow g^{-1}hg\in H \hspace{0.2cm} \forall g \in G$, es decir, H tiene que ser grupo normal. En general si $gH=g'H$ entonces $g'=gH$ con $h\in H$.

 \smallskip
 \textbf{Teorema:} el conjunto de cosets (por la izquierda) de un subgrupo normal H de un grupo G forma un grupo respecto a la operación de multiplicación de cosets anteriormente definida por *. Este grupo se llama grupo cociente (\textit{factor group}). Se denota por G/H. Si G es finito, el orden del grupo cociente es el índice.

 $$G=\lbrace g_1,...,g_n,g_{n+1},...,g_m\rbrace$$
 $$H=\lbrace g_1,...,g_n\rbrace$$
 $$G/H=\left (\lbrace H,g_{n+1}H,...,g_mH\rbrace,*\right )$$

 Nótese que $gH=g'H$ define una relación de equivalencia distinta a la conjugación, podemos pensar en el grupo cociente como el conjunto de esas clases de equivalencia bajo esta relación. Nos sirve para clasificar conjuntos.

 La partición de los elementos del grupo G en costets es única y una "factorización" de G basada en esta partición es natural. Veámoslo como un ejemplo:

 \smallskip
 Sea $G=S_3$ y $H=A_3$ del ejemplo anterior. El grupo cociente viene definido por la tabla de multiplicar:
 \begin{center}
\begin{tabular}[b]{ c | c c}
 * & $A_3$ & $\tau _1$ $A_3$\\
 \hline
 $A_3$ & $A_3$ & $\tau _1 A_3$ \\

 $\tau _1 A_3$ & $\tau _1 A_3$ & $A_3$
 \end{tabular}

 \end{center}


 Cumple:

\begin{itemize}
    \item $A_3*eA_3=(e\cdot e)A_3$
    \item $A_3*\tau _1A_3=(e\tau _1)A_3$

 \item $(\tau _1A_3)*(\tau _1 A_3)=(\tau _1 \tau _1)A_3=A_3$

\end{itemize}

 \smallskip
 El grupo cociente $S_3/A_3= \lbrace A_3, \tau _1 A_3\rbrace $ me da información sobre la paridad de los grupos $S_3$ y $A_3$.

 \subsection{Homomorfismos entre grupos}
 Un homomorfismo entre dos grupos G y G' es una aplicación

 $$\phi : \hspace{0.2cm} G \rightarrow G'$$

 $$g\rightarrow \phi(g)$$

 que verifica que $\phi (g\cdot h)=\phi (g)*\phi (h)$ respetando la estructura de grupo.

 \smallskip
$\bullet$ Un homomorfismo se dice \textbf{fiel} si es inyectivo.

 $$\phi (h)=\phi (g) \hspace{0.2cm} \text{si y solo si g=h}$$

 \smallskip
$\bullet$ Un \textbf{isomorfismo} es un homomorfismo biyectivo (inyectivo y suprayectivo). Un mapa uno a uno de G en otro grupo G' del mismo orden que respeta la multiplicación $G\cong G'$.

 \smallskip
$\bullet$ Un \textbf{automorfismo} es un isomorfismo de un grupo en sí mismo (es decir cambio de mapa dentro de un mismo grupo).

 \subsubsection{Propiedades de los homomorfismos}

 Sean dos grupos $(G,\cdot)\cong (G',*)$ con la identidad bien definida y con homomorfismo entre ellos.

 \begin{itemize}
     \item $\phi (e_g)=e_{g'}$
     \item $\phi (g^{-1})=(\phi (g))^{-1}$

     La imagen de $\phi$ denotada por $\phi (G)$ es la parte de G' alcanzada mediante $\phi$, $\phi (G)=\lbrace g' \in G' \hspace{0.1cm} /\hspace{0.1cm} \exists \hspace{0.1cm} g \in G \hspace{0.1cm}\text{con} \hspace{0.1cm} \phi (g)=g' \rbrace \subset G' $

     \item El núcleo de $\phi$, denotado Ker $\phi$ (o $\phi ^{-1}(e_{g'})$) es el subconjunto de G mapeado bajo $\phi$ a la identidad en G'.

     $$Ker\phi = \lbrace g\in G \hspace{0.2cm} / \hspace{0.1cm} \phi (g)=e_{g'}\rbrace$$

     \begin{center}
     $\phi$ es inyectivo $\leftrightarrow$ Ker $\phi$= $\lbrace e_{g}\rbrace$

     \end{center}
      \end{itemize}

     $\hspace{0.5cm}$\textbf{Teorema de Caley}: todo grupo de orden n es isomorfo a un subgrupo de $S_n$.
     \bigskip



 \textbf{Ejemplo}: Consideremos la aplicación $\phi: \hspace{0.2cm} \mathds{R}\rightarrow S^1=\lbrace z\in \mathds{C}, \hspace{0.2cm} |z|=1 \rbrace ; \hspace{0.2cm} x \longrightarrow \phi (x)=e^{ix}$ que es un homomorfismo de ($\mathds{R},+$) en $S^1$.


 $$a+b \longrightarrow \phi (a+b) =e^{i(a+b)}=e^{ia}\cdot e^{ib}=\phi(a)\phi (b)$$

 Sin embargo, su Ker $\phi =2\pi \mathds{Z}\neq e_g \rightarrow \phi $ no es un isomorfismo.

 \bigskip
 \textbf{Ejemplo 2}: veamos un ejemplo de isomorfismo entre grupos, el grupo $S_3$ es isomorfo a $D_3$ (el grupo de simetrías en el plano del triángulo equilátero).

 \begin{itemize}
     \item Invariante bajo rotaciones de  $0º \rightarrow \hat{e}$, $120º \rightarrow \hat{\sigma}_1$ y $240º \rightarrow \hat{\sigma} _2$. Reflexiones con respecto a los ejes que pasan por sus vértices.

     \item  Reflexiones respecto al vértice que pasa por el vértice i-ésimo ($\hat{\tau}_1$).
 \end{itemize}

 \subsubsection{Teoremas}

 Sea $\phi : G\rightarrow G'$ un homomorfismo entre grupos, entonces:

      \begin{enumerate}
\item Ker $\phi$ es un subgrupo normal de G.

\item La imagen de $\phi (G)$ es subgrupo de G'.

\item El grupo cociente $G/Ker\phi$ es isomorfo a $\phi (G)$, con el isomorfismo dado por

$\hat{\phi} : G/Ker\phi \rightarrow \phi (G)$ (los cosets de Ker$\phi$).

 \end{enumerate}

 Veamos la demostración de los teoremas en orden.

 \bigskip
 Para el \textbf{primero} debemos probar que Ker$\phi$ es subgrupo.

 \smallskip
 Consideremos $g,h\in Ker\phi $ , es decir $\phi (g) =\phi (h)=e_{g'}$. Entonces:

 $$\phi (gh^{-1})=\phi (g)\phi (h^{-1})=\phi (g)(\phi(h))^{-1}=e_{g'}e_{g'}^{-1}=e_{g'} \rightarrow gh^{-1} \in Ker \phi $$

 y por tanto Ker $\phi$ \textbf{es subgrupo} de G.
 \smallskip

 Para probar que Ker$\phi$ es normal, consideremos cualquier elemento de G, $h\in G$ (no necesariamente en Ker$\phi$); llamemos $g\in Ker \phi$ a cualquier elemento del núcleo. Tenemos:

 $$\phi (hgh^{-1})=\phi(h)\phi (g)\phi (h^{-1})=\phi(h)(\phi(h))^{-1}=e_{g'}\rightarrow hgh^{-1}\in Ker\phi$$


Luego $hKer\phi h^{-1}=Ker \phi \hspace{0.2cm} \forall h\in G$, \textbf{es normal}.

 \bigskip
 Vamos con la del \textbf{segundo}.
 \smallskip

 Sean $\phi (g),\phi (h)\in  \phi(G)$ entonces:

 $$\phi (g)(\phi(h))^{-1}=\phi (g)\phi (h^{-1})=\phi (gh^{-1})\in \phi (G)\rightarrow \phi (G)$$

es subgrupo.

  \bigskip
  Y el \textbf{tercero}.

  Veamos que $\hat{\phi}$ esta bien definido (es decir, no existen elementos de G para los que $gKer\phi =g'Ker\phi$), por tanto, hay que asegurarse de que $\hat{\phi} (gKer\phi)=\hat{\phi}(g'Ker\phi)$, es decir que $\phi(g)=\phi (g')$.
  \smallskip

  En efecto si $gKer\phi =g'Ker \phi \rightarrow ker\phi=g^{-1}g'Ker\phi \rightarrow g^{-1} g' \in Ker \phi$. Entonces:

 $$ \phi (g')= \phi (gg^{-1}g')=\phi (g)\phi (g^{-1}g')=\phi (g)$$

 como queríamos probar.

 \smallskip
 Necesitamos ahora probar que es \textbf{homomorfismo}, es decir, debemos probar que respeta el producto en el grupo. Si tomamos el producto de los mapas de dos cosets $\hat{\phi} (gKer\phi)\cdot \hat{\phi} (hKer\phi)=\phi (g)\phi (h)=\phi (gh)=\hat{\phi} (ghKer\phi)$ vemos que efectivamente respeta el producto de cosets.

 \smallskip
 Debemos probar también que es \textbf{inyectivo} (su núcleo es la identidad). Sea el coset $gKer\phi \in Ker\hat{\phi} \leftrightarrow \hat{\phi} (gKer\phi)=e_{g'}\rightarrow \phi (g)=e \rightarrow g\in Ker \phi \rightarrow gKer\phi =Ker\phi$, que es el elemento identidad del grupo cociente y por tanto el núcleo es trivial.

 \smallskip
 Además debe ser \textbf{suprayectivo}; como el mapa es suprayectivo (o sobreyectivo) por construcción, es decir la imagen es todo $\phi (G)$, entonces lo es. Por lo tanto el grupo cociente es efectivamente un isomorfismo de $\phi (G)$.

 \bigskip
 Este teorema proporciona una manera sencilla de ver si un subgrupo es normal, buscando un homomorfismo del cual dicho subgrupo es su núcleo. Se trata además de un criterio exhaustivo ya que para cualquier subgrupo normal $H\lhd G$ el mapa $\uppi : G\rightarrow G/H$;  $g\rightarrow gH$ también es un homomorfismo entre grupos con $Ker\uppi =H$.

 \smallskip
 \textbf{Corolario}: un subgrupo $H\subset G$ es normal si y solo si existe un homeomorfismo entre grupos $\phi : G\rightarrow G'$ con $Ker\phi =H$.

 \newpage
 Veamos algunos ejemplos.

 \begin{itemize}
     \item Para cualquier grupo matricial sobre un cuerpo $\mathds{K}$, el determinante es un homomorfismo a $\mathds{K}^*$, el cuerpo sin el cero (las matrices de determinante 0 no tienen inverso y no se podran mapear). El núcleo de este mapa consiste en todas las matrices con determinante 1. Que por el teorema 1 vemos que determinan un subgrupo normal y podemos hablar de un grupo cociente. Por el teorema 3, los grupos cocientes son isomorfos a las respectivas imágenes.

     \item Dentro de grupos matriciales vemos, por ejemplo, el grupo general lineal de matrices $GL(n,\mathds{K})$ tiene el grupo $SL(n,\mathds{K}) \lhd GL(n,\mathds{K})$ y su grupo cociente $GL(n,\mathds{K})/SL(n,\mathds{K})\cong \mathds{K}^*$.

    \item También el grupo de matrices unitarias $U(n)$ tiene un subgrupo $SU(n)\lhd U(n)$ tal que su cociente $U(n)/SU(n) \cong S^1 \cong U(1)$.
 \end{itemize}

 \subsection{Veamos un ejemplo con algunas cosillas del tema, el grupo $S_3$ de permutaciones con 3 elementos}
Sean las siguientes permutaciones:

$$e=\begin{bmatrix}
1 & 2 & 3 \\
1 & 2 & 3
\end{bmatrix} \hspace{0.5cm}
\tau_1=\begin{bmatrix}
1 & 2 & 3 \\
1 & 3 & 2
\end{bmatrix} \hspace{0.5cm}
\tau_2=\begin{bmatrix}
1 & 2 & 3 \\
3 & 2 & 1
\end{bmatrix} \hspace{0.5cm}
\tau_3=\begin{bmatrix}
1 & 2 & 3 \\
2 & 1 & 3
\end{bmatrix} \hspace{0.5cm}
$$
$$
\sigma_1=\begin{bmatrix}
1 & 2 & 3 \\
2 & 3 & 1
\end{bmatrix} \hspace{0.5cm}
\sigma_2=\begin{bmatrix}
1 & 2 & 3 \\
3 & 1 & 2
\end{bmatrix} \hspace{0.5cm}$$

Sus clases de conjugación son:

 \begin{enumerate}
     \item La identidad.
     \item Los ciclios (taus).
     \item Los 3-ciclos (sigmas).
 \end{enumerate}

 Su tabla de multiplicar:


$$\begin{tabular}[b]{ c | c c c c c }

e & $\tau _1$ & $\tau _2$ & $\tau_3$ & $\sigma _1$ & $\sigma _2$\\
\hline
$\tau_1$ & e & $\sigma _1$ & $\sigma _2$ & $\tau _2$ & $\tau _3$\\

$\tau _2$ & $\sigma _2$ & e & $\sigma _1$ & $\tau _3$ & $\tau _1$\\

$\tau _3$  & $\sigma _1$ & $\sigma _2$ & e & $\tau 1$ & $\tau _2$\\

$\sigma _1$ & $\tau _3$ & $\tau _1$ & $\tau _2$ & $\sigma _2$ & e\\

$\sigma _2$ & $\tau _2$ & $\tau _3$ & $\tau 1$ & e & $\sigma _1$

\end{tabular} $$

Tiene dos subgrupos propios, el de los 3 ciclos y el de $\tau _1$ con la unidad:

\begin{center}
$\begin{bmatrix}
e & \sigma _1 & \sigma _2\\
\sigma _1 & \sigma _2 & e \\
\sigma _2 & e & \sigma _1
\end{bmatrix}$\hspace{3cm}
$\begin{bmatrix}
e & \tau _1\\
\tau _1 & e
\end{bmatrix}$
\end{center}

Son grupos abelianos.

\smallskip
Veamos sus clases de conjugación, deben cumplir (para $\sigma _1$):

$$g\sigma _1 g^{-1}=h\in G$$

\smallskip

Con $\sigma _1$ y $\sigma _2$, vemos que forman una clase de conjugación (en este caso subgrupo normal):

\smallskip

$$\sigma _2\sigma _1 \sigma _2^{-1}=\sigma _2 \sigma _2=\sigma _1$$

$$\tau _1\sigma _1 \tau ^{-1}_1=\tau _1\tau _3=\sigma _2 \leftrightarrow \tau _1 \sigma _2 \tau _1 ^{-1}=\sigma _1$$

$$\tau _2\sigma _1 \tau ^{-1}_2=\tau _2\tau _1=\sigma _2 \leftrightarrow \tau _2 \sigma _2 \tau _2 ^{-1}=\sigma _1$$

$$\tau _3\sigma _1 \tau ^{-1}_3=\tau _3\tau _2=\sigma _2 \leftrightarrow \tau _3 \sigma _2 \tau _3 ^{-1}=\sigma _1$$

\smallskip
$$\left \lbrace e \right \rbrace U \left \lbrace \sigma _1, \sigma _2 \right \rbrace=A_3$$

Para $\tau _1$:

$$\tau _2 \tau _1 \tau _2^{-1}=\tau _2\tau _1 \tau _2 =\tau _2 \sigma _1=\tau _3$$

$$\tau _3 \tau _1 \tau _3^{-1}=\tau _3\tau _1 \tau _3 =\tau _3 \sigma _2=\tau _2$$

Que es el grupo $\lbrace \tau _1, \tau _2 , \tau _3 \rbrace$.

Busquemos los cosets de $A_3$, deben cumplir que $gA_3=\lbrace ge, g\sigma _1, g\sigma _2\rbrace$.

$$eA_3=A_3$$

$$\sigma _1 A_3=\lbrace \sigma _1, \sigma _2 , e \rbrace=A_3$$

$$\sigma _2 A_3=\lbrace \sigma _2, e, \sigma _1 \rbrace=A_3$$

Vemos que $gH=H$ si $g\in H$.

$$\tau _1 A_3=\lbrace \tau _1, \tau _2, \tau _3\rbrace$$

$$\tau _2 A_3=\lbrace \tau _2, \tau _3, \tau _1\rbrace$$

$$\tau _3 A_3=\lbrace \tau _3, \tau _1, \tau _2\rbrace$$

Y también comprobamos que $\tau _1 A_3=\tau _2 A_3=\tau _3 A_3$.


 \subsection{Automorfismos internos}

 Definimos los automorfismos como

 $$\phi : G \to G$$
 $$Ker \phi \lhd G$$
 $$\phi (G) \hspace{0.1cm} \text{es subgrupo de G'}$$
 $$\exists \hspace{0.1cm} \hat{\phi} \hspace{0.1cm} $$

  El conjunto de automorfismos internos forman grupo y $g \to \phi _g$ es un homomorfismo de grupos $\phi _g \phi _h=\phi _{gh}$ (el producto de mapas es el mapa del producto).

  El núcleo de este homeomorfismo es el conjunto de elementos que conmutan con todos los elementos de G, es decir, su centro Z(G). Esto es consistente con que Z(G)$\lhd$G.

\smallskip
HE LLEGADO TARDE Y AQUI FALTA ALGO IMPORTANTE DE LA DEFINICION DE AUTOMORFISMO.

\subsection{Producto de grupos}

El \textbf{producto directo} de dos grupos $G_1$ y $G_2$ se define como el conjunto

$$G_1\times G_2=\lbrace (g_1,g_2) \hspace{0.1cm}| \hspace{0.1cm} g_1 \in G_1, \hspace{0.1cm} g_2\in G_2\rbrace$$

que tiene estructura de grupo con respecto a la multiplicación $(g_1,g_2)*(g_1'g_2')=(g_1\cdot g_1',g_2\cdot g_2')$. Existe identidad, inverso y el orden del grupo producto es $N_1\times N_2$ siendo los órdenes de $G_1$ y $G_2$ respectivamente (si tienen orden finito).

\begin{itemize}
        \item $G_1 \times G_2$ tiene subgrupos normales evidentes tales como $(e_g,G_1)=\lbrace (g_1,e_g) \hspace{0.1cm} | \hspace{0.1cm}g_1 \in G_1 \rbrace \cong G_1$ y $(e_g,G_2)=\lbrace (g_2,e_g) \hspace{0.1cm} | \hspace{0.1cm}g_2 \in G_2 \rbrace \cong G_2$, sugieren también unos homomorfismos naturales:

    $$\pi _1: G_1\times G_2 \to G_1$$
    $$\pi _2 : G_1\times G_2 \to G_2$$

    $(e_g,G_1)$ y $(e_g,G_2)$ conmutan entre sí así que cada elemento de $G_1 \times G_2$ se puede escribir de forma única como $(g_1,g_2)=(g_1,e_{G_1})*(g_2,e_{G_2})$.
    \item Un grupo G' se dice que es grupo producto directo si es isomorfo a algún grupo con esta estructura. Los elementos de G' a priori no necesitan tener una estructura en forma de pares.

\end{itemize}

 \textbf{Teorema}: G es un producto directo de sus subgrupos $G_1$,$G_2$ si se cumplen las siguientes condiciones:

 \begin{enumerate}
     \item $G_1$y $G_2$ son subgrupos normales (o equivalentemente, los elementos de uno conmutan con los elementos del otro).
     \item $G_1$ y $G_2$ han de ser disjuntos salvo identidad.
     \item $G_1$ y $G_2$ generan G, es decir, $G=G_1G_2$ (cada elemento de G se puede escribir como producto de elementos de $G_1$ y $G_2$).
 \end{enumerate}

 \textbf{Corolario}: si G es producto directo de dos subgrupos normales $G_1,G_2$ entonces $G/G_1 \cong G_2$ y $G/G_2 \cong G_1$. \textbf{Ojo}, si H es un grupo normal de G y H' es el grupo cociente no significa que $G=H\times H'$.

 \bigskip
 \textbf{Ejemplos:}

 \begin{itemize}
     \item U(n)=U(1)$\times$SU(n); cada matriz unitaria se puede escribir como un producto $e^{i\phi}\mathds{1}\cong U(1)$ y una matriz SU(n) y ambas conmutan.
     \item O(3)$\cong$ SO(3)$\times G_2$ con $G_2 =\lbrace \mathds{1},-\mathds{1}\rbrace$
 \end{itemize}

 ¿Qué ocurre si alguno de los subgrupos no es normal? Un grupo $G^*$ se dice que es \textbf{producto semidirecto} si posee dos subgrupos $G_1$ y $G_2$ tales que:

 \begin{enumerate}
     \item $G_1$ es subgrupo normal de G.
     \item $G_1 \cap G_2 =\lbrace e\rbrace$ (como antes, disjutos salvo identidad).
     \item Cada elemento de $g\in G$ puede escribirse como $g=g_1g_2$.
 \end{enumerate}

 $$G^*\cong G_1\rtimes G_2 \to \text{Producto semidirecto}$$

 \newpage
 \textbf{Ejemplos}:

 \begin{itemize}
     \item $S_3$ y $A_3=\lbrace e,\sigma _1, \sigma _2 \rbrace$.

    \item E(2), grupo de traslaciones afines en el plano, cuyos elementos vienen especificados por una matriz ortogonal O(2) y un vector traslación. Mientras que las traslaciones forman un subgrupo normal. O(2) no es normal pues no es invariante bajo conjugación. Así, $E(2)\cong \mathds{R}^2\rtimes O(2)$.
    \item Poincaré $\cong \mathds{R}^4 \rtimes$ Lorentz
 \end{itemize}



\subsection{Ejercicios del tema 2}

 $\hspace{0.5cm}$ \textbf{1.} Probar que un grupo finito de orden n (primo) debe ser un grupo cíclico  (generado por a).

 $$C_n=\lbrace a,a^2,...,a^{n-1},a^n=e\rbrace \cong Z_n$$


 \bigskip
 Por el teorema de Lagrange el orden de un subgrupo H de un grupo G debe ser divisor de orden n de G. Por otra parte, cada elemento del grupo genera un subgrupo cíclico y llamamos orden del elemento al orden m del subgrupo cíclico que genera. Por el teorema de Lagrange m debe ser divisor de n y como m es primo $m=\lbrace 1,n\rbrace$.

 Si n es primo entonces el orden de los elementos de G ($m_g, \hspace{0.1cm} \forall g\in G$) debe ser n (a excepción de la identidad) y el subgrupo cíclico que generan es el mismo grupo G.

 \smallskip

 Deducimos también que dos grupos cualesquiera cuyos órdenes sean el mismo número primo son isomorfos e isomorfos al grupo cíclico de ese orden.

 Nota, definiendo el isomorfismo:

 $$C_n\cong Z_n, \hspace{2cm} \begin{array}{c}
 \phi : Z_n \to C_n \\
 m \to \phi (m)=a^m
 \end{array} $$

 $$m_1+m_2 \to \phi(m_1+m_2)=a^{m_1+m_2}=\phi(m_1)\phi (m_2)$$

 \bigskip
 $\hspace{0.5cm}$ \textbf{2.} Probar que $G=H_1\times H_2$ implica que $G/H_1\cong H_2$.

 \bigskip
 Por ser G producto directo de los otros dos grupos, estos son normales y tiene sentido construir los grupos cociente. $G/H_1$ es grupo con respecto a la multiplicación de cosets. Tenemos que:

 $$G/H_1=\lbrace gH_1\rbrace=\lbrace h_1h_2H_1 \rbrace=\lbrace (h_1H_1)(h_2H_1)\rbrace=\lbrace (e,H_1)(h_2H_1)\rbrace=\lbrace h_2 H_1\rbrace$$

 Concluimos que los cosets de $H_1$ generados por los elementos de $H_2$ son los únicos elementos del grupo cociente $G/H_1$. Esto sugiere la correspondencia natural uno a uno dada por:

 $$ \left . \begin{array}{c}
 \phi : H_2 \to G/H_1\\
h_2 \to \phi (h_2)=h_2H_1
 \end{array} \right | G/H_1\cong H_2$$

 Además:

 $$\phi (hh')=hh'H_1=(hH_1)(h'H_1)=\phi (h) \phi (h') \hspace{0.1cm}\forall h,h' \in H_2$$

\bigskip
\textbf{3.} Probar que, salvo isomorfismos, solamente hay dos grupos diferentes de orden 4: el grupo cíclico $C_4$ y el grupo de
reflexiones en el plano $V_4$ (también llamado grupo de Klein). Demostrar también que el grupo de Klein es producto
directo de $C_2$ consigo mismo.

\smallskip
$$C_4=\lbrace  a,a^2,a^3,A^4=e\rbrace$$

$$V_4=\lbrace e,\sigma ,\tau ,\rho \rbrace$$

$$e: \begin{array}{c}
     x\to x  \\
     y\to y
\end{array}; \hspace{0.5cm} \sigma : \begin{array}{c}
     x\to -x  \\
     y\to y
\end{array}; \hspace{0.5cm} \rho : \begin{array}{c}
     x\to x  \\
     y\to -y
\end{array}; \hspace{0.5cm} \tau : \begin{array}{c}
     x\to -x  \\
     y\to -y
\end{array}; \hspace{0.5cm}$$

\smallskip
La tabla de $V_4$ es (se ve que es un grupo abeliano):

\begin{center}
\begin{tabular}[b]{ c | c c c c }
$V_4$ & e & $\sigma $ & $\tau $ & $\rho $\\
 \hline
e & e & $\sigma $ & $\tau $ & $\rho $ \\
$\sigma $ & $\sigma $ & e & $\rho $ & $\tau $ \\
$\tau $ & $\tau $ & $\rho $ & e & $\sigma $ \\
$\rho $ & $\rho $ & $\tau $ & $\sigma $ & e
\end{tabular}
\end{center}


\smallskip
Para demostrar que solo hay estos grupos de orden 4 sabemos que sus elementos generan subgrupos cíclicos. Por el teorema de Lagrange los ementos de G solo pueden ser o de orden 4 o de orden 2 (divisores de 4). Si G tiene al menos un elemento de orden 4 entonces ese elemnto genera $C_4$ y como $C_4 \in G \to G \cong C_4$.

Si G no tiene ningún elemento de orden 4 entonces todos sus elementos son de orden 2, llamémoslos $\lbrace e,\sigma ,\tau ,\rho \rbrace$, con e identidad y los cuadrados de todos iguales a la identidad pues generan $C_2$. Nos falta conocer como pueden ser los productos cruzados. Por ejemplo $\sigma \tau =\rho$ necesariamente, ya que $\sigma \tau \neq e$ pues $\sigma \sigma =e$; $\sigma \tau \neq \sigma$ pues $\tau \neq e$ y $\sigma \tau \neq \tau$ pues $\sigma \neq e$. Ocurre lo mismo con el resto de cruzados lo que implica que solo hay esta tabla (este grupo abeliano) y por tanto solo dos grupos de orden 4; el cíclico y el abeliano.

\smallskip
Para la demostración de que $V_4\cong C_2 \times C_2$, sabemos que $V_4=\mathds{Z}_2 \times \mathds{Z}_2$. Sabemos que $C_2\cong \mathds{Z}_2$ y que son subgrupos normales pues $V_4$ es abeliano. Además, es única (salvo conmutación ya que es abeliano) pues dados los subgrupos normales $\lbrace e, \sigma \rbrace$ y $\lbrace e, \tau \rbrace$ su producto directo genera $V_4$ de forma única (genera $\rho$ únicamente bajo el producto $\tau \sigma$ o $\sigma \tau$ al ser abeliano).

\bigskip
\textbf{4.} Consideremos el grupo diédrico $D_4$, que es el grupo de simetría de un cuadrado. Si situamos el cuadrado en el
plano xy, centrado en el origen de coordenadas y con sus lados paralelos a los ejes de coordenadas, entonces el
grupo consiste en rotaciones en torno al centro y reflexiones con respecto a los ejes vertical, horizontal y diagonales
de pendiente $\pm 1$. Llamemos e a la identidad, g a la rotación de ángulo $\pi/2$ (en sentido antihorario) y h a la reflexión
con respecto a la diagonal y = x. Demuestra que el grupo está generado por g y h y escribe su tabla de multiplicar.

\smallskip

Sean las rotaciones:

$$R=\lbrace  g,g^2,g^3,g^4=e\rbrace$$

$$g: \hspace{0.1cm} \text{giro de} \hspace{0.1cm} \pi/2; \hspace{0.5cm} g^2: \hspace{0.1cm} \text{giro de} \hspace{0.1cm} \pi; \hspace{0.5cm} g^3: \hspace{0.1cm} \text{giro de} \hspace{0.1cm} 3\pi /2; \hspace{0.5cm} g^4 :  \hspace{0.1cm} \text{giro de} \hspace{0.1cm} 2\pi=e$$

Y las reflexiones:

$$H=\lbrace h_1, h_2, h_3, h_4 \rbrace$$

$$h_1: \hspace{0.1cm} y=x; \hspace{0.5cm} h_2: \hspace{0.1cm} y=-x; \hspace{0.5cm} h_3: \hspace{0.1cm} x=0; \hspace{0.5cm} h_4: \hspace{0.1cm} y=0$$

La combinación de R con $h_1$ genera el grupo $D_4=R\times H$, lo vemos en su tabla de multiplicar:
\bigskip
\begin{center}

\begin{tabular}[b]{p{0.7cm}| p{0.7cm}p{0.7cm}p{0.7cm}p{0.7cm}p{0.7cm}p{0.7cm}p{0.7cm}p{0.7cm} }
D$_4$ & e & g & g$^2$ & g$^3$ & h & gh & g$^2$h & g$^3$h \\
\hline
e & e & g & g$^2$ & g$^3$ & h & gh & g$^2$h & g$^3$h \\
g & g & g$^2$ & g$^3$ & e & gh & g$^2$h & g$^3$h & h \\
g$^2$ & g$^2$ & g$^3$ & g & e & g$^2$h & g$^3$h & h & gh \\
g$^3$ & g$^3$ & e & g & g$^2$ & g$^3$h & h & gh & g$^2$h \\
h & h & g$^3$h & g$^2$h & gh & e & g$^3$ & g$^2$ & g \\
gh & gh & h & g$^3$h & g$^2$h & g & e & g$^3$ & g$^2$ \\
g$^2$h & g$^2$h & gh & h & g$^3$h & g$^2$ & g & e & g$^3$\\
g$^3$h & h$^3$h & g$^2$h & gh & h & g$^3$ & g$^2$ & g & e
\end{tabular}

\end{center}
\bigskip

Que se ve bien como se ha construido la tabla representándolo gráficamente.


\newpage
\section{Representación de grupos}
\subsection{Acciones de grupo}
Cuando un grupo actúa sobre un espacio vectorial, se obtiene una representación.

Antes de estudiarlas veamos qué es la acción de un grupo sobre un conjunto genérico X. Las biyecciones de este conjunto X forman un grupo, el grupo simétrico de X o sym(X). Notar que si X es finito y contiene n elementos el grupo de biyecciones es sym(X)=$S_n$.

\smallskip
Una \textbf{acción de grupo} es un homomorfismo de G en sym(X).

$$\begin{array}{c}
     G \to sim(X)  \\
     g\to gx \hspace{0.1cm} \forall x\in X
\end{array}$$

Se suele escribir como multiplicación por la izquierda en lugar de $f_g(X)$. Automáticamente vemos que se satisface que $(gh)x=g(hx)$ y $ex=x$.

\smallskip
\textbf{Ejemplos}:

\smallskip
Algunos grupos se definen por su acción:

\begin{itemize}
    \item El grupo simétrico $S_n$ actúa en el conjunto de n elementos.
    \item El grupo euclídeo actúa en el espacio afín $\mathds{R}^n$.
    \item El grupo ortogonal actúa en la esfera unidad $S^{n-1}$.
    \item Cualquier otro grupo actúa sobre sí mismo de dos maneras: por multiplicación por la izquierda $h\to hg$ (o por la derecha $h \to gh$) o por conjugación $h \to ghg^{-1}$.
    \end{itemize}

La acción del grupo presenta varias \textbf{propiedades}:

\begin{enumerate}
    \item La acción es fiel si el homomorfismo es isomorfismo.
    \item La acción es transitiva si $\forall x,y\in X \hspace{0.1cm}\exists \hspace{0.1cm} g\in G \hspace{0.1cm}/ \hspace{0.1cm} gx=y$.
    \item La acción es libre si $gx=x \to g=e$.
    \item La acción es regular si es transitiva y libre.
\end{enumerate}

\textbf{Definición (1)}: si la accion no es fiel, los elementos de grupo que dejan todos los elementos de X invariantes forman un subgrupo.

$$N= \left \lbrace g \in G, \hspace{0.1cm} gx=x \hspace{0.2cm} \forall x \in X \right \rbrace$$

\textbf{Definición (2)}: fijado un punto $x\in X$ entonces su órbita es el conjunto de imágenes.

$$Gx=\left \lbrace gx \hspace{0.1cm} / g\in G \right \rbrace$$

\textbf{Definición (3)}: inversamente, el estabilizador (o grupo pequeño) es el conjunto de elementos del grupo que dejan X invariante.

$$G_x=\left \lbrace g\in G  \hspace{0.1cm}/ \hspace{0.1cm} gx=x \right \rbrace$$

$\hspace{0.5cm}$ \textbf{Teorema órbita-estabilizador:} Dado un punto x la órbita $Gx$ está en correspondencia uno a uno con el conjunto de cosets por la izquierda del estabilizador.El mapa $gx \hspace{0.1cm}/\hspace{0.1cm} gG_x$ es un isomorfismo $|Gx|=\frac{|G|}{|G_x|}$.

$$gx/gG_x \simeq |Gx| =\frac{|G|}{|G_x|}$$

\newpage
\subsection{Representaciones lineales}

La multiplicación (o composición) de transformación lineal sobre un espacio vectorial es básicamente una multiplicación de grupo. Un conjunto de trasformaciones lineales invertibles cerrado con respecto a la multiplicación satisface los axiomas de grupo. Tal conjunto forma un grupo de representaciones lineales o grupo de operadores.

\subsubsection{Representación de grupo G}

Si existe un homomorfismo de un grupo G a un grupo de operadores D(G) en un espacio vectorial V

$$D(G): \hspace{0.1cm} G\to GL(V)$$

decimos que D(G) es una forma de representación lineal de G.

\begin{itemize}
    \item La dimensión de la representación es la dimensión del espacio vectorial V (consideramos dim(V)$<\infty$ y $V\in \mathds{C}$ aunque muchos resultados se pueden generalizar a dimensión infinita).
    \item Una representación se dice fiel si el homomorfismo es isomorfismo. Si no es fiel es degenerada. Siendo más específicos, una representación lineal de G en el espacio vectorial V es una aplicación de D que a cada elemento de $g\in G$ le asocia un operador invertible:

    $$D(G): \hspace{0.1cm} V \hspace{0.2cm} \to V \hspace{0.1cm}\text{de forma que } \hspace{0.1cm} D(gh)=D(g)D(h)\hspace{0.1cm} \forall g,h \in G$$
\end{itemize}

\subsubsection{Terminología}
G actúa sobre V (via D). Los elementos de V se transforman bajo la representación de:

\begin{itemize}
    \item Representación matricial.

    $$V=\mathds{C}^n\to GL(V)=GL(n, \mathds{C})$$
     \item Representación n-dimensional.

     Equiparamos aplicaciones lineales $A: \hspace{0.1cm} \mathds{C}^n \to  \mathds{C}^n$ con su matriz en la base canónica de $\mathds{C}^n$. La multiplicación de grupo se traduce a multiplicacción matricial.

     $$\begin{array}{cc}
          D: \hspace{0.1cm}G \to GL(n, \mathds{C}) \\
          g\to D(g)_{ji}
     \end{array}$$

    Actúa sobre la base $\lbrace \Vec{e}_i\rbrace_{i=1,...,n}$ de $\mathds{C}^n$.
\end{itemize}

\textbf{Ejemplos}:

\begin{itemize}
    \item Todo grupo posee una representación 1-dimensional trivial. Sea $V=\mathds{C}$ y $D(g)=1$, $\forall \hspace{0.1cm} g$; claramente $D(g_1)D(g_2)=1\cdot 1=1=D(g_1g_2)$.
    \item Los grupos matriciales $GL(n,\mathds{C})$ y subgrupos de este tienen de forma natural la representación por ellos mismos. Esta represetación se llama representación de definición.
    \item Sea G un grupo de matrices, $V\in \mathds{C}$ y $D(g)=det(g)$ esto define una representación 1-dimensional no trivial ya que $det(g_1)det(g_2)=det(g_1g_2)$.
    \item Recordamos que para cualquier real $\xi$ el intervalo $(-\pi, \pi]$ los números $\lbrace e^{-in\xi}; \hspace{0.1cm} n=0,\pm 1, \pm 2... \rbrace$ forman una representación del grupo e traslaciones discretas en una dimensión espacial.
\end{itemize}

 \smallskip
    \textbf{Ejercicio:} sea $G=S^1 \cong U(1)$ demostrar que la aplicación

    $$\begin{array}{cc}
         D: S^1 \to GL(2,\mathds{C})  \\
         e^{i\theta} \to D(e^{i\theta})= \left (\begin{array}{cc}
           cos \theta   & -sen \theta  \\
           sen \theta   & cos \theta
         \end{array} \right )=R( \theta), \hspace{0.1cm} \theta \in \mathds{R}
    \end{array}$$

    es una representación de $S^1$. Lo mismo para:

     $$\begin{array}{cc}
         D: S^1 \to GL(3,\mathds{C})  \\
         e^{i\theta} \to D(e^{i\theta})= \left (\begin{array}{ccc}
           cos \theta   & -sen \theta  & 0\\
           sen \theta   & cos \theta & 0 \\
           0 & 0 & 1
         \end{array} \right )=R( \theta), \hspace{0.1cm} \theta \in \mathds{R}
    \end{array}$$

    \begin{itemize}
        \item    D está bien definida: $e^{i\theta} =e^{i\theta '} \leftrightarrow \theta =\theta ' + 2\pi k \hspace{0.1cm} k\in \mathds{Z} \to R(\theta)=R(\theta ')$.


 \item  Respeta la estructura de grupo $D(e^{i\theta _1}e^{i\theta _2})=R(\theta _1 +\theta _2)=R(\theta _1)R(\theta _2)^=D(e^{i\theta _1})D(e^{i\theta _2})$.

   \item Además es una representación fiel pues $D(e^{i\theta _1})=D(e^{i\theta _2})\leftrightarrow \theta _1=\theta _2 +2k\pi \hspace{0.1cm} \forall k\in \mathds{Z} \leftrightarrow e^{i\theta _1}=e^{i\theta _2}$.

    La $3\times 3$ lo cumple también pues cumple todo lo anterior.

        \end{itemize}

    \bigskip
    \textbf{Proposiciones:}

    \begin{itemize}
        \item Si el subgrupo normal H de G existe, entonces cualquier representación del grupo cociente también es una representación de G (esta es necesariamente degenerada).
        \item Toda representación no trivial de un grupo simple es fiel.
    \end{itemize}

    \textbf{Ejercicio:} da una representación no trivial de $S_3 / A_3$ y demuestra que es una representación no fiel de $S_3$.

    \bigskip
    $S_3/A_0 \simeq C_2=\lbrace e,a \rbrace$ tal que $a^2=1$. Una representación del grupo cociente sería $D(e)=1$, $D(a)=-1$; esto nos genera la tabla de multiplciación:

    $$\begin{tabular}[b]{ c | c c }

 & e & a  \\
\hline
e & e & a  \\
a & a  & e  \\

\end{tabular}  $$

$$S_3 = \lbrace e,\sigma _1, \sigma _2 , \tau _1, \tau _2, \tau _3 \rbrace \hspace{0.2cm} A_3=\lbrace e, \sigma _1, \sigma _2 \rbrace \hspace{0.2cm} S_3/A_3 = \lbrace A_3, \tau _1 A_3 \rbrace$$

Es decir,

$$\begin{array}{cc}
    D(e)=1 & D(\tau _1)=-1 \\
   D(\sigma _1)=1  & D(\tau _2)=-1 \\
   D(\sigma _2)=1 & D(\tau _3)=-1
\end{array}$$

Por ello esta representación nos recupera la tabla de multiplicar, porque $\tau _1 \tau _j=\sigma _k$, donde $i,j,k=1,2$. Pero a cada elemento de salida le corresponden varios elementos de entrada, por lo que la representación no es fiel.

\subsection{Representación conjugada y contragradiente}

\begin{itemize}
    \item Si $D:\hspace{0.1cm} G \to GL(n, \mathds{C})$ es una representación matricial de G y la aplicación

$$\begin{array}{cc}
     \Bar{D}: \hspace{0.1cm} G \to GL(n \mathds{C})  \\
      g \to \Bar{D}(g)=\overline{D(g)}
\end{array}$$

es también representación de G y se llama compleja conjugada.
\item La aplicación

$$\begin{array}{cc}
    \Tilde{D}: \hspace{0.1cm} G\to GL(n, \mathds{C})  \\
     g \to \Tilde{D}(g)=(D(g)^t)^{-1}
\end{array}$$

es también representación de G y se llama representación contragradiente.
\end{itemize}

    \subsection{Equivalencia de representaciones}
    Dos representaciones D(G) y D'(G) de un grupo de G en un espacio vectorial V. Son equivalentes si están relacionadas a través de una transformación de similaridad. Es decir, si existe un operador lineal invertible:

    $$A: \hspace{0.1cm} V\to AVA^-1$$
    $$D'(G)=AD(G)A^{-1}$$

    $\hspace{0.5cm}$ \textbf{Ejemplo:} una representación 1-dimensional solo puede ser equivalente a sí misma pues A conmuta y por tanto $D'(G)=D(G)AA^{-1}=D(G)$.

    Lo mismo ocurre con las representaciones de dimensión n cuyas matrices son proporcionales a la matriz unidad de orden n.

    \subsubsection{¿Cómo podemos decir si dos representaciones son equivalentes o no?}

    Para contestar habrá que buscar caracterizaciones de la representación que sean invariantes bajo transformaciones de similaridad. Por ejemplo, la traza nos ayuda a definir el carácter de la representación.

    \smallskip
    Definimos el \textbf{carácter de una representación} $\mathcal{X} (g)$ de $g\in G$ en la representación D(G) es la función:

    $$\begin{array}{cc}
         \mathcal{X}^D: \hspace{0.1cm} G \to \mathds{C} \\
         g \to \mathcal{X}^D(g)=Tr(D(g))
    \end{array}$$

    Debido a la propiedad cíclica de la traza ($Tr(D(g))=Tr(AD'(g)A^{-1})=Tr(D'(g))$), si tengo dos representaciones equivalentes la traza es independiente de la representación.

    \smallskip
    \textbf{Propiedades:}

    \begin{itemize}
        \item Dos representaciones equivalentes tienen el mismo carácter.
        \item El carácter es una función de clase, esto significa que toma el mismo valor para todos los elementos del grupo que viven en la misma clase de conjugación.

        $$\mathcal{X}(g)=\mathcal{X}(hgh^{-1})$$

        \item El carácter de la identidad es la dimensión del espacio.

        $$Tr(\mathds{1}_N)=N=dim V$$
    \end{itemize}

    \subsection{Representaciones reducibles e irreducibles}

    \textbf{Espacio invariante;} sea $D(G)=\lbrace D(g_1),...,D(g_n) \rbrace$ una representación del grupo G en el espacio vectorial V. Decimos que $V_1 \subset V$ es un subespacio invariante con respecto a la representación D(G) si la representación me deja $V_1$ invariante.

    $$\begin{array}{cc}
         D(g)v_1\subset V_1 \hspace{0.5cm} \forall \hspace{0.1cm} g \in G, \hspace{0.1cm} \forall \hspace{0.1cm} v_1\in V_1  \\
         D(G)V_1 \subset V_1
    \end{array}$$

    Dos espacios invariantes triviales son el mismo V y $\lbrace \Vec{0}\rbrace \in V$.

    \begin{itemize}
        \item Decimos que una representación D(G) en V es irreducible si V no contiene ningún subespacio invariante (no trivial) bajo D(G). En caso contrario es reducible.

        \textbf{Ejemplo:} toda representación 1-dimensional es irreducible pues un espacio vectorial de dim=1 no tiene subespacios propios.

        \item Una representación D(G) reducible es descomponible si V se puede descomponer como suma directa de dos subespacios no triviales (propios) invariantes bajo la acción de la reprsentación.

        Si es suma direta se descompone en la suma de un espacio y su complemento ortogonal. Tiene sentido descomponer la representación en dos acciones, cada una sobre uno de estos dos espacios.

        $$V=V_1 \oplus V_2\hspace{0.6cm} \forall v \in V, \hspace{0.1cm} v=v_1+v_2 \hspace{0.1cm} \text{de forma única}$$

        $$D(G)=D_1(G)\oplus D_2(G) \hspace{0.1cm} \text{con} \hspace{0.1cm} D_i(G) \hspace{0.1cm} \text{restricción de} D(G) \hspace{0.1cm} \text{a} \hspace{0.1cm} V_i$$
        $$D_i(G)V_i=D(G)V_i\subset V_i$$

        \item Una representación D(G) descomponible es \textbf{completamente reducible} si se descompone en suma directa de representaciones irreducibles.

        $$\begin{array}{cc}
             V=V_1 \oplus V_2 \oplus ... V_m  \\
             D= D_1\oplus D_2 \oplus ... D_m
        \end{array} \hspace{0.1cm} \text {con} \hspace{0.1cm} D_1(G)V_i=D(G)Vi \subset V_i \hspace{0.1cm} \forall i=1,...,m$$

        Se dice que $D_i(G)$ es irreducible en $V_i$. Matricialmente:

    $$D(G)=\left ( \begin{array}{cccc}
        D_1(g) & 0 & 0 & 0  \\
        0 & D_2(g) & 0 & 0 \\
        0 & 0 & ... & 0 \\
        0 & 0 & 0 & D_m(g)
    \end{array} \right ) \hspace{0.1cm} \forall g \in G$$

    \end{itemize}

    \textbf{Ejercicio:} sea G=($\mathds{C},+$) y sea la aplicación $D(z)=\left ( \begin{array}{cc}
        1 & z \\
        0 & 1
    \end{array} \right ) \hspace{0.1cm} \forall z\in \mathds{C}$. Demostrar que D es una representación reducible de G. ¿Es descomponible?

    \bigskip
    Será una representación si cumple los siguientes puntos:

    \begin{itemize}
        \item La identidad es z=0

        $$D(z=0)=\left ( \begin{array}{cc}
            1 &  0\\
            0 & 1
        \end{array} \right )$$

        \item  ($\mathds{C},+$) el inverso de z es -z (opuesto), se puede comprobar sin más que multiplicar que:

        $$D(-z)=(D(z))^{-1}$$

        \item $D(z_1)D(z_2)=D(z_1+Z_2)$

        $$\left ( \begin{array}{cc}
            1 & z_1 \\
            0 & 1
        \end{array}\right)\left ( \begin{array}{cc}
            1 & z_2 \\
            0 & 1
        \end{array}\right)=\left ( \begin{array}{cc}
            1 & z_1 +z_2 \\
            0 & 1
        \end{array}\right)=D(z_1+z_2)$$
    \end{itemize}

    Por otra parte

    $$D: \hspace{0.1cm} (\mathds{C},+)\to GL(2, \mathds{C}) \to V=\mathds{C_2}=lin \lbrace \Vec{e}_1,\Vec{e}_2\rbrace$$

    Es reducible ya que $W= lin \lbrace \left ( \begin{array}{c}
         1  \\
         0
    \end{array}\right) \rbrace$ es invariante bajo $D(z)\hspace{0.1cm} \forall z \in \mathds{C}$.

    $$\left ( \begin{array}{cc}
        1 & z \\
        0 & 1
    \end{array}\right) \left( \begin{array}{c}
         a  \\
         0
    \end{array}\right)=  \left( \begin{array}{c}
         a  \\
         0
    \end{array}\right)$$

    \smallskip
    Por último, ¿es descomponible? Deben ser invariantes W y su complemento ortogonal $W^\bot =lin \lbrace \Vec{e}_2 \rbrace$.

    $$\left ( \begin{array}{cc}
        1 & z \\
        0 & 1
    \end{array}\right) \left( \begin{array}{c}
         0  \\
         1
    \end{array}\right)=  \left( \begin{array}{c}
         z  \\
         1
    \end{array}\right) \notin W^\bot \hspace{0.1cm} \text{si} \hspace{0.1cm} z\neq 0$$


    Por lo que no es descomponible pues no lo es $\forall z$.

    \newpage
    \subsubsection{Suma directa de representaciones}

    Dadas dos representaciones actuando sobre espacios vectoriales distintos $D_i: \hspace{0.1cm} \to GL(V_i)\hspace{0.1cm} i=1,2$ podemos formar la suma directa $D_1\oplus D_2$ actuando sobre $V_1 \oplus V_2$.

    $V_1 \oplus V_2$ se entiende como el espacio de pares ($v_1,v_2$) tal que $v=v_1+v_2 \in V \hspace{0.1cm} \text{con} \hspace{0.1cm} v_i\in V_i$ sobre los que actúan la representación combinada como:

    $$(v_1,v_2)\to (D_1(g)v_1,D_2(g)v_2)$$

    Esto define una representación (que por construcción es reducible y descomponible pues se construye a partir de dos cosas descompuestas a priori) de mayor dimensión $dim (D_1\oplus D_2)=dim (D_1)+dim (D_2)$.

    $$(D_1 \oplus D_2)(g)=\left (\begin{array}{cc}
        D_1(g) & 0 \\
        0 & D_2(g)
    \end{array} \right)$$

    Matriz que representa a g en las bases de $V_1$ (fila 1) y $V_2$ (fila 2) respectivamente.

    \subsection{Unitariedad }

    Dado un espacio vectorial V con producto escalar, una representación de un grupo $D: \hspace{0.1cm} G \to GL(V)$ se denomina \textbf{unitaria} si $D(g)$ es un operador lineal unitario $\forall g \in G$.

    $$D(g)D(g)^{+}=D(g)^{+}D(g)=\mathds{1}_{V}, \hspace{0.1cm} \forall g\in G$$

    Esto es lo mismo que decir que, tomando el producto escalar de dos vectores es V, el producto escalar no cambia bajo la aplicación de la representación sobre dichos vectores:

    $$(u,v)=(D(g)u,D(g)v)\hspace{0.1cm} \forall u,v\in V$$

    \smallskip
    Por ejemplo, en el caso $V=\mathds{C}^n: \hspace{0.5cm} D(g)\in U(n), \hspace{0.1cm} \forall g\in G$.

    \smallskip
    Las representaciones unitarias son de gran importancia para los físicos a la hora de estudiar simetrías. Por ejemplo, en mecánica cuántica es necesario medir los observables a través de una representación que mantenga los productos escalares invariantes.

    \bigskip
    $\hspace{0.5cm}$ \textbf{Propiedad fundamental:} si una representación unitaria es reducible es completamente reducible.

    Sea $W \subset V$ un espacio invariante tal que $D(g)W\subset W, \hspace{0.2cm} \forall g\in G; \hspace{0.3cm} V=W\oplus W^\bot$. Sea $v\in W^\bot$ y $u\in W$, entonces:

    $$(u, D(g)v)=(D^+(g)u,v)=(D^{-1}(g)u,v)=(\underbrace{D(g^{-1})u}_{\in W},\underbrace{v}_{\in W^\bot})=0$$

    Luego $D(g)v\in W^\bot \to W^\bot$ invariante $\to$ D es descomponible.

    \smallskip
    Falta demostrar que es irreducible.
    Si la dimensión de D es 1 entonces es irreducible y por tanto completamente irreducible. Si dimD fuera mayor que 1 se puede demostrar por inducción que $D=D_1\oplus D_2 \oplus ... \oplus D_m$ se puede escribir como suma directa de irreducibles.

    \bigskip
    $\hspace{0.5cm}$ \textbf{Teorema de Schur-Auerbach:} toda representación D(g) de un grupo finito G sobre un espacio vectorial V con producto escalar es equivalente a una representación unitaria.

    \textit{La demostración del teorema la omito pues llegué tarde a clase}.


    \bigskip

    $$D'(g)=T^{-1}D(g)T \hspace{0.2cm} \text{Representación equivalente a D(g), unitaria respecto al producto escalar de partida}$$

    La demostración del teorema se hace para grupos finitos pues se necesita que la suma de elementos converja. El producto escalar $\bra{-}\ket{-}$ no existe en general. No obstante para grupos de Lie compactos veremos que existe una única medida dg invariante bajo la acción del grupo (a esta medida se le llama medida de Haar). Se puede hacer el cambio suma-integral, siendo esta integral convergente debido a la compacidad del grupo y por tanto la prueba sigue siendo válida.

    \newpage
    Para grupos de Lie no compactos, sus representaciones fieles de dimensión finita nunca son unitarias. No obstante, algunos grupos no compactos pueden tener representaciones unitarias con respecto a algún producto escalar no definido positivo.

    \smallskip

    Por ejemplo, el grupo de Lorentz, cuya representación de definición es finita y unitaria con respecto al producto escalar de Minkowski.

    \bigskip

    $\hspace{0.5cm}$\textbf{Teorema de Maschke:} todas las representaciones de un grupo finito o un grupo no finito pero compacto son completamente reducibles (pues las unitarias lo son). Basta con estudiar sus representaciones irreducibles.

    \subsection{Lemas de Schur (1905)}

    Antes de los lemas de Schur enunciaremos otro lema:

    \smallskip
    Sea $D: G\to GL(V)$ y $D': G \to GL(V')$ dos representaciones de G y sea el operador lineal $A: V\to V'$ que entrelaza ambas representaciones.

    $$AD(g)=D'(G)A \hspace{0.2cm} \forall  \hspace{0.1cm} g\in G$$

    Los subespacios ker(A) y A(V) son invariantes bajo D(G) y D(G') respectivamente.

    \smallskip
    \textbf{Demostración 1:} si $v\in$ ker(A), $A(D(G)v)=D'(G)(Av)=0\longrightarrow D(G)v\in$ ker(A) $\forall  \hspace{0.1cm} g\in G$.

    \smallskip
    \textbf{Demostración 2:} si $v'=Av\in A(V)$, $D'(G)v'=D'(G)(Av)=A(D(G)v)\in$ A(V) $\forall g\in V$

    \subsubsection{Lemas de Schur}

    Sean D y D' las representaciones de antes, representaciones matriciales y, por hipótesis, irreducibles; y el operador A que las entrelaza. Entonces se verifica:

    \begin{itemize}
        \item Si la dim(D)$\neq$ dim(D'), entonces A=0 forzosamente (no se pueden entrelazar más allá de la forma trivial). Por eso cuando estudiábamos representaciones equivalentes consideramos que V' era isomorfo a V.

        \item Si dim(D)=dim(D') entonces o bien A=0 o bien A es un isomorfismo en cuyo caso D es equivalente a D'.
        \item Si D=D', es decir, $AD(G)=D(G)A$ $\forall  \hspace{0.1cm} g\in G$ entonces A=$\lambda \mathds{1}$ (multiplo de la identidad). En otras palabras, si D es una representación irreducible de un grupo G y A es un operador que conmuta con todos los operadores D(g) de la representación entonces no nos queda otra más que A sea múltiplo de la identidad.
    \end{itemize}

    \smallskip

    \textbf{Proposición:} sea $D: G\to GL(V)$ una representación de un grupo G que es finito o compacto y supongamos que los únicos operadores lineales de V en V que conmutan con todos los operadores D(g) son los múltiplos de la identidad. Entonces D es irreducible.

    \smallskip
    \textbf{Proposición:} una represemtación de un grupo abeliano es irreducible si y solo si es unidimensional.

    \smallskip
    \textbf{Corolario:} Todas las representaciones irreducibles de un grupo abeliano finito o compacto son unitarias.

    \newpage
    \textbf{Ejercicio:} consideremos el grupo cíclico de tres elementos $C_3$ cuya tabla de multiplicar es:

 $$\begin{tabular}[b]{ c | c c c}

 & e & a & $a^2$  \\
\hline
e & e & a & $a^2$ \\
a & a & $a^2$ & e  \\
$a^2$ & $a^2$ & e & a
\end{tabular}$$


Una representación de este grupo es $D: C_3\to GL(3, \mathds{C})$:

$$D(e)=\left (\begin{array}{ccc}
    1 & 0 & 0  \\
     0 & 1 & 0 \\
     0 & 0 & 1
\end{array} \right ) \hspace{1.5cm} D(a)=\left (\begin{array}{ccc}
    0 & 0 & 1  \\
     1 & 0 & 0 \\
     0 & 1 & 0
\end{array} \right ) \hspace{1.5cm} D(a^2)=\left (\begin{array}{ccc}
    0 & 1 & 0  \\
     0 & 0 & 1 \\
     1 & 0 & 0
\end{array} \right )  $$

Como no es unidimensional y el grupo es abeliano ha de ser reducible, además por los teoremas de Maschke y los lemas de Schur podemos afirmar que es completamente reducible. Es decir, se puede descomponer en la suma directa de 3 representaciones irreducibles unidimensionales y unitarias.


\smallskip
Para ello observamos que los autovalores de D(a) son $\lbrace 1,e^{\frac{2\pi i}{3}}, e^{\frac{4\pi i}{3}}\rbrace$ y que por tanto existe $T\in GL(3,\mathds{C})$ (matriz de cambio de base) que me diagonaliza D(a).

    $$D(a)=T\left ( \begin{array}{ccc}
         1 & 0 & 0 \\
         0 & e^{\frac{2\pi i}{3}} & 0 \\
         0 & 0 & e^{\frac{4\pi i}{3}}
    \end{array}\right )T^{-1}$$

Y que también me diagonaliza $D(a^2)$ al ser simplemente el cuadrado de D(a):

$$D(a^2)=T\left ( \begin{array}{ccc}
         1 & 0 & 0 \\
         0 & e^{\frac{2\pi i}{3}} & 0 \\
         0 & 0 & e^{\frac{4\pi i}{3}}
    \end{array}\right )T^{-1} \hspace{0.2cm} T\left ( \begin{array}{ccc}
         1 & 0 & 0 \\
         0 & e^{\frac{2\pi i}{3}} & 0 \\
         0 & 0 & e^{\frac{4\pi i}{3}}
    \end{array}\right )T^{-1}=T\left ( \begin{array}{ccc}
         1 & 0 & 0 \\
         0 & e^{\frac{4\pi i}{3}} & 0 \\
         0 & 0 & e^{\frac{2\pi i}{3}}
    \end{array}\right )T^{-1}$$

    \smallskip
    Denotemos por $v_k$ a los autovectores de D(a), $T=(v_0 |v_1 | v_2)$ de modo que $D(a)v_k=e^{\frac{2\pi i k}{3}} v_k$.

    \smallskip
    Se tiene que $\mathds{C}^3=V_0\oplus V_1 \oplus V_2$ siendo $V_k=lin\lbrace v_k \rbrace$ invariante bajo D.

    $$D(a)=I_0 \oplus \left ( e^{\frac{2\pi i}{3}}I_1\right) \oplus \left( e^{\frac{4\pi i}{3}}I_2\right) \hspace{0.2cm} \text{con} \hspace{0.1cm} I_k \hspace{0.1cm} \text{la identidad en} \hspace{0.1cm} V_k$$

    Este espacio es isomorfo a $D^{(0)}\oplus D^{(1)}\oplus D^{(2)}$ con $D^{(k)}(a^j)=e^{\frac{2\pi jk}{3}}$

    \bigskip
    \textbf{Ejercicio:} dada la representación del grupo bidimensional

    $$\begin{array}{cc}
         D: S^1 \to DL(2, \mathds{C})  \\
         e^{i\Theta}\to D(e^{i\Theta})= \left (\begin{array}{cc}
            cos \theta  & -sen \theta  \\
            sen \theta  & cos \theta
         \end{array}\right )
    \end{array}$$

    encontrar un operador A que conmute con $\mathds{R}(\theta)$ y que no sea la identidad (tal A existe pues D no es irreducible en $V=\mathds{C}$). Encontrar los subespacios de $\mathds{C}^2$ invariantes bajo D y descomponer D en suma directa de representaciones irreducibles.

    \bigskip

    Pista, los autovectores de un D concreto son autovectores de todos los Ds, es decir son subespacios de todos los Ds.

    \bigskip
    Un ejemplo de matriz que conmuta con R($\theta$) es A=$\left (\begin{array}{cc}
        0 & -1 \\
        1 & 0
    \end{array} \right)$

    Autovalores de R($\theta$), su polinomio carácterístico es $1-2\lambda +\lambda ^2=0$. Sus autovalores son entonces:

    $$\lambda_\pm =e^{\pm i\theta}$$

    Sus subespacios propios son W=lin $\left \lbrace \left ( \begin{array}{c}
         1  \\
         \pm i
    \end{array}\right) \right\rbrace $.  Por ello, P= $ \frac{1}{\sqrt{2}}\left ( \begin{array}{cc}
         1 & 1 \\
         -i & i
    \end{array}\right) $ de modo que:

    $$R(\theta)=P\left( \begin{array}{cc}
        e^{i\theta} & 0 \\
        0 & e^{-i\theta}
    \end{array}\right)P^{+} \to D(e^{i\theta})=D_+(e^{i\theta})\oplus D_-(e^{-i\theta})$$

    donde $D_+(e^{i\theta})=e^{i\theta}\mathds{1}$ y $D_-(e^{-i\theta})=e^{-i\theta}\mathds{1}$.

    \subsection{Relaciones de ortonormalidad y completitud}

    Sea un grupo finito o compacto y sean $D^{(\rho)}(G)=\lbrace D^{(1)}(G), D^{(2)}(G)\rbrace$ sus representaciones irreducibles inequivalentes (y que podemos tomar unitarias) etiqueadas por el índice discreto $\rho$, con dimensión dim D$^{(\rho)}(G)$=$d_\rho <\infty$.

    Sea $D^{(\rho)}(G)$ la matriz en una base ortonormal que representa a $g\in G$ en la representación $D^{(\rho )}(G)$. Estas matrices cumplen la relación de ortonormalidad:

    $$\text{G finito:} \hspace{0.3cm} \frac{1}{|G|}\sum _g D_{ij}^\rho (g) \bar{D}_{i'j'}^{\rho '}(g)=\frac{1}{d \rho}\delta _{\rho , \rho '}\delta _{i, i'}\delta _{j,j'}$$

    $$\text{G compacto:} \hspace{0.3cm} \frac{1}{v(G)}\int _G d\mu (g) D_{ij}^{(\rho )} (g) \bar{D}_{i'j'}^{ (\rho ')}(g)=\frac{1}{d\rho}\delta _{\rho , \rho '}\delta _{i, i'}\delta _{j,j'}$$

    donde v(G) es el volumen del grupo (si este es compacto). Estas relaciones se pueden usar para construir representaciones irreducibles a partir de otras repreentaciones ireducibles conocidas.

    \smallskip

    \textbf{Ejemplo:} consideremos el grupo de Klein $V_4=C_2 \times C_2$ de elementos $\lbrace e, \rho , \sigma , \tau \rbrace $, que es abeliano con $\sigma ^2 = \rho ^2 = \tau ^2 =e^2 =e$, $\sigma \rho = \tau$ subgrupos normales que nos permiten definir el grupo cociente:

    $$V_4/V_\sigma \cong \frac{V_4}{V_\tau}\cong V_4/V_\rho \cong C_2 $$

    $$\begin{array}{c}
         V_\sigma =\lbrace e, \sigma \rbrace \cong C_2  \\
         V_\tau = \lbrace e, \tau \rbrace \cong C_2 \\
         V_\rho = \lbrace e, \rho \rbrace  \cong C_2
    \end{array}$$

    \smallskip
    Consideramos por ejemplo $V_4/V_\sigma =\lbrace \lbrace e, \sigma \rbrace , \lbrace e, \tau \rbrace \rbrace$.
    \bigskip

    Construyamos representaciones irreducibles de $V_\sigma $ y con ellas las de $V_4$. La más fácil es la representación trivial $D^{(1)}=D^{(1)}(e)=1$, $D^{(1)}(\sigma)=1$.

    \smallskip
    La relación de ortonormalidad para $\rho =1$ y $\rho '=2$ implica:

    $$D^{(1)}(e)D^{(2)}(e)+D^{(1)}(\sigma)D^{(2)}(\sigma)=0 \to D^{2}(e)=1, D^{(2)}(\sigma)=-1$$

    Por consiguiente, al ser ambos grupos isomorfos:

    $$V_4/V_\sigma =\lbrace eV_\sigma, \tau V_\sigma \rbrace =\lbrace E, \Sigma \rbrace; \hspace{0.5cm} E=\lbrace e, \sigma \rbrace; \Sigma = \lbrace e, \tau \rbrace$$

    $$D^{(1)}(E)=1, \hspace{0.2cm} D^{(1)}(\Sigma)=-1, \hspace{0.2cm} D^{(2)}(E)=1, \hspace{0.2cm} D^{(2)}(\Sigma)=-1, \hspace{0.2cm}$$

    $$D^{(1)} \hspace{0.1cm} \text{de} \hspace{0.1cm} V_4/V_\sigma \to D^{(1)}(e)=1, D^{(1)}(\rho)=1, D^{(1)}(\tau)=1, D^{(1)}(\sigma)=1$$

    $$D^{(2)} \hspace{0.1cm} \text{de} \hspace{0.1cm} V_4/V_\sigma \to D^{(2)}(e)=1, D^{(2)}(\rho)=-1, D^{(2)}(\tau)=-1, D^{(2)}(\sigma)=1$$

    Y lo mismo para $V_4/V_\sigma$ y $V_4/V_\tau$.

    \newpage
    Se puede ver que estas representaciones para $V_4$ verifican que las relaciones de ortonormalidad son las únicas permitidas por tales relaciones:

    $$\begin{tabular}[b]{ c | c c c c }

g/w & e & $\sigma $ & $\rho $ & $\tau $\\
\hline
1 & 1 & 1 & 1 & 1 \\
2 & 1 & 1 & -1 & -1 \\
3 & 1 & -1 & 1 & -1 \\
4 & 1 & -1 & -1 & 1
\end{tabular}  $$

    Las matrices de representación cumplen la siguiente relación de completitud:

    $$\text{G finito:} \hspace{0.3cm} \frac{1}{|G|}\sum _\rho d\rho \sum _{ij} \underbrace{D_{ij}^{(\rho)}\bar{D}_{ij}^{(\rho ')}(g')}_{\mathcal{X}^{(\rho)}(gg'^{-1})}=\delta _{\rho, \rho '}$$

    Se tiene para grupos finitos, haciendo g=g':

    $$\sum _\rho d\rho ^2 =|G|^2$$

    Y para grupos compactos:

     $$\text{G compacto:} \hspace{0.3cm} \frac{1}{v(G)}\sum _\rho d\rho \sum _{ij} \underbrace{D_{ij}^{(\rho)}\bar{D}_{ij}^{(\rho ')}(g')}_{\mathcal{X}^{(\rho)}(gg'^{-1})}=\delta (g,g')$$

    donde $\delta (g,g')$ es la delta de Dirac adaptada a la medida de Haar del grupo

    $$f(g')=\int _G d\mu (g)\delta (g,g')f(g)$$

    Por tanto, esta relación de completitud nos dice que cualquier función $f: G \to \mathds{C}$ continua o de cuadrado sumable (integrable) puede expandirse en funciones $D^{(\rho)}_{ij}(g)$.

    \smallskip
    \textbf{Teorema de Peter-Weyl:}

    $$f(g)=\sum _{g'}\delta _{g,g'}f(g')=\sum _{\rho,i,j}D_{ij}^{(\rho)}(G)\sum _g \frac{1}{|G|}D_{ij}^{(rho)+}(g')f(g')=\sum _{\rho ,i, j} d\rho D_{ij}^{(\rho)}(g)f_{ij}^{(\rho)}$$

    \smallskip
     En el caso en el que G=$S^1\cong U(1)$ recuperamos la descomposición de Fourier:

     $$\int _G d \mu (G)=\int ^{2\pi}_0 d\theta$$

    \subsubsection{Representaciones de ortonormalidad y completitud con caracteres}

    Mientras que las matrices de representación dependen de la base escogida los caracteres no lo hacen y además no cambian dentro de la misma clase de conjugación. Por tanto es más útil expresar las relaciones de ortonormalidad y completitud en función de estos caracteres.

         $$\text{G finito:} \hspace{0.3cm} \frac{1}{|G|}\sum _g\mathcal{X}^{(\rho)}(g)\mathcal{X}^{(\rho ')}(g)=\delta _{\rho , \rho '}$$

         $$\frac{1}{|G|}\sum _{i=1}^m |e_i|\mathcal{X}_i^{(\rho)}(g)\bar{\mathcal{X}}_i^{\rho '}(g')=\delta _{\rho , \rho '}$$

         donde m es el numero de clases de conjugación, $|e_i|$ es el número de elementos en la clase $e_i$ y $\mathcal{X}_i^{(\rho)}$ el caracter de la representación.

         \textbf{Se tiene que el numero de representaciones irreducibles equivalentes es el número de clases de conjugación.}

          $$\text{G compacto:} \hspace{0.3cm} \frac{1}{v(G)}\int _G d\mu (g) \mathcal{X}^{(\rho)} (g)\mathcal{X}^{(\rho  ')}(g)=\delta _{\rho , \rho '}$$

         $$\frac{1}{v(G)}\sum _\rho d \rho \mathcal{X}_i^{(\rho)} \bar{\mathcal{X}}_i^{\rho'} (g') =\delta (g,g') $$


  \subsubsection{Tabla de caracteres:}

  \smallskip
  $\mathcal{X}_i^{(\rho)}$ con $\rho $=1,...,m y i=1,...,m puede usarse como una matriz o tabla cuadrada con $\rho$ el índice de la fila e i el de la columna.

  \smallskip

  \textbf{Ejemplo:} Para grupos abelianos cada elemento del grupo forma una clase por si mismo y todas las representaciones irreducibles son unidimensionales. De este modo, las tablas $D^{(\rho)}(g)$ también son tablas de caracteres.
\smallskip

    \textbf{Consecuencias}(de las propiedades de los caracteres):

    \begin{itemize}
        \item Debido a que cualquier representación es totalmente reducible, $D= \oplus _\rho m_\rho D^{(\rho)}$, el carácter se puede descomponer según:

        $$\mathcal{X}=\sum _{\rho} m_\rho \mathcal{X}^{(\rho)}$$

        Las multiplicidades vienen dadas a partir de los caracteres como:

        $$m_\rho =\frac{1}{|G|}\sum _i |e_i|\mathcal{X}\bar{\mathcal{X}}_i^{(\rho)}=\frac{1}{v(G)}\int _G d_\mu (g)\mathds{X}(g)\Bar{\mathcal{X}}^{(\rho)}(g)$$

        \item También es cierto que:

        $$||\mathcal{X}||^2=\frac{1}{|G|}\sum _g|\mathcal{X}(g)|^2=\sum _\rho m_\rho ^2$$

        y una representación es irreducible si y  solo si  $||\mathcal{X}||^2=1$.

        \item Dos representaciones de un grupo finito o compacto son equivalentes si y solo si tienen los mismos caracteres.

        \item \textbf{Peter Weyl}: cualquier función de clase se puede expandir en caracteres irreducibles (una función es de clase si es invariante bajo conjugación).

    \end{itemize}

    \subsection{Producto tensorial de representaciones y coeficientes de Clebsch-Gordan}

    Un método habitual para construir representaciones irreducibles de un grupo dado consiste en construir el producto tensorial de representaciones conocidas y descomponerlo en irreducibles.

    \subsubsection{Tensores}

    Dados dos espacios vectoriales $V_1$ y $V_2$ podemos formar su producto directo o producto tensorial $V_1 \otimes V_2$.

    Dadas bases $\lbrace v_i\rbrace _{i=1}^{dim V_1}$ y $\lbrace w_j\rbrace _{j=1}^{dim V_2}$ de $V_1$ y $V_2$ respectivamente, entonces una base de $V=V_1 \otimes V_2$ está dada por el conjunto de pares ordenados $\lbrace v_i \otimes W_j \rbrace$ es decir los vectores de V son todas las combinaciones lineales de los elementos de la base de la forma:

    $$\sum _{i=1}^{d_1=dim V_1}\sum _{j=1}^{d_2=dimV_2} a^{ij}(v_i\otimes w_j)=a^{ij}(v_i \otimes w_j)$$

    donde en el último paso utilizamos el criterio de suma de índices repetidos. Están caracterizados por las componentes:

    $$a^{ij}=\left ( \begin{array}{ccc}
        a^{11} & ... & a^{1d_2} \\
        ... & ... & ... \\
        a^{d_11} & ... & a^{d_1d_2}
    \end{array}\right) \hspace{0.2cm} \text{tensor de rango 2 (puede entenderse como una matriz)}$$

    \smallskip

    Es posible generalizar esta definición para tensores de mayor rango. Claramente:
    \begin{center}
    dimV=dim$V_1 \cdot $ dim $V_2$.

    \end{center}

    \textbf{Nota:} si agrupamos índices $(i,j)=k$ con k=1,...,$d_1d_2$ podríamos llamar $a^{ij}=a^k$

    \subsubsection{Producto tensorial de operadores}

    Dados dos operadores $D_1$ y $D_2$ actuando sobre los espacios $V_1$ y $V_2$ respectivamente, podemos definir su producto tensorial actuando sobre $V_1 \otimes V_2$ como:

    $$(D_1\otimes D_2)(v\otimes w)=D_1v \otimes D_2w  \hspace{0.1cm} \in V_1 \otimes V_2$$

Usando los elementos de matriz $(D_1)^j_i$, $(D_2)^m_l$ (índices abajo son fila e índices arriba son columna) de estos operadores en sus respectivas bases podemos obtener los elementos de matriz del producto tensorial $D_1 \otimes D_2$ en la base $\lbrace v_i\otimes w_j \rbrace$.

$$(D_1 \otimes D_2)^{nm}_{ij}=(D_1)^j_i (D_2)^m_l$$

La acción de $(D_1\otimes D_2)$ sobre V entonces viene dada por:

$$D_1 \otimes D_2: \hspace{0.2cm} a^{ij} (v_i\otimes w_j) \to ((D_1 \otimes D_2)^{ij}_{nm}a^{nm})v_i\otimes w_j$$

    $$a^k \to (D_1\otimes D_2)^ka^N \hspace{0.2cm} \text{Regla usual de transferencia de vectores}$$

\subsubsection{Producto tensorial de representaciones}

Sean $D_1$ y $D_2$ representaciones de un grupo G sobre los espacios vectoriales $V_1$ y $V_2$ el producto tensorial $D_1 \otimes D_2$ da lugar a otra representación de G de dimension D= dim $D_1 \cdot$ dim $D_2$.

\smallskip
\textbf{Propiedad:} el carácter en la representación producto tensorial es el producto de los caracteres en cada una de las representaciones de partida.

$$\mathcal{X}_D(g)=\mathcal{X}_{D_1}(g)\mathcal{X}_{D_2}(g)$$

\subsubsection{Descomposición de Clebsch-Gordan}

EL producto tensorial de dos representaciones irreducibles D y D' en general no es irreducible; sí es completamente irreducible (como es el caso para representaciones unitarias). De este modo podemos llevar a cabo la descomposición en suma directa de irreducibles o descomposición de Clebsch-Gordan.

$$D\otimes D'=\oplus _j D_j$$

siendo $D_j$ representaciones irreducibles y $\oplus _j$ indica suma directa de los j elementos.

\smallskip
Si G es finito o compacto y clasificamos con un índice discreto $\rho$ sus representaciones irreducibles inequivalentes, entonces tendremos:

$$D^{(\sigma)}\otimes D^{(\tau)}=\uplus _\rho m_\rho ^{\sigma \tau} D^{(\rho)}$$

donde $m_\rho ^{\sigma \tau}$ es la multiplicidad de $D^{(\rho)}$ en esta descomposición.

$$m_\rho ^{\sigma \tau} =\frac{1}{|G|} \sum _g \mathcal{X}^{(\sigma)}(g)\mathcal{X}^{(\tau)}\bar{\mathcal{X}}^{(\rho)}(g)$$

$$\mathcal{X}^{(\sigma)}(g)\mathcal{X}^{(\tau)}= \sum _\rho m_\rho ^{\sigma \tau}\mathcal{X}^{(\rho)}$$

$$dimD^{(\sigma)}dim D^{(\tau)}=\sum _\rho m_\rho ^{\sigma \tau} dimD^{(\rho)}$$

O bien en vez de la suma a los $g_s$ la integral $\frac{1}{v(G)}\int _G d_\mu (g)$ si el grupo es compacto.

\smallskip
\textbf{Proposición:} la representación trivial aparece en el producto $D^{(\sigma)}\otimes D^{(\tau)}$ si y solo si $D^{(\tau)}= \bar{D}^{\sigma}$.

\smallskip
\textbf{Ejercicio:} dada la representación irreducible unitaria $D^{(2)}$ de $S_3$, construye $D^{(2)}\otimes D^{(2)}$ y descomponlas en suma directa de representaciones irreducibles (recordar que eran: la identidad, $D^{(0)}$; la paridad $D^{(1)}$ y la que teniamos que contruir para el ejercicio $D^{(2)}$).

$$D^{(2)}\otimes D^{(2)}=m_{(0)}D^{(0)}+m_{(1)}D^{(1)}+m_{(2)}D^{(2)}$$

\textbf{Ejercicio:}

    \smallskip
    Tabla de caracteres de $S_3$:

    \smallskip
    $S_3$ tiene 3 clases de conjugación: $C_1=\lbrace e \rbrace, C_2= \lbrace \tau _3=(12),\tau _1=(23), \tau _1=(31)\rbrace, C_3=\lbrace \sigma _1=(123),\sigma _2=(321)$. Tiene por tanto tres representaciones irreducibles no equivalentes.

    \begin{itemize}
        \item La trivial, que siempre existe $D^{(0)}$ (unidimensional).

        $$\mathcal{X}^{(1)}_1=1 \hspace{0.2cm} \mathcal{X}^{(1)}_2=1 \hspace{0.2cm} \mathcal{X}^{(1)}_3=1 \hspace{0.2cm}$$

        \item $S_3/A_3 \cong C_2$ tiene una representación fiel (asignar a un elemento el 1 y a otro el -1) llamada representación paridad $D^{(1)}$.

        $$D^{(1)}(e)=D^{(1)}(\sigma _1)=D^{(1)}(\sigma _2)=1$$

        $$D^{(1)}(\tau _1)=D^{(1)}(\tau _2)=D^{(1)}(\tau _3)=-1$$

        $$\mathcal{X}^{(1)}_1=1 \hspace{0.2cm}\mathcal{X}^{(1)}_2=-1 \hspace{0.2cm}\mathcal{X}^{(1)}_3=1 \hspace{0.2cm}$$

    \item Nos falta saber los caracteres de la tercera representaión irreducible $D^{(2)}$. Primero calculamos su dimensión:

    $$|G|=\sum  _\rho d_\rho ^2 \to 6=1+1+d_{(2)}^2 \to d^2_{(2)}=4$$

    Luego la tercera representación es bidimensional (matrices 2 $\times$ 2).

    $$D^{(2)}(e)= \left ( \begin{array}{cc}
       1 1 & 0 \\
        0 & 1
    \end{array} \right); \hspace{0.5cm} \mathcal{X}_1^{(2)}=2$$

    No conocemos sin embargo el resto de $D^{(2)}(g)$.

    $$\begin{tabular}[b]{ c | c c c  }
 & $G_1$ & $G_2$ & $G_3$ \\
\hline
$\mathcal{X}^{(0)}$ & 1 & 1 & 1 \\
$\mathcal{X}^{(1)}$ & 1 & -1 & 1  \\
$\mathcal{X}^{(2)}$ & 2 & x & y
\end{tabular}$$

    De las relaciones de completitud:

    $$\frac{|e_i|}{|G|}\sum _\rho \mathcal{X}_i^{(\rho)}\Bar{\mathcal{X}}_j^{(\rho)}=\delta _{ij}$$

    $$0=1\cdot 1+1(-1)+ 2x \to x=0$$
    $$0=1\cdot 1+1\cdot 1 +2y \to y=-1$$

    Podrian haberse utilizado las relaciones de ortonormalidad:

    $$\frac{1}{|G|}\sum _{i=1}^m |e_i|\mathcal{X}_i^{(\rho)}\mathcal{X}_i^{(\rho ')}=\delta _{\rho , \rho '}$$

    $$0=1\cdot 2+3\cdot 1\cdot x+2\cdot 1\cdot y; \hspace{0.3cm} \rho =(0), \rho '=(2)$$
    $$0=1\cdot 2-3\cdot 1\cdot x +2 \cdot 1\cdot y; \hspace{0.3cm} \rho =(1),\rho '=(2)$$
    $$x=0, \hspace{0.2cm} y=-1$$
    \end{itemize}

    \textbf{Ejercicio:} construir la representación bidimensional irreducible y unitaria de $S_3$.

    \newpage

    \subsubsection{Coeficientes de Clebsch-Gordan}
La descomposición de CG describe como se descomponen las matrices de representación en representaciones irreducibles bajo la acción de un grupo. También es importante saber como se descomponen los vectores del espacio vectorial de representación.

Sea $\lbrace v_\alpha (\sigma)\rbrace^{dim D(\sigma)}_{\alpha =1}$ una base ortonormal de vectores del espacio $V_\tau$ sobre el que actúa $D^{(\sigma)}$. Queremos expandir el producto tensorial

$$V_i^{(\sigma)}\otimes V_J^{(\rho)}=\sum _\tau \sum _k ...V_k^{(\tau)}$$

Como la representación $D^{(\tau)}$ podrá aparecer $m_\tau \rho ^\sigma$ veces introducimos un índice extra $a=1,..., m_\tau \rho ^\sigma$ tal que:

$$V_i^{(\rho)}\otimes V_j^{(\sigma)}=\sum _{\tau , a, k}C_{\rho , i, \sigma, j | \tau, a, k}V_k^{(\tau)}$$

En notación Dirac:

$$\ket{\rho, i, ,\sigma , j}\equiv \ket{\rho, i} \otimes \ket{\sigma , j}=\sum _{\tau , a,k}\bra{\tau , a, k}\ket{\rho, i,  \sigma ,j } \ket{\tau , a, k}$$

Los coeficientes de CG se obtienen por identificación:

$$C_{\rho , i, \sigma, j | \tau, a, k}=\sum_{\tau , a,k}\bra{\tau , a, k}\ket{\rho, i,  \sigma ,j } $$

     \smallskip

     Si las representaciones son unitarias y las bases se eligen ortonormales, los coeficientes de CG cumplen las relaciones de ortogonalidad y por tanto:


    $$\sum_{\tau , a,k}\bra{\tau , a, k}\ket{\rho, i,  \sigma ,j } \cdot \underbrace{\bra{\rho, i',  \sigma ,j '}}_{\text{Complejo conj}} \ket{\tau , a, k}=\delta _{i,i'}\delta _{j,j'}$$

       $$\sum_{\tau , a,k}\bra{\tau , a, k}\ket{\rho, i,  \sigma ,j } \cdot {\bra{\rho, i,  \sigma ,j }} \ket{\tau ' , a', k'}=\delta _{a,a'}\delta _{\tau,\tau '}\delta _{k, k'}$$


       Y podemos invertir la relación original

       $$\ket{\tau, a, k}=\underbrace{\sum _{ij} \bra{e,i, \sigma , j} \ket{ \tau , a ,k}}_{\text{Coeficiente conj.}} \underbrace{\ket{e,i,\sigma ,j}}_{Prod. espacios}$$

            \subsubsection{Ejercicio: calcular los coeficientes de CG para $D^{(2)}\otimes D^{(2)}$}

            Tenemos el espacio $V=\lbrace f(x,y)=ax^1x^2 + bx^1y^2 + c x^2y^1 +d y^1y^2\rbrace$ producto directo de $\mathds{C}^2$ con $\mathds{C}^2$.


        Dentro de este espacio se pueden encontrar subespacios invariantes de forma que pasamos a escribirlo como:


        $$V=Lin \lbrace \frac{1}{\sqrt{2}} (x^1x^2 + y^1y^2) \rbrace \oplus Lin \lbrace \frac{1}{\sqrt{2}}(x^1y^2 - y^1x^2)\rbrace \oplus Lin \lbrace \frac{1}{\sqrt{2}}(x^1y^2 - y^1x^2), \frac{1}{\sqrt{2}}(x^1y^2 + y^1x^2) \rbrace$$


    De forma que la matriz de representación es:

    $$\Tilde{W}(S_3)=P^tWP$$

    Con P:

    $$P=\frac{1}{\sqrt{2}}\left (\begin{array}{cccc}
         1 & 0 & 1 & 0  \\
         0 & 1 & 0 & 1  \\
         0 & -1 & 0 & 1  \\
         1 & 0 & -1 & 0
    \end{array} \right)$$


        Los coeficientes de CG se obtienen por comparación entre los vectores antiguos y los nuevos:

        $$x^1x^2=\frac{1}{\sqrt{2}}(u_1+u_3) \to C_{D^2,1,D^2,1| D^0, 1}=\frac{1}{\sqrt{2}}$$

         $$x^1y^2=\frac{1}{\sqrt{2}}(u_2+u_4)$$

         $$y^1x^2=\frac{1}{\sqrt{2}}(-u_2+u_4)$$

          $$y^1y^2=\frac{1}{\sqrt{2}}(u_1-u_3)$$

           Notación $C_{\rho, i, \sigma ,j | \tau , k}$; $\rho$ y $\sigma$ indican las representaciones ($D^{(2)}$ en este caso), i y j son el vector concreto de las representaciones que $\rho$ y $\sigma$ que estamos usando y $\tau$ me indica que representación es.

           Por ejemplo $C_{D^2,,D^2, |D^0 ,1}$ es el coeficiente que me lleva del producto tensorial $D^{(2)}$ con $D^{(2)}$ a la representación trivial $D^{(0)}$, los números son el vector que estamos cogiendo en cada caso, es decir el 1 es $u_1$ y los 1,2 son los $x^1,x^2$. Así se sacan los demás.



      \subsection{Teorema de Wigner-Eckart}

       \begin{itemize}
           \item \textbf{Conjunto de operadores tensoriales irreducibles.}

           Sean $V_\rho$ y $V_\sigma$ espacios vectoriales con producto escalar y $D^{(\rho)}$, $D^{(\sigma)}$ representaciones irreducibles de un grupo G sobre dichos espacios respectivamente. Sea Q: $V_\rho \to V_\sigma$ un operador lineal ( $Q \in L(V_\rho ,C_\sigma)$) de modo que  $Qv\in V_\sigma \hspace{0.2cm} \forall v\in V_\sigma$.

           El conjunto de tales operadores forma un espacio vectorial con suma $(Q_1+Q_2)V=Q_1v+Q_2v$, producto escalar complejo $(aQ)v=a(Qv)$ y cero $0v=\Vec{0}$. Si las dimensiones de $V_\sigma, V_\rho$ son $d_\sigma, d_ \rho$, la dimensión de este nuevo espacio L es $d_ \sigma d_\rho$.

           Definamos ahora, para cada $g\in G$ un operador D'(g) que actúa en L de la siguiente forma:

           $$D'Q=DQD^{-1} \hspace{0.2cm}\forall Q\in L$$

           Entonces D' es un operador lineal y dados $g_1, g_2 \in G$ se tiene:

           $$D'(g_1)D'(g_2)=D'(g_1g_2); \hspace{0.4cm} (g_1g_2)^{-1}=g_1^{-1}g_2^{-1}$$

           Por tanto, el conjunto de operadores D' da lugar a una representación de G sobre el espacio vectorial L, en general reducible.

           Supongamos que D'(G) es completamente reducible y que $D^{(\tau)}$ es una representación de las representaciones irreducibles que aparece en su reducción. Sea $\lbrace Q_1,Q_2,...,Q_{d_\tau}\rbrace$ una base del subespacio correspondiente de L donde actúa $D^{(\tau)}$. Entonces:

           $$D'Q_i=\sum _{j=1}^{d_\tau} D_{ji}Q_j \hspace{0.2cm} \forall i=1,..., d_\tau$$

           y por la definición dada de D':

           $$DQ_iD^{-1}=\sum _{j=1}^{d_\tau} D_{ji}Q_j \hspace{0.2cm} \forall i=1,..., d_\tau$$

           este conjunto $\lbrace Q_i\rbrace$ de operadores se llama conjunto de operadores tensoriales irreducibles de la representación $D^{(\tau)}$ de G.
       \end{itemize}

       \smallskip
       \textbf{Teorema de Wigner-Eckart:}

\smallskip
Sea G finito o compacto, sean $D^{(\sigma)}, D^{(\tau)}$ y $D^{(\rho)}$ representaciones unitarias irreducibles de G de dimensiones $d_\sigma,d_\tau, d_\rho$ respectivamente y sean $\lbrace v_i^{\sigma}\rbrace, \lbrace v_i^{\rho}\rbrace $ dos bases ortonormales asociados a sus respectivos espacios $V_\sigma, V_\rho$ sobre los que están definidas estas representaciones unitarias.

Finalmente, sea $\lbrace Q_k^{(\tau)} \rbrace _{k=1}^{d_\tau}$ un conjunto de operadores irreducibles de $D^{(\tau)}$. Entonces:

$$(V_j^{(\sigma)}, V_k^{(\tau)}, V_i^{(\rho)})=\sum _{a=1}^{m_\sigma ^{\rho \tau}} \bar{C}_{\rho, i, \tau ,k | \sigma , a, j}\cdot (\sigma || Q^{(\tau)}||\rho)_a$$

donde el último término forma un conjunto de $m^{\rho \tau}$ (elementos de matriz reducidos) que son independientes de i,j y k.

Este teorema muestra que la dependencia en estos índices de las cantidades $(V_j^{(\sigma)}, V_k^{(\tau)}, V_i^{(\rho)})$ está completamente recogida en los coeficientes de CG y que la totalidad del conjunto $D_\sigma d_\rho d_\tau$ que forman depende solo de $m^{\rho \tau}_\sigma$ elementos de matriz reducidos.

\smallskip
\textbf{Nota:} se puede generalizar para mucgos grupos no compactos, como grupos de Lie semisimples con representación finita o con representación unitaria de dimensión infinita.
Las reglas de selección atómicas salen de aplicar este teorema al grupo SU(2).

\subsection{Representaciones del producto directo de grupos}

Sean $D_1(G_1)$ y $D_2 (G_2)$ representaciones de los grupos $G_1, G_2$ respectivamente. El conjunto de operadores $D((g_1,g_2))$ por:

$$D((g_1,g_2))=D_1(g_1)\otimes D_2(g_2)$$

da lugar a una representación del producto directo $G_1\times G_2$ que es unitaria si $D_1(G_1)$ y $D_2(G_2)$ lo son y son fieles si son representaciones irreducibles de $G_1$ y $G_2$ respectivamente entonces D es una representación irreducible de $G_1 \times G_2$. Además, cada representación irreducible de $G_1 \times G_2$ es equivalente a una construida de esta forma.

\newpage
\subsection{Ejercicios}

\begin{enumerate}
    \item Dada una representación de un grupo sobre los complejos, los vectores de $\mathds{C}$ que se transforman bajo la acción de D:

    $$D: \hspace{0.2cm} G \to GL(n,\mathds{C})$$
    $$\Vec{x}\to \Vec{x}'-D(g)\Vec{x}$$

    Si consideramos funciones de $\mathds{C}^n$ en $C$ tales como $f'(\Vec{x})=f(\Vec{x})$ tenemos que $f(D(g)\Vec{x})=f(\Vec{x})$ y concluimos que:

    $$f\longrightarrow f'$$
    $$f'(\Vec{x})=f(D^{-1}(g)\Vec{x})[A]$$

    Demostrar que este mapa es un homeomorfismo en el espacio de funciones y que por tanto el conjunto de transformaciones [A] forman una representación del grupo sobre dicho espacio.

    $$\begin{array}{cc}
         D: \hspace{0.2cm} \Vec{x}\to \Vec{x}'  \\
         D': \hspace{0.2cm} f(\Vec{x})\to f'(\Vec{x})
    \end{array} \hspace{2cm} \text{De forma que} \hspace{0.2cm} f'\equiv D'(f)$$

    \smallskip
    $\hspace{1cm}$ Para demostrar que este mapa define una representación del grupo G debemos probar que respeta la estructura de grupo, es decir, que si $g=g''g'$ entonces $f\overset{g}{\longrightarrow} f''$ coincide con la composición de $f \overset{g'}{\longrightarrow} f'$ con $f'\overset{g''}{\longrightarrow}f''$. Tenemos que:

    $$f'(\Vec{x})=f(D^{-1}(g),\Vec{x})$$
    $$f''(\Vec{x})=f'(D^{-1}(g), \Vec{x})=f(D^{-1}(g')D^{-1}(g''),\Vec{x
    })=f(D(g''g')^{-1},\Vec{x})=f(D((g)^{-1},\Vec{x})$$

    Como queríamos demostrar.

    \item Consideramos la representación bidimensional irreducible de $S_3 \cong D_3$, denotada anteriormente por $D^{(2)}(S_3)$, y dos vectores en el espacio complejo de dos dimensiones de coordenadas $(x^1,y^1)$ y $(x^2,y^2)$ que se transforman independientemente bajo la acción de la representación D. Esta representación da lugar a una representación de dimensión 4 W($S_3$) dada por la envolvente lineal de los monomios $X^1x^1, x^1y^2,y^1x^2$ y $y^1y^2$.

    \begin{enumerate}
        \item Calcular las matrices de representación W(g) siendo g un elemento de $S_3$.

        $$f: \hspace{0.2cm} (x^1,x^2)\otimes (y^1,y^2) \to \mathds{C}$$
        $$\hspace{0cm} V_1 \hspace{0.5cm}\otimes \hspace{0.5cm} V_2 $$

        Por ejemplo $\tau _3 \tau _1=\sigma _1$, $\sigma _1^{-1}=\sigma _2$, $\tau _1 \sigma _1=\tau _2$.

        Partiendo de $D^{(2)}(\tau _1)=\left ( \begin{array}{cc}
           -1  & 0 \\
            0 & 1
        \end{array}\right)$ y $D^{(2)(\tau _3)}=\frac{-1}{2}\left ( \begin{array}{cc}
           -1  & \sqrt{3} \\
            \sqrt{3} & 1
        \end{array}\right)$.

        Con estas dos podemos construir $W(\tau _1)$ y $W(\tau _3)$, y con ellas podemos construir todas las demás mediante la tabla de multiplicación:

        $$\tau _1: \begin{array}{cc}
            x^1\to -x^1 & x^2\to -x^2  \\
            y^1\to y^1 & y^2 \to y^2
        \end{array}$$

        $$x^1x^2\overset{\tau _1}{\to} (x^1x^2)^1=x^1x^2$$
        $$x^1y^2\overset{\tau _1}{\to} (x^1y^2)^1=-x^1y^2$$
        $$y^1x^2\overset{\tau _1}{\to} (y^1x^2)^1=-y^1x^2$$
        $$y^1y^2\overset{\tau _1}{\to} (y^1y^2)^1=y^1y^2$$

        $$V=lin\lbrace x^1x^2,x^1y^2,y^1x^2,y^1y^2 \rbrace; \hspace{1cm} f'(\Vec{x})=W(\tau _1)f(\Vec{x})=f(W(\tau _1)^{-1}\Vec{x})$$

        Ahora tomando la base ortonormal más sencilla:

        $$x^1x^2=\left( \begin{array}{c}
             1  \\
             0 \\
             0 \\
             0
        \end{array} \right ); \hspace{0.2cm} x^1y^2=\left( \begin{array}{c}
             0  \\
             1 \\
             0 \\
             0
        \end{array} \right ); \hspace{0.2cm} y^1x^2=\left( \begin{array}{c}
             0  \\
             0 \\
             1 \\
             0
        \end{array} \right ); \hspace{0.2cm} y^1y^2=\left( \begin{array}{c}
             0  \\
             0 \\
             0 \\
             1
        \end{array} \right )$$

        Queda la representación:

        $$W(\tau _1)=\left (\begin{array}{cccc}
           1  & 0 & 0 & 0 \\
           0  & -1 & 0 & 0 \\
             0 & 0 & -1 & 0 \\
             0 & 0 & 0 & 1
        \end{array} \right)$$

        Ahora, para $W(\tau _3)$ tenemos que sigue la siguiente relación:

        $$\left ( \begin{array}{c}
             x^{(i)^1}  \\
               y^{(i)^1}
        \end{array}\right) = D^{(2)} \left ( \begin{array}{c}
             x^{(i)}  \\
               y^{(i)}
        \end{array}\right)=\frac{-1}{2}\left ( \begin{array}{cc}
            -1 & \sqrt{3} \\
               \sqrt{3} & 1
        \end{array}\right)\left ( \begin{array}{c}
             x^{(i)}  \\
               y^{(i)}
        \end{array}\right)= \left ( \begin{array}{cc}
             \frac{x^{(i)}}{2} & \frac{-\sqrt{3}}{2}y^{(i)}  \\
             \frac{-\sqrt{3}}{2}y^{(i)}  & \frac{-1}{2}y^{(i)}
        \end{array}\right)$$

        Por lo que las transformaciones son del estilo:

        $$x^1x^2 \overset{\tau _3}{\longrightarrow} (x^1 x^2)^1=\left ( \frac{x^1}{2}-\frac{\sqrt{3}}{2}y^1 \right) \left ( \frac{x^2}{2}-\frac{\sqrt{3}}{2}y^2 \right)$$

        Y los elementos de la base se transforman a:

        $$(x^1x^2) \longrightarrow (x^1x^2)^1$$

        $$\left( \begin{array}{c}
             1  \\
             0 \\
             0 \\
             0
        \end{array} \right )\longrightarrow \frac{1}{2} \left( \begin{array}{c}
             1  \\
             -\sqrt{3} \\
             -\sqrt{3}\\
             3
        \end{array} \right )$$

        Repitiendo este procedimiento podemos hallar la transformación para cada elemento de la base de V. Por tanto, es posible hallarnos $W(\tau _3)$ y con ella obtener $W(\tau _2)$, $W(\sigma _1)$ y $W(\sigma _2)$ mediante el producto adecuado de matrices.
    \end{enumerate}
\end{enumerate}

\newpage

\section{ Grupos y álgebras de Lie}

Un grupo de Lie combina tres estructuras matemáticas diferentes. Verifica:

\begin{itemize}
    \item Los axiomas de grupo.
    \item Los elementos de grupo forman un espacio topológico (grupo topológico).
    \item Los elementos del grupo forman una variedad analítica.
\end{itemize}

De este modo vemos que un grupo de Lie puede analizarse de formas diferentes. No los estudiaremos aquí de forma exhaustiva de forma topológica pues los grupos de Lie que se utilizan más en física son los grupos de Lie lineales. Tienen propiedades adicionales que nos permiten analizarlo mediante métodos más sencillos.

\subsection{Elementos básicos sobre espacios topológicos}

Un espacio topológico S es un conjunto no vacío de elementos llamados puntos para los cuales hay una correlación $\mathds{T}$ de subconjuntos, llamados conjuntos abiertos, que satisfacen:

\begin{enumerate}
    \item El conjunto vacío $\phi$ y el conjunto S pertenecen a $\mathds{T}$.
    \item La unión de conjuntos de $\mathds{T}$ pertenece a $\mathds{T}$.
    \item La intersección de un número finito de conjuntos de $\mathds{T}$ pertenece a $\mathds{T}$.
    \item La colecció de $\mathds{T}$ se llama topología.
    \item Los complementarios a esos conjuntos se llaman conjuntos cerrados.
\end{enumerate}

\subsubsection{Compacidad}

Una familia de conjuntos abiertos de un espacio topológico S es un recubrimiento abierto de S si la unión de sus conjuntos abiertos contiene a S. Si por cada recubrimiento abierto de S siempre hay un recubrimiento finito que contiene a S (es decir, unión de un número finito de abiertos), el espacio topológico S se dice que es compacto. En caso contrario se dice que es no compacto.

\subsubsection{Conexion}

Un espacio topológico es conexo si no es la unión de dos conjuntos abiertos disjuntos no vacíos (que no tiene agujeros). Como consecuencia los únicos subconjuntos de S conexo que son a la vez abiertos y cerrados son solo el vacío y el propio S (si S es conexo).

\smallskip
Un \textbf{camino} en S desde $X_0$ a $X_1$  es un mapa continuo  $\phi: [0,1]\in \mathds{R}\to S$ con $\phi (0)=x_0,\phi (1)=x_1$. Si los puntos son idénticos y los valores del mapa en esos puntos son iguales se dice que el camino es cerrado (\textit{loop}). Dos caminos cerrados son equivalentes u homotópicos si uno puede llebar al otro mediante deformaciones continuas. Todos los loops equivalentes forman una clase de equivalencia.

\begin{itemize}
    \item S es \textbf{arco-conexo} si dados dos puntos de S cualesquiera siempre existe un camino con $\phi (0)=x_0$ y $\phi (1)=x_1$.

    \item S arco-conexo es \textbf{simplemente conexo} si todo camino cerrado se puede encoger a un punto con deformaciones contínuas.

    Si hay n clases de equivalencia distintas de caminos cerrados entonces S se dice n-veces conexo.
\end{itemize}

\textbf{Ejemplos:}

\begin{itemize}
    \item Una región X del espacio euclídeo $\mathds{R}^n$ es compacto solo si es finita.
    \item El espacio $\mathds{R}^2$ es simplemente conexo, regiones suyas con agujeros no lo son.
\end{itemize}

 \subsubsection{Mapa homeomórfico}

 Dados dos espacios topologicos (S,T) y (S',T'), un mapa de S en S' se dice contínuo en S si para todo abierto de S' la imagen inversa del mapa es un abierto de S. Si el mapa $\phi , \phi ^{-1}$ es contínuo entonces esta aplicación es un homeomorfismo y S y S' son homeomorfos.

 \smallskip
 Las propiedades topológicas son invariantes bajo homeomorfismos, también llamados invariantes topológicos.

 \subsubsection{Espacio Hausdorff}

 Un espacio topológico (S,T) es Hausdorff si dos puntos cualquiera de S pertenecen a subconjuntos abiertos de T disjuntos (axioma de separabilidad).

 Un espacio localmente euclídeo de dimensión n es un espacio topológico Hausdorff tal que cada uno de sus puntos está contenido en un conjunto abierto que es homeomorfo a un subconjunto de $\mathds{R}^n$.


 \subsubsection{Carta}
 Sea V un abierto de dicho espacio y $\phi$ un homeomorfismo de V en un subconjunto de $\mathds{R}^n$ entonces para cada punto de V $p\in V$ existe un conjunto de coordenadas ($x_1,x_2,..,x_n$) tal que $\phi (p)= (x_1,x_2,..,x_n) $. El par $(p, \phi)$ se llama \textbf{carta}.

 \subsubsection{Variedad analítica de dimensión n}

 Consideremos un espacio localmente euclídeo de dimensión n y tal que posee una base numerable (una base de la topología T es un subconjunto $B \in T$ de abiertos tal que cualquier conjunto abierto es unión de elementos de B) y un homeomorfismo de un abierto $V \subset \mathcal{V}$ en un subconjunto de $\mathds{R}^n$.

 \smallskip
 Si para cada par de cartas $(V_\alpha , \phi _\alpha)$ y $(V_\beta , \phi _\beta)$ del subgrupo $\mathcal{V}$ con intersección no vacía, el mapa $\phi _\beta  \hspace{0.1cm} o \hspace{0.1cm} \phi _\alpha ^{-1}$ es una función analítica, entonces $\mathcal{V}$ es una variedad analítica de dimensión n (como por ejemplo $\mathds{R}^n$).

 \subsection{Grupo de Lie, definición.}

 Un grupo de Lie de dimensión n es un conjunto de elementos que satisfacen las siguientes condiciones:

 \begin{enumerate}
     \item Forman grupo.
     \item Forman una variedad analítica de dimensión n.
     \item El mapa $\begin{array}{cc}
         \phi: \hspace{0.2cm} G\times G \to G  \\
          (g_1,g_2)\to \phi (g_1g_2)=g_1g_2
     \end{array}$ es analítico para todo $g_1,g_2 \in G$.
     \item Este mapa es también analítico (es infinitamente diferenciable; esta dado localmente por una serie de potencias convergente).
      \end{enumerate}

     La característica básica de un grupo de Lie es que tiene un número no contable de elementos dentro de una región $\textit{cercanos}$ a la identidad y la estructura de estos elementos determina esencialmente la estructura del grupo completo.

     \smallskip
     Los elementos de dicha región estarán parametrizados de manera analítica y debemos tener una noción de distancia.

     En el caso de grupos de Lie lineales existe una representación natural que permite una definición de distancia precisa, que permite asegurar que el resto de requerimientos topológicos se verifican.

     \subsection{Grupos de Lie lineales}

     Un grupo G es un grupo lineal de Lie de dimensión n si satisface las cuatro coniciones siguientes:

     \begin{enumerate}
         \item G posee una representación matricial D que es fiel y de dimensión finita m.

         Definimos la distancia entre dos elementos $g,g'\in G$ como:

         $$d(g,g')=\sqrt{\sum _{i,j=1}^m |D(g)_{ij}-D(g')_{ij}|^2}$$

         y el conjunto de matrices D(g) satisface las condiciones de espacio métrico.

         El conjunto $\lbrace g_i\rbrace$ con $g_i \in G$ tal que $d(g_i,e)<\delta$, con $\delta \in \mathds{R}^+$. Se dice que esta en una bola de radio $\delta$ centrada en la identidad e y denotada por $M_s$ que a veces llamaremos entorno de e.

         \item Existe un real positivo tal que los elementos de $M_s$ se pueden parametrizar (de modo diferente) por n parámetros reales independientes ($x_1,...,x_n$) con e correspondiente a $x_1,...,x_n=0$.

         Cada elemento de $M_s$ se corresponde con un único punto de $\mathds{R}^n$ que se corresponde con más de un elemento $g_i\in M_s$.

         \item Existe un real $\epsilon >0$ tal que cada punto de $\mathds{R}^n$ para el que se cumpla $\sum _{i=1}^n x_1^2 < \epsilon ^2$ se corresponde con algún elemento de $g_i \in M_s$ y la correspondencia es uno a uno.

         \item Sea $D(g(x_1,...,x_n))$ la matriz de representación del elemento $g(x_1,...,x_n)\in G$. Entonces cada elemento de matriz de D es una función analítica de ($x_1,...,x_n$) para todo punto de $\mathds{R}^n$ que satisfaga la condición anterior.

     \end{enumerate}

       \textbf{Nota:} Todo grupo de Lie lineal es isomorfo a algún subgrupo del grupo general lineal de matrices de dimensión adecuada.

     \smallskip
     Cada grupo de Lie lineal se dice \textbf{conexo} si el espacio topológico que forman sus elementos es conexo. Análogamente puede ser simplemente conexo o múltiplemente conexo.

     \subsubsection{Recubridor universal}

     Si G es un grupo de Lie multiplemente conexo existe un grupo $\Tilde{G}$ simplemente conexo (único salvo isomorfismos) tal que G es isomorfo al grupo cociente $\Tilde{G}/Z(\Tilde{G})) \hspace{0.2cm} \left [ Z(\Tilde{G})=\lbrace h\in \Tilde{ G} | hg=gh  \hspace{0.2cm} \forall g  \in \Tilde{G} \rbrace\right]$ o alguno de sus subgrupos.

     $\Tilde{G}$ se llama el recubridor universal de G.

     \smallskip
     Un grupo de Lie se dice \textbf{compacto} si su espacio topológico es compacto.

     \subsubsection{Representaciones unitarias del grupo de Lie}

     \begin{enumerate}
         \item Si G es un grupo de Lie compacto, toda representación de G es equivalente a alguna unitaria.

     \item Si G es un grupo de Lie compacto toda representación reducible de G es completamente reducible (completamente descomponible).

     \item Si G es un grupo de Lie no compacto, entonces no posee representaciones unitarias de dimensión finita no triviales.

        \end{enumerate}

\subsubsection{Ejemplos}

    \begin{itemize}
        \item    $GL(n,\mathds{C})$: grupo general lineal de matrices complejas M con det M$\neq$ 0 de dimensión 2$n^2$.

        \item $SL(n, \mathds{C})$: grupo especial lineal, subgrupo del general con detM=1 de dimensión 2$n^2$ -2 (pues el determinante da dos restricciones; Re(detM)=1 y Im(detM)=0).

        \item GL($n,\mathds{R}$): de dimensión $n^2$.

        \item $SL(n,\mathds{R})$: de dimensión $n^2-1$.

        \item El grupo $U(n)$: grupo unitario de matrices complejas U tal que $U^+U=UU^+=\mathds{1}^n$ de dimensión $n^2$ (en principio es subgrupo de GL pero la condición de conmutación nos quita la mitad).

        \item SU(n): grupo especial unitario, subgrupo de U(n) que agrupa las matrices con detU=1, de dimensión $n^2-1$ (como el det U es un complejo de fase libre y norma 1 solo pone 1 condición sobre el detU).

        \item O(n): grupo ortogonal de matrices reales que cumplen $OO^+=O^+O=\mathds{1}_n$ de dimensión $\frac{n(n-1)}{2}$.

        \item SO(n): grupo ortogonal especia, subgrupo de O(n) con detO=1, de la misma dimensión que O(n).

        \item Sp(n): grupo simpléptico, grupo de matrices unitarias (n $\times$ n) con n par. Satisfacen $U^T J U=J$. La matriz J$=\left (\begin{array}{cc}
            0 & \mathds{1}_{n/2} \\
            -\mathds{1}_{n/2} & 0
        \end{array} \right )$ de dimensión $\frac{n(n+1)}{2}$.

        \item U(l,n-l): grupo pseudo-unitario de matrices complejas U que satisfacen $UgU^+=g$ siendo g una matriz diagonal de unos y menos unos de forma que $g_{kk}=1$ para $1 \leq k \leq l$ y $g_{kk}=-1$ para $ l+1 \leq k \leq n$. LA dimensión es $n^{2}$

        \item O(n,l-n): grupo pseudo-ortogonal de matrices reales con $OgO^+=g$ con la misma g, de dimensión $\frac{n(n-1)}{2}        $. Es el grupo de Lorentz, la g es una pseudo-métrica.

        \begin{enumerate}
            \item Compactos: U(n), SU(n), O(n), SO(n), Sp(n).

            \item No compactos: GL(n), SL(n), U(n,l-n), O(n,l-n.)
        \end{enumerate}
     \end{itemize}

     \subsubsection{Ejercicio: ¿Son O(n) y U(n) grupos conexos?}

     O(n) está formado por rotaciones y reflexiones luego no es conexo; sus dos subconjuntos son: rotaciones (subgrupo) y reflexiones (que no es subgrupo pues $(det=-1)^2 \neq -1$). Consiste en dos componentes disjuntas SO(n) (rotaciones) y la componente con det=-1 (reflexiones).

     \smallskip
     U(n) por otra parte sí es conexo (es decir no es la unión de conjuntos abiertos disjuntos) pues las fases se pueden parametrizar de forma continua para que tome todos los valores complejos unitarios.

     \subsubsection{Ejercicio: ¿Son SO(2)$\simeq $
     U(1) y SU(2) simplemente conexos?}

     Cualquier elemento de SO(2) puede escribirse como $\mathcal{R}= \left ( \begin{array}{cc}
         a &  -b\\
         b & a
     \end{array}\right)$ con $a^2+b^2=1$. Es decir, escrita como variedad diferencial, SO(2) es el círculo unidad. Vemos pues que SO(2) no es simplemente conexo (no se puede reducir el círculo a un punto sin cortarlo).

     \bigskip
     Ahora para SU(2) tenemos que deben cumplir $|a|^2+|b|^2=1$ siendo ahora a,b $\in \mathds{R}$ y la matriz $\mathcal{R}= \left ( \begin{array}{cc}
         a &  b\\
         -\bar{b} & \bar{a}
     \end{array}\right)$. Así que los U (elementos de SU(2)) pueden identificarse con un punto (x,y,z,w) $\in \mathds{R}^4$ siendo $a=(x,y)$ y $b=(z,w)$. Deberán satisfacer: $x^2 +y^2 + z^2 + w^2=1$ (ecuación de la 3-esfera o hiperesfera de dimensión 4). La 3-esfera es simplemente conexa al ser la generalización de la esfera. Las curvas sobre la esfera pueden '' achicarse '' a un punto sin salirse de la esfera no como con el círculo. Las únicas esferas que son grupos de Lie son la 1-esfera (círculo) y la 3-esfera (por cuestiones de restricciones topológicas).

     \newpage
     \subsubsection{Ejercicio: justificar por qué el grupo SO(1,1) no es compacto}

     Pista: SO(1,1) es isomorfo a los reales con la operación de suma.


     \bigskip
     Es el grupo de transformaciones lineales en $\mathds{R}^2$ que dejan invariante el producto interno $\braket{\Vec{v}|\Vec{u}}= v_1u_1 - v_2u_2$ con determinante 1.

     \smallskip
     Las matrices $\Lambda$ que me dejan invariante ese producto son aquellas que dejan la métrica invariante $\Lambda g \Lambda ^t =g$ . Son:

     $$\Lambda = \left (\begin{array}{cc}
        a  & b \\
         b & a
     \end{array} \right)$$

     Además tienen que cumplir que $a^2 -b^2=1$ para que su determinante sea 0, pueden ser cualquier real. El grupo tiene dimensión 1 debido a esta ligadura.

     Esta condición es la fundamental de las trigonométricas hiperbólicas, parametrizando según:

     $$\left \lbrace \begin{array}{cc}
          a=cosh \mathcal{X}  \\
         b=senh \mathcal{X}
     \end{array} \right .  \longrightarrow \Lambda (\mathcal{X})=\left ( \begin{array}{cc}
         ch\mathcal{X} & sh\mathcal{X} \\
       sh\mathcal{X}   & ch\mathcal{X}
     \end{array}\right)$$

     Tenemos un isomorfismo de $\mathds{R}$ a estas matrices. Dado que este espacio es isomorfo a los reales con la suma, como estos no son un grupo compacto este tampoco lo es.


     \subsubsection{Ejercicio: Acabamos de ver que SO(2) $\simeq$ U(1) $\simeq S^1$  no es simplemente conexo. ¿Qué grupo es su recubridor universal? Buscar el grupo normal de G (H) tal que $S^1 \simeq G/H$ (el grupo cociente es el círculo).}

     Pista: la dimensión de $S^1$ es 1. Su recubridor universal tendrá también dimensión 1.


     \smallskip
     La dimensión del recubridor ha de ser 1 pues la dimensión de $S^1$ es 1. No puede coincidir con $S^1$ pues si no sería él mismo, es entonces la recta real $\mathds{R}$ (toda variedad analítica unidimensional tiene como recubridor universal o bien $S^1$ o bien la recta real salvo isomorfismos).

     \smallskip
     Buscamos pues un homomorfismo de grupo que nos lleve de $\mathds{R}$ a $S^1$.

     $$\phi: x \to e^{2\pi i x}$$

     $$\mathds{R} \to S^1$$

     Sabemos que su núcleo es un subgrupo normal, vemos que son los enteros pues son los números que bajo $\phi$ nos llevan a la identidad.

     $$ker \phi = \mathds{Z}$$

     Obtenemos pues el isomorfismo $S^1 \simeq \mathds{R} / \mathds{Z}$.

     \textbf{Moraleja:} si encontramos un homomoerfismo sobreyectivo $\phi: \Tilde{G} \to G$, el núcleo del homomoerfismo es un subgrupo normal de $\Tilde{G}$ y el grupo cociente $\Tilde{G}/ker\phi$ es isomorfo a $\phi (\Tilde{G})=G$ siendo $\Tilde{ G}$ el recubridor universal de G.


     \subsection{Medida de integración invariante}

     Dada una función definida en G con valores complejos f: $\begin{array}{c}
         G \to \mathds{C}  \\
          g \to f(g)
     \end{array}$ tenemos que si G es un grupo finito (por el teorema del reordenamiento) podemos escribir:

     $$\sum _{g\in G} f(gg')=\sum _{g\in G} f(g)=\sum _{g\in G} f(gg')$$

     decimos que la suma es invariante por la izquierda e invariante por la derecha respectivamente.

     \smallskip
     Además, para grupos de Lie (análogamente a lo que ocurría con los grupos finitos) si f(g)=1 $\forall g \in G$, la suma es finita:

     $$\sum _{g \in G} 1 =|G|$$

     Para grupos de Lie lineales la suma se puede sustituir por una integral que también va a ser invariante por la izquierda y por la derecha.

     $$\int _G f(g)d_lg=\int ^{b_1}_{a_1} dx_1...\int ^{b_n}_{a_n} dx_n f(g(x_1,...,x_n))\sigma _l(x_1,...,x_n)$$

     Donde $\sigma _l$ es una función peso que hace que la integral sea invariante por la izquierda. Análogamente se haría para que fuera invariante por la derecha:

     $$\int _G f(g)d_rg=\int ^{b_1}_{a_1} dx_1...\int ^{b_n}_{a_n} dx_n f(g(x_1,...,x_n))\sigma _r(x_1,...,x_n)$$

     Si multiplico por g' por la izquierda a la primera o por g' por la izquierda a la segunda las integrales no cambian.

     $$\int _G f(g'g)d_lg=\int _G f(g)d_lg \hspace{1cm} \int _G f(gg')d_rg=\int _G f(g)d_rg=$$

     Esto provoca una restricción en las funciones peso que las hace únicas salvo constante.

     \bigskip
     \begin{itemize}
         \item      \textbf{Teorema:} si G es un grupo de Lie compacto entonces podemos asegurar que $\sigma _r=\sigma_l=\sigma$. La integral invariante que define, $\int _G f(g)d g$, es por tanto igual por la izquierda que por la derecha. Además existe (converge) y es finita para toda función f(g) continua. Podemos escoger $\sigma$ (que era única salvo constante) para que $\int _G dg=\int ^{b_1}_{a_1} dx_1...\int ^{b_n}_{a_n} dx_n \sigma (x_1,...,x_n)=1$. La medida así definida la llamamos \textbf{medida de Haar} (única e invariante por izquierda y derecha).

         \item \textbf{Teorema:} si G es un grupo de Lie no compacto las medidas invariantes por la izquierda y la derecha son infinitas (y por tanto no tiene sentido definir la medida de Haar). Por ejemplo, en el círculo las medidas son:

         $$\int ^{2\pi}_0 d\theta f(\theta) \to \int ^{2\pi}_0 d\theta f(\theta +\phi)= \int ^{2\pi + x}_{0+x} d\Tilde{\theta} f(\Tilde{\theta})$$

         donde multiplicando por la derecha lo que hacemos es sumar por la derecha. Para que sea medida de Haar basta con normalizarla:

         $$\int ^{2\pi}_0 d \theta \frac{1}{2\pi}$$

     \end{itemize}

     \subsection{Estudio local de un grupo de Lie: álgebras de Lie}

     La mayoría de la información de la estructura de un grupo de Lie proviene del análisis de sus propiedades locales (un grupo de Lie es una variedad diferenciable con un álgebra determinado). De este modo, las propiedades vendrán determinadas por álgebras de Lie reales (en el caso de grupos de Lie lineales).

     \begin{itemize}
         \item \textbf{Álgebra de Lie real:} un álgebra de Lie real de dimensión mayor o igual a uno es un espacio vectorial real con una ley de composición llamada corchete de Lie $[A,B]$.

         \begin{enumerate}

             \item $[A,B] \in \mathcal{L} \hspace{1cm}$ No sale del cuerpo.

             \item $[\alpha A + \beta B,C]=\alpha \alpha[A,C] + \beta [B,C] \in \mathcal{ L} \hspace{0.5cm}$ Es lineal.

            \item $[A,B]=-[B,A]$

            \item Cumple la identidad de Jacobi $[A,[B,C]]+[B,[C,A]]+[C[A,B]]=0$

            En el caso de un álgebra de Lie de matrices el corchete de Lie es el conmutador. Si conmutan el álgebra de Lie es abeliana.
             \end{enumerate}
     \end{itemize}

     Un subálgebra de Lie ($\mathcal{L}'$) es un subconjunto de $\mathcal{L}$ que forma un álgebra de Lie con el mismo corchete que $\mathcal{L}$.

     Un subálgebra $\mathcal{L}'$  de un álgebra $\mathcal{L}$ se dice invariante si el conmutador de un elemento del subálgebra con otro del álgebra es 0 para todo elemento de estos conjuntos.

     $$[A,B]=0; \hspace{0.2cm} \forall A\in \mathcal{L}' , \hspace{0.1cm} B \in \mathcal{L}$$

     \subsubsection{Función exponencial para matrices}

     Si A es una matriz mxm $e^A$ es la matriz definida como:

     $$e^A=\mathds{1}+\sum _{k=1}^\infty \frac{1}{k!}A^k$$

     Esta serie converge para toda matriz. Tiene una serie de propiedades:

     \begin{enumerate}
         \item $(e^A)^+=e^{(A^+)}$
         \item $e^A$ es no singular y $(e^A)^{-1}=e^{-A}$
         \item $det(e^{A}) = e^{Tr(A)}$
         \item Si S es una matriz no singular, $e^{SAS^{-1}}=Se^{A}S^{-1}$
         \item Si A tiene una serie de autovalores, los autovalores de $e^{A}$ son la exponencial de los autovalores.
         \item Si S y B son matrices mxm que conmutan entonces $e^{A}$ también conmuta con $e^B$.
         \item Si no conmutan y sus coeficientes son suficientemente pequeños podemos escribir $e^Ae^B=e^C$ con C=A+B+$\frac{1}{2}[A,B]+\frac{1}{12}([A,[A,B]]+[B[A,B]])+... $(fórmula de Campbell-Baker-Hausdorff (CBH)).
         \item El mapa exponencial $\phi (A)=e^A$ es un mapa contínuo 1 a 1 desde un entorno pequeño de la mariz 0 de tamaño mxm ($O_{mxm}$) a un entorno pequeño de la matriz identidad de tamaño mxm.
     \end{enumerate}

     \subsubsection{Subgrupos uniparamétricos asociados a grupos de Lie lineales}

     Dado un grupo de Lie lineal G de matrices mxm, un subgrupo uniparamétrico de G es un subgrupo de Lie de G formado por matrices etiquetadas por un solo parámetro $T(t)$ real t tal que $T(t)T(t')=T(t+t')$ para cualquier parámetro real. Claramente este subgrupo es abeliano y cumple que T(0)=e.

     \smallskip
     El inverso de los elementos de este grupo se obtiene tomando la matriz del parámetro opuesto $T(t)T(-t)=e$. Es un subgrupo de dimensión 1 pues solo dependen de un parámetro.

     \smallskip
    Podemos definir $\left . \frac{dT(t)}{dt}\right |_{t=0}\equiv \omega$, que existe y es no trivial, como el vector tangente al subgrupo uniparamétrico evaluado en la identidad. Esto implica que cada subgrupo uniparamétrico de un grupo de matrices de Lie lineal de matrices mxm se forma por exponenciación según:

    $$T(t)=e^{\omega t}$$

    \subsubsection{Generadores del álgebra de Lie}

     De la definición de un grupo de Lie lineal G de dimensión n se sigue que las matrices de representación son funciones de las coordenadas de $\mathds{R}^n$ de forma que $D(g(x_1,...,x_n))=D(x_1,...,x_n)$. Son además funciones analíticas. Las n matrices $A_1,..., A_n$ definidas por:

     $$\left . (A_r)_{ij}=\frac{\partial D_{ij}(g)}{\partial x_r} \right |_{x_r=0}$$

     los elementos $g\in M_g$ son elementos cercanos a la identidad; estas matrices forman una base de un espacio vectorial real de dimensión n. Este espacio vectorial es el álgebra de Lie asociada al grupo G, siendo el corchete de Lie el conmutador.

     Las matrices $A_n$ se llaman generadores del álgebra de Lie, nosotros en concreto las elegimos hermíticas.


     \subsubsection{Relación entre álgebras de Lie reales y grupos de Lie lineales}

     Podemos asociar un álgebra de Lie real de dimensión n a cada grupo de Lie lineal G de la misma dimensión de acuerdo con los siguientes teoremas.

     \begin{itemize}
         \item Todo elemento A de un álgebra de Lie real de un grupo de Lie lineal está asociado a un subgrupo uniparamétrico de G definido por $T(t)=e^{At}$.

         \item Todo elemento g de un grupo de Lie lineal G en un entorno cercano a la identidad pertenece a un subgrupo uniparamétrico de G.

         $$T(0)=\mathds{1}_{mxm}+\cancel{0(t^2)}+...$$

         \item Si G es un grupo de Lie lineal compacto todo elemento de un subgrupo conexo de G se puede expresar de la forma $e^A$ donde A es un elemento del álgebra de Lie correspondiente. En particular, si G es compacto y conexo todo elemento $g\in G$ es de la forma $e^A$ con $A\in  \mathcal{L}$.

         \textbf{Resumiendo}: el álgebra de Lie es el espacio tangente de un grupo lineal G evaluado en la identidad ($T_eG$) es decir, se trata del espacio vectorial generado por los vectores tangentes a todos los subgrupos uniparamétricos.

         $$T(t)=e^{\omega t} \longrightarrow \frac{dT(t)}{dt}=\omega T(t)$$

         \end{itemize}

         \subsubsection{Ejemplos}

         \begin{itemize}
             \item El álgebra de Lie real del grupo SU(n). Sea $e^{At}$ un subgrupo uniparamétrico de SU(n). Como T es una matriz nxn que satisface $T^+T=T^+=\mathds{1}_{nxn}$ y det(T)=1, obtenemos:

             $$A^+=A; \hspace{2cm} Tr(A)=0$$

             El conjunto de matrices nxn antihermíticas de traza nula son las matrices de representación de SU(n). Las propiedades de A salen de aplicar lo visto en el apartado propiedades a las características del grupo SU(n).

             \item Álgebra de Lie lineal del grupo SL(n,$\mathds{R}$): los elementos de un subgrupo uniparamétrico son matrices reales nxn con determinante 1 con entradas reales. El álgebra de Lie asociada es el conjunto de matrices reales nxn con traza nula.
         \end{itemize}

         \subsubsection{Ejercicio: caracteriza el álgebra de Lie so(2) de SO(2), busca una base y muestra que los elementos de SO(2) se obtienen por exponenciación del álgebra.}


         Las características de SO(2) son que se trata de matrices ortogonales de determinante 1, La traza de A ha de ser 0 y las matrices cumplen $AA^{-1}=\mathds{1}$.

         Este ejercicio esta resuelto en el canal de mi amigo el de youtube:

         (https://www.youtube.com/user/jamesjamesbondbond) en su curso de grupos de Lie.

         \smallskip
         SO(2) son matrices 2x2 ortogonales de det=1, su álgebra de Lie está formada por matrices 2x2 reales, antisimétricas de traza nula:

         $$A^t=A; \hspace{0.5cm} \text{Antisimétrica}$$

         La dimensión de SO(2) es 1, la base es:

         $$T=\left ( \begin{array}{cc}
             0 &  1\\
             -1 & 0
         \end{array}\right)$$

         Las matrices de SO(2) las llamamos R por la rotación:

         $$R= \left ( \begin{array}{cc}
             cos \theta & -sen \theta \\
            sen \theta  & cos \theta
         \end{array}\right)$$

         Los elementos del álgebra son todas las matrices del tipo:

         $$SO(2)=\lbrace \theta T , \theta \in \mathds{R} \rbrace$$

         Exponenciando y desarrollando en Taylor:

         $$e^{\theta T}= \mathds{1} + \theta T + \frac{1}{2!}(\theta T)^2 + ...$$

         Tenemos en cuenta que:

         $$T=\left ( \begin{array}{cc}
             0 &  1\\
             -1 & 0
         \end{array}\right); \hspace{0.2cm} T^2=-\left ( \begin{array}{cc}
             1 &  0\\
             0 & 1
         \end{array}\right); \hspace{0.2cm} T^3=-\left ( \begin{array}{cc}
             0 &  1\\
             -1 & 0
         \end{array}\right); \hspace{0.2cm} ...$$

         Y entonces vemos que se puede separar en dos:

         $$e^{\theta T}=\sum _{n=0}^\infty \frac{1}{n!} (\theta T)^n = \underbrace{\sum _{n=0}^\infty \frac{(-1)^{n}}{(2n)!}\theta ^{2n} \mathds{1}}_{cos \theta \cdot \mathds{1}} + \underbrace{ \sum _{n=0}^\infty \frac{(-1)^{n}}{(2n+1)!}\theta ^{2n+1} T}_{sen \theta \cdot \mathds{T}}= \left ( \begin{array}{cc}
             cos \theta & -sen \theta \\
            sen \theta  & cos \theta
         \end{array}\right) $$


         \subsection{Representaciones adjuntas de álgebras de Lie y de grupos de Lie}

         \subsubsection{Representación de un álgebra de Lie}

         Suponemos que a cada $A \in \mathcal{L}$ le corresponde una matriz mxm D(A) tal que:

         $$\left \lbrace \begin{array}{c}
              D(\alpha A+\beta B)=\alpha D(A) + \beta D(B)  \\
              D([A,B])=[D(A),D(B)]
         \end{array} \right .$$

         Entonces estas matrices forman una representación de dimensión m del grupo de Lie. Si los elementos de $\mathcal{L}$ son matrices, entonces $D(A)=A$.

         \smallskip
         \textbf{Teorema:} sea $D_g$ una representación analítica m-dimensional de un grupo de Lie lineal G. Su correspondiente álgebra de Lie es $\mathcal{L}$.

         Se cumple que:

         \begin{itemize}
             \item Existe una representación $D_\mathcal{L} de \mathcal{L}$ definida para todo elemento del álgerbra mediante:

             $$D_\mathcal{L}(A)=\left .\frac{d}{dt}D_G(e^{tA})\right |_{t=0}$$

             \item La exponenciación del álgebra nos da la representación del grupo:

             $$e^{tD_\mathcal{L}(A)}=D_G(e^{tA})$$

             \item Si $D_G'$ es otra representación analítica de G y $D_\mathcal{L}'$ la representación asociada de $\mathcal{L}$ entonces esta representación es equivalente a $D_\mathcal{L}$ si $D_\mathcal{L}'$ es equivalente a $D_\mathcal{L}$. Lo contrario también es cierto si G es conexo.

             \item La representación $D_\mathcal{L}$ es reducible si $D_G$ es reducible y es completamente reducible (o completamente descomponible) si $D_G$ lo es. Lo contrario es cierto si G es conexo.

             La gracia es que si G es conexo el grupo está totalmente determinado por el álgebra y estamos hablando de la misma cosa (nos vale el álgebra para describir el grupo).

             \item Si G es conexo entonces $D_\mathcal{L}$ es irreducible sí y solo si G es irreducible.

             \item Si la representación del grupo ($D_G$) es unitaria la representación del álgebra ($D_\mathcal{L}(A)$ es antihermítica. Lo contrario también es cierto si G es conexo.
         \end{itemize}



      \subsubsection{Representación adjunta de un álgebra de Lie: constantes de estructura}

      Sea $\mathcal{L}$ un álgebra de Lie lineal de dimensión n y sea una base de ese álgebra de elementos $\lbrace A_i\rbrace _{i=1}^n$ una base de $\mathcal{L}$. Para cualquier A de este álgebra definimos ad(A) como la matriz nxn que cumple:

      $$[A,A_i]=\sum _{i=1}^n (ad(A))_{ji}A_j$$

      Visto como operador tendríamos $[A,A_i]=ad(A)A_i$. El conjunto de estas matrices forma una representación de dimensión n (la misma que el álgebra), llamada representación adjunta de $\mathcal{L}$ (sería la representación "natural").

      La importancia de esta representación se acentúa en el estudio de álgebras semisimples.

     \smallskip
     Podemos definir las constantes de estructura a través de esto de la siguiente manera:

     $$[A_i,A_j]=\sum _{k=1}^n c_{ij}^k A_k$$

     Como los coeficientes de las relaciones de conmutación en la base que acabamos de ver. Las constantes de estructura me caracterizan completamente el álgebra.

     Las constantes de estructura no son independientes, cumplen la identidad de jacobi y la siguiente propiedad de antisimetría:

     \begin{enumerate}
     \item $     c_{ij}^k=-c_{ji}^k $
         \item $
          c_{ij}^kc_{lm}^n + c^k_{jl}c_{im}^n+ c_{li}^kc^n_{jm}=0 $
     \end{enumerate}


     \subsubsection{Representación adjunta de un grupo de Lie lineal}

     Sea G un grupo de Lie lineal de dimensión n y sea $\lbrace A_i \rbrace _{i=1}^n$ una base de su álgebra de Lie correspondiente. Para cualquier elemento del grupo se presenta Ad(g) como la matriz nxn definida tal que:

     $$gA_ig^{-1}=\sum _{i=1}^n (Ad(g))_{ji} A_j$$

     Recordamos que $ge^{A_i}g^{-1}=e^{gA_ig^{-1}}$, es decir, la exponenciación de un elemento del grupo es idéntica a la exponenciación de los elementos del álgebra (pues existe una relación grupo-álgebra).

     El conjunto de matrices Ad(g) forman una representación de G analítica de dimensión n con la misma dimensión que el grupo (se llama representación adjunta de G). Esta representación actúa directamente sobre el espacio vectorial que es el álgebra de Lie.

     \smallskip
     \textbf{Teorema:} sean ad(g) y Ad(g) las representaciones adjuntas del álgebra y del grupo respectivamente, estas representaciones están relacionadas:

     $$ad(A)= \left . \frac{d}{dt}Ad(e^{tA})\right |_{t=0}$$

      Además $e^{t ad(A)}=Ad(e^{tA})$ para cualquier A elemento del álgebra y t parámetro real.

      \smallskip
      \textbf{Teorema:} sea G un grupo de Lie lineal conexo y $\mathcal{L}$ su álgebra de Lie correspondiente; para cualquier elemento del grupo, el mapa inyectivo definido por:

      $$\uppsi _g \cdot \mathcal{L} \to \mathcal{L}$$

      $$\uppsi _g (A)=gAg^{-1}$$

      es un automorfismo del álgebra (pues el mapa del grupo es lineal y respeta el corchete) y se llama automorfismo interno de $\mathcal{L}$.

      $$\uppsi _g (\alpha A + \beta B)=\alpha \uppsi _g (A) + \beta \uppsi _g (B)$$

      $$\uppsi _g ([A,B])=[\uppsi _g (A), \uppsi _g (B)]$$

      \smallskip
      \textbf{Teorema:} el conjunto de todos los automorfismos de $\mathcal{L}$ forman un grupo, Aut($\mathcal{L}$), y el conjunto de todos los automorfismos internos $Int(\mathcal{L})$ es subgrupo normal de Aut($\mathcal{L}$).

      \subsection{Álgebras de Lie simples y semi-simples}

      Un álgebra de Lie es \textbf{simple} si no es abeliana y no tiene subálgebras de Lie invariantes no triviales.

      Un álgebra de Lie es \textbf{semi-simple} si no es abeliana y no tiene subálgebras de Lie invariantes abelianas.

      \begin{itemize}
          \item Toda álgebra semi-simple es suma directa de álgebras de Lie simples.
     \item Un grupo de Lie lineal es simple (semi-simple) si y solo sí su álgebra de Lie es simple (semi-simple).
      \end{itemize}

      \subsubsection{Operadores de Casimir}

      Consideremos un álgebra de Lie semi-simple con base $\lbrace A_i \rbrace _{i=1}^n$ y corchetes de Lie $[A_i,A_j]=\sum _{k=1}^n c_{ij}^k A_k$. Definamos una matriz h de tamaño nxn dada por:

      $$h_{ij}=\sum _{l,k}c^l_{ik}c^k_{jl}$$

      El operador de Casimir (de segundo órden) se define por:

      $$C=\sum _{ij}h_{i,j}A_i,A_j$$

      La característica de los operadores de Casimir es que conmutan con todos los elementos del álgebra (con los generadores del grupo).

      $$[C,A_r]=0$$

     \subsection{Forma de Killing}

     Dados dos elementos A y A' de un álgebra de Lie de dimensión n su forma de Killing se define como:

     $$K(A,A')=Tr(ad(A)ad(A'))$$

     Las cantidades K($A_i,A_j$) coinciden con $h_{i,j}$.

     \textbf{Teorema:} un álgebra de Lie real es semisimple si y solo si el determinante de su forma de Killing es distinto de cero (su forma de Killing no es degenerada).

    $$det(B_{ij})\neq 0$$

    Las álgebras semisimples son suma directa de simples, existe un sistema para clasificarlas (tiene que ver con las \textit{raices}, no veremos esto) que utiliza los diagramas de Dynkin.

     \newpage

     \section{Rotaciones en $\mathds{R}^3$: SO(3) y SU(2)}

     \subsection{Descripción de SO(3)}

     Las rotaciones tridimensionales (propias, con det=1) son las transformaciones lineales de los vectores de $\mathds{R}^3$ que dejan invariante su norma preservando la orientación.

     En una base ortonormal los elementos de SO(3) son matrices reales, ortogonales y con determinante 1.

    La dimensión del grupo es 3, dos parámetros para el eje y uno para el ángulo de giro.

    \begin{itemize}
        \item \textbf{Parametrización ángulo-eje}:

        Se da un vector unitario $\Vec{n}$ como eje de giro y un ángulo $\uppsi \in [0,\pi]$ de rotación en torno a ese eje.

        A su vez, el vector unitario $\Vec{n}$ puede describirse a través de los ángulos polar y acimutal. La rotación se caracteriza por tres parámetros.


        \begin{figure}[h!]
            \centering
            \includegraphics[width= 0.7\textwidth]{parametrizacion1.png}
            \caption{Representación en el espacio de los parámetros de la parametrización}
            \label{fig:my_label}
        \end{figure}

    $$0\leq \uppsi \leq \pi \hspace{0.5cm} 0\leq \Theta \leq \pi \hspace{0.5cm}  0\leq \phi \leq 2\pi $$

        \end{itemize}
        \textbf{Redundancia:} .$R_{\Vec{n}}(\pi) = R_{-\Vec{n}}(\pi) $. Puntos en las antípodas quedan identificados por el cociente $S^3/\mathds{Z}_2$. SO(3) es conexo pero no simplemente conexo. Hay dos clases de caminos cerrados (doblemente conexo).

         En componentes, las matrices de rotación son:

        $$(R_{\Vec{n}}(\uppsi))_{ij}=\delta _{ij} cos \uppsi + n_in_j (1-cos \uppsi) - sen \uppsi \sum _K \epsilon _{ijk} n_k$$

        \newpage
        \textbf{Clases de conjugación}

        $$RR_{\Vec{n}}(\uppsi ) R^{-1} = R_{\Vec{n}'}(\uppsi)$$

        con $\Vec{n}'=R\Vec{n}$ para cualquier rotación en tres dimensiones.

        \smallskip
        \textbf{Teorema:} Todas las rotaciones por el mismo ángulo $\uppsi$ independientemente del eje de rotacción pertenecen a la misma clase de conjugación.

        \begin{itemize}
            \item \textbf{Parametrización con ángulos de Euler}:

            Trata de especificar las rotaciones a través de la configuración relativa de dos referenciales cartesianos, uno ortogonal inicial ($O_x,O_y,O_z$) y otro rotado (OX,OY,OZ).

            Este último resulta de la composición de una rotación de ángulo $\alpha$ en torno a $O_z$:

            $$ (O_x,O_y,O_z) \longrightarrow (O_u,O_v,O_z)$$

            Seguida de una rotación de ángulo $\beta$ en torno a $O_v$:

            $$(O_u,O_v,O_z) \longrightarrow (O_{u'},O_v,OZ)$$

            Seguida de una rotación de ángulo $\gamma$ en torno a OZ:

            $$(O_u',O_v,OZ) \longrightarrow (OX,OY,OZ)$$

            De forma que esta parametrización consiste en 3 giros:

            $$R(\alpha, \beta , \gamma)=R_{\Vec{Z}(\gamma)}R_{\Vec{v}(\beta)}R_{\Vec{z}(\alpha)}$$

            Usando la propiedad de conjugación, podemos escribir la fórmula en torno al referencial inicial (y no a intermedios como estamos haciendo ahora):

            $$R(\alpha, \beta , \gamma)=R_{\Vec{z}}(\alpha)R_{\Vec{y}}(\beta)R_{\Vec{z}}(\alpha)$$

            \begin{figure}[h!]
                \centering
             \includegraphics[width=0.6\textwidth]{parametrizacion2.png}
                \caption{Representación de las rotaciones que se hacen en la parametrización de ángulos de Euler}
                \label{fig:my_label}
            \end{figure}

            \newpage
            Podemos descomponer cualquier rotación en producto de rotaciones en z y en y.

            $$R_{z}(\uppsi )=\left ( \begin{array}{ccc}
                 cos \uppsi & -sen \uppsi  & 0 \\
                 sen \uppsi & cos \uppsi & 0 \\
                 0 & 0 & 1
            \end{array}\right) \hspace{0.5cm} R_{y}(\uppsi )=\left ( \begin{array}{ccc}
                 cos \uppsi & 0 & sen \uppsi \\
                 0 & 1 & 0 \\
                 sen \uppsi & 0 & cos \uppsi
            \end{array}\right)$$
        \end{itemize}

        \textbf{Ejercicio:} obtener la expresión explícita de la matriz R($\alpha , \beta , \gamma$) y escribe los ángulos $\phi, \theta , \uppsi $ como funciones de $\alpha , \beta , \gamma$.

        \subsection{De SO(3) a SU(2)}

        Recordamos que SU(2) son matrices complejas unitarias de determinante 1. Veamos como se asocian con el grupo que acabamos de estudiar.

        \smallskip
        Asociemos a la rotación $R_{\Vec{n}}(\uppsi)$ el 4-vecotor unitario $u^\mu =(cos \frac{\uppsi}{2}, \Vec{n}sen \frac{\uppsi}{2})$. La transformación:

        $$\uppsi \to \uppsi '= \uppsi + (2n+1) 2\pi$$

        Lleva de $\Vec{u}$ (la parte espacial) a $-\Vec{u}$. Entonces hay una biyección entre $R_{\Vec{n}}(\uppsi)$ y el par ($\Vec{u}$,$-\Vec{u}$), es decir, entre SO(3) y $S^3/\mathds{Z}_2$, la esfera con antípodas identificadas.

        \smallskip
        Veamos que los 4-vectores u están en correspondencia 1 a 1 con los elementos de SU(2). Para ello introducimos las matrices de Pauli $\sigma ^i$ que junto con la identidad forman base del espacio vectorial de matrices hermíticas 2x2 y satisfacen $\sigma _i \sigma _j = \delta _{ij} \mathds{1} + i\epsilon _{ijk}\sigma _k$.

        \smallskip
        Gracias a que se cumple que $(\Vec{n}\cdot \Vec{\sigma})^2=\mathds{1}$ tenemos que $cos \frac{\uppsi}{2} \mathds{1} -isen \frac{\uppsi}{2} \Vec{n} \Vec{\sigma} = e^{-i\frac{\uppsi}{2}\Vec{n}\Vec{\sigma}}$ que es precisamente una matriz unitaria y de determinante unidad.

        Entonces, variando $\uppsi$ en [0,2$\pi $) construimos todo SU(2). Es decir, es el conjunto de matrices unitarias que cumplen:

        $$SU(2)= \left \lbrace U_{\Vec{n}}(\uppsi) = e^{-i\frac{\uppsi}{2}\Vec{n}\Vec{\sigma}} , 0 \leq \uppsi < 2\pi \right \rbrace$$

        \textbf{Ejercicio:} demostrar esta igualdad (la del seno y el coseno).

        \smallskip
        Concluimos que SU(2) es isomorfo a $S^3$ (la 3-esfera) y este es un isomorfismo entre grupos: inferimos el producto en $S^3$ (la esfera vista como grupo) del producto de matrices $U_{\Vec{n}}(\uppsi)$. También deducimos que SO(3) $\simeq$ SU(2)/$Z_2$, es decir que SU(2) es el recubridor universal de SO(3).

        \smallskip
        Demostremos que en efecto a cada matriz de SU(2) se le puede asociar una matriz SO(3) y que el producto de SU(2) se corresponde con el producto de rotaciones SO(3) (tenemos un homomorfismo de grupos).

        \smallskip
        A cada vector $\Vec{x}=(x_1,x_2,x_3) \in \Vec{R}^3$ le asociamos la matriz hermítica:

        $$X=\Vec{x}\Vec{\sigma} = \left ( \begin{array}{cc}
            x_3 & x_1-ix_2  \\
            x_1+ix_2 & -x_3
        \end{array}\right ) \to x_i=\frac{1}{2}Tr(X\sigma _i)$$

        y hacemos actuar SU(2) por conjugación sobre X, me mapeo X a X' tal que $X'=UXU^+$ (en realidad $X\in su(2)$ pertenece al álgebra y las cosas que viven en el álgebra se transforman por conjugación, es decir, en la representación adjunta del grupo).

        Esta transformación me induce una transformación en $\mathds{R}^3$ según:

        $$X'=\Vec{x}'\Vec{\sigma} \to \Vec{x}'=T\Vec{x}$$

        Como el determinante de X' es el mismo que el de X estamos ante una T que es una isometría, es decir, no cambia la norma del vector. Necesitamos ver su determinante para concluir si se trata de una rotación o de una reflexión.

        \smallskip
        Como en el caso trivial U=$\mathds{1}$ , T debe ser también la identidad y se que SU(2) es conexo, la función continua det(T) no puede saltar al valor -1, así que en efecto concluimos que se trata de una rotación en $\mathds{R}^3$.

        \smallskip


        De hecho se puede comprobar que $X'=U_{\Vec{n}}(\uppsi)XU_{\Vec{n}}(\uppsi)^+$ nos devuelve la expresión que vimos antes para las rotaciones pero multiplicada por las matrices de Pauli:

         $$X'=[\delta _{ij} cos \uppsi + n_in_j (1-cos \uppsi) - sen \uppsi \sum _K \epsilon _{ijk} n_k ]\Vec{\sigma}$$

         \subsection{Generadores infinitesimales y álgebra de Lie.}

         \subsubsection{Generadores de SO(3).}

         Las rotaciones de eje fijo $\Vec{n}$ dan lugar a un subgrupo uniparamétrico de SO(3), este subgrupo es isomorfo a SO(2). Asociado a cada uno de estos subgrupos existe un generador $J_\Vec{n}$, hermítico o autoadjunto (generador de las rotaciones en torno al eje $\Vec{n}$). Obtenemos el grupo por exponenciación.

         $$R_\Vec{n}(\uppsi)=e^{-i\uppsi J_\Vec{n}}$$

         Basta tomar una base de generadores en las direcciones cartesianas:

         $$J_k=i\left . \frac{dR_k(\uppsi)}{d\uppsi}\right |_{\uppsi =0}$$

         Que explicitamente nos da:

         $$J_x=\left ( \begin{array}{ccc}
             0 & 0 & 0  \\
             0 & 0 & -i \\
              0 & i & 0
         \end{array}\right) \hspace{0.5cm} J_x=\left ( \begin{array}{ccc}
             0 & 0 & i  \\
             0 & 0 & 0 \\
              -i & 0 & 0
         \end{array}\right) \hspace{0.5cm}J_x=\left ( \begin{array}{ccc}
             0 & -i & 0  \\
             i & 0 & 0 \\
              0 & 0 & 0
         \end{array}\right) $$

         Cualquier rotación será la exponenciación de una combinación lineal de estos generadores. Llamando $\Vec{J}$ al vector de componentes $J_x,J_y,J_z$ tenemos:

         $$R_\Vec{n}(\uppsi) = e^{-i}\uppsi \Vec{n}\cdot \Vec{J}$$

         Con la parametrización de ángulos de Euler, la rotación se puede escribir, también en función de estos generadores como:

         $$R(\alpha ,\beta ,\gamma)=e^{i\alpha J_z}e^{-i\beta J_y}e^{-i\gamma J_z}$$

         Satisfacen la condición de conmutación bien conocida $[J_i,J_j]=i\epsilon _{ijk}J_k$.
         \smallskip

         La forma de Killing no es degenerada y el álgebra es simple, solo tienen un operador de casimir que es (salvo cambio de signo) C=$\Vec{J}^2=J_x^2+J_y^2+J_z^2$, es decir, que conmuta con todos los generadores del grupo (es invariante bajo rotaciones). Se puede diagonalizar simultáneamente los generadores y el invariante casimir.

        \smallskip
    \textbf{Los generadores $S_k$ tienen las mismas propiedades que los operadores de momento angular de mecánica cuántica.}

    \subsubsection{Generadores de SU(2).}

    Toda matriz unitaria y compleja 2x2 puede escribirse como la exponenciación de una H (hermítica y de traza nula para que U tenga las características que se le piden):

    $$U=e^{iH}$$

    El conjunto de estas H forma un espacio vectorial real de dimensión 3, con base formada por las matrices de Pauli (son generadores del álgebra de SU(2)):

    $$H=\sum _{i=1}^3 \eta _k\frac{\sigma _k}{2} \longrightarrow U=e^{\frac{i}{2}\Vec{\eta}\cdot \Vec{\sigma}}$$

    Luego $\sigma /2$ juega el mismo papel que J, de este modo, sus conmutadores serán iguales que para estos: $[\frac{\sigma _i}{2},\frac{\sigma _j}{2}]=i\epsilon _{ijk}\frac{\sigma _k}{2}$. Luego forman el mismo álgebra que los J. El álgebra de SU(2) es la misma que es álgebra de SO(3), son dos representaciones unitarias distintas de un mismo álgebra de Lie.

    \smallskip
    Todas las representaciones de SU(2) son de SO(3) pero habrá alguna de SO(3) que no los sea de SU(2) al ser de dimensión menor.

    \newpage

    \textbf{En resumen:}

    \begin{itemize}
        \item El álgebra de Lie su(2)$\equiv$ so(3) es la generada por tres generadores $J_k$ con ciertas relaciones de conmutación (ya vistos como elementos abstractos del álgebra).

        \item A veces usaremos otra base $\lbrace J_z, J_+, J_- \rbrace$ donde $J_+=J_x+iJ_y$ y $J_-=J_x-iJ_y$. En esta base los conmutadores son [$J_z,J_\pm$]=$\pm J_\pm$ y $[J_+,J_-]=2J_z$. El casimir toma la forma $\Vec{J} ^2=J_z^2 + J_z + J_-J_+$.

        \item Al estar interesados en representaciones unitarias, los generadores $J_k$ se llaman hermíticos $J_\pm ^+=J_\mp$. Se puede comprobar que el casimir conmuta con los nuevos generadores $J_+$ y $J_-$.
    \end{itemize}

          \subsection{Representaciones unitarias irreducibles de SU(2)}

          Ya hemos visto una bidimensional (j=1/2) y una tridimensional (j=1) en mecánica cuántica. Vamos a construirlas todas.

          \smallskip
          Consideremos $J_\pm ,J_z$, como $J_\pm$ y $\Vec{J}^2$ conmutan comparten autovectores y además, al ser hermíticos, sus autovalores son reales.

          Además $\Vec{J}^2$ es semidefinido positivo (es decir tiene autovalores positivos o cero) al ser el cuadrado de un operador hermítico.
          Podemos escribir sus autovalores de la forma j(j+1) con j real y positivo o cero.

          \smallskip
          Llamemos $\ket{j \ m}$ a los autovectores comunes de $\Vec{J}^2$ y $J_z$ tal que:

          $$\Vec{J}^2 \ket{j \ m}= j(j+1)\ket{ j \ m}$$
        $$J_z\ket{j \ m}=m \ket{j \ m}$$

        Con j real positivo y m real. \textbf{Busquemos más propiedades acerca de m.}

        \begin{enumerate}
            \item Actuamos con $J_+$ y $J_-$ sobre el vector $\ket{j \ m}$. Aprovechando la relación $J_\mp J_\pm =\Vec{J}^2- J_z^2 \mp J_z$ podemos hallar el siguiente producto escalar:

            $$\bra{ j \ m} J_\mp J_\pm  \ket{j \ m}= [j(j+1) - m^2 \mp m]\braket{j \ m}{j \ m}$$

            Estamos hallando la norma pues $J_\mp J_\pm =\mathds{1}$ luego esa norma ha de ser positiva:

            $$(j+m)(j-m+1) \geq 0 \hspace{2cm} (j-m)(j+m +1) \geq 0$$

            Luego $-j \leq m \leq j$. También tenemos que $J_+ \ket{j \ j}$ ha de ser nulo pues no hay elementos mayores que este, del mismo modo $J_- \ket{j \ -j}=0$ por el mismo argumento pero por abajo.

            \item Supongamos que m no es ni j ni -j, en este caso se puede escribir lo siguiente:

            $$\Vec{J}^2J_\pm \ket{j \ m} =j(j+1) J_\pm \ket{j \ m}$$
            $$J_z J_\pm \ket{ j \ m}= (m\pm 1)J_\pm \ket{j \ m}$$

            Luego llegamos a la conclusión de que $J_\pm \ket{j \ m}$ es autovector de $\Vec{J}^2$ y $J_z$ con los autovalores que vemos en las relaciones de arriba.

            \item La secuacia de vectores $\ket{j \ m}, \ J_- \ket{j \ m}, \ \ket{J}_-^2 \ket{j \ m}, \ ...$ si no se anulan serán autovectores de los que hemos visto antes con autovalores $m, m-1, m-2, \ ..., m-p$.

            Esta secuencia se anula en el momento en el que p es el último entero tal que $J_-^p \ket{ j \ m} \neq 0$ y debe cumplir que $m-p=-j$. En otras palabras j+m es un entero no negativo.

            \item Actuando análogamente con $J_+$ se llega a la conclusión de que j-m es un entero no negativo.
        \end{enumerate}

        Por tanto j y m son simultáneamente o enteros o semienteros. Los valores que puede tomar son:

        $$j=0,1/2,1,3/2, \ ...$$
        $$m=-j,-j+1,-j+2, \ ... \ , j-1, j$$

        El autovalor m toma 2j+1 valores, esto me marca la dimensión del espacio. Eligiendo $\ket{ j \ j}$ con norma 1 construimos la base ortonormal $\ket{j \ m}$ del espacio vectorial de dimensión 2j+1 ($V_j$) por aplicación reiterada de $J_-$ y obtenemos:

        $$\boxed{J_\pm \ket{j \ m}=\sqrt{j(j+1)-m(m \pm 1)}\ket{ j \ m \pm 1}}$$

        $$\boxed{J_z \ket{j \ m} =m \ket{ j \ m}}$$

        Que es la representación de espín j del álgebra su(2).

        \subsubsection{Representación de espín de SU(2)}

        Bajo la acción de una rotación la matríz $D^j$ de la representación de espín j actúa sobre el espacio vectorial $V_j=lin \lbrace \ket{ j \ m } \rbrace$ del siguiente modo:

        $$D^j(U)\ket{j \ m} = \sum _{m=-j}^j \ket{j \ m '}D^j_{m,m'}(U)$$

        En la parametrización de los ángulos de euler:

        $$D^j_{m,m'}(\alpha , \beta , \gamma)= \bra{j \ m'} D(\alpha , \beta , \gamma ) \ket{j \ m} = e^{-i\alpha m'}d^{j}_{m,m'}(\beta) e^{i\gamma m}$$

        usando la matriz de Wigner para simplificar la notación: $d^j_{m,m'}=\bra{j \ m'} e^{-i\beta J_y} \ket{j \ m}$.

       \textbf{ Hay alguna relación extra que combiene recordar:}

        $$D^j_{m,m'}(\Vec{z},\uppsi)= \bra{j \ m'} e^{-i\uppsi J_z} \ket{j \ m}=e^{-i\uppsi m} \bra{j \ m'} \ket{j \ m}=e^{-i\uppsi m} \delta _{m,m'}$$

        $$D^j_{m,m'}(\Vec{y},  \uppsi
        ) = \bra{j \ m'} e^{-i\uppsi J_y} \ket{j \ m} = d^j_{m,m'}(\uppsi)$$

        $$D^j_{m,m'}(\Vec{x},\uppsi)= \bra{j \ m'} e^{-i\frac{\pi}{2} J_z}e^{i\uppsi J_y} e^{i\frac{\pi}{2}J_z} \ket{j \ m}=e^{i\frac{\pi}{2}(m-m')}d^j_{m,m'}(\uppsi)$$

        \smallskip

        \textbf{Para rotaciones de ángulo $2 \pi$}:

        $$D^j(\Vec{z},2\pi)= (-1)^{2j} \mathds{1}$$

        \smallskip
        Y por conjugación se puede llegar al general:

        $$D^j(\Vec{n},2\pi)=(-1)^{2j}\mathds{1}$$

        \textbf{Conclusión:} si j es entero la matriz que representa una rotación de ángulo $2\pi$ es la unidad pero si fuera semientero sería \textbf{menos} la unidad. Los j enteros tienen una representación par, da lugar a representaciones tensoriales de SO(3). Los j semienteros tienen una representación impar que es la representación espinorial de SO(3). \textbf{Los espinores cambian de signo bajo una rotación de $2\pi$}.


        \subsection{Producto directo de representaciones de SU(2)}

        Consideremos el producto directo de representaciones $j_1$ y $ j_2$ y su descompasición en representaciones irreducibles.

        Usando las bases $\{ \ket{j_1 \ m_1}\}, \{ \ket{j_2 \ m_2} \}$ tenemos que la base del producto directo es $ \ket{j_1 \ m_1} \otimes \ket{j_2 \ m_2} \equiv \ket{ j _1 \ m_1 , j_2 \ m_2}$.

        Sobre la que los generadores infinitesiamles actúan como $\Vec{J}= \Vec{J}^{(1)}\otimes \mathds{1}^{(2)} + \mathds{1}^{(1)}\otimes \Vec{J}^{(2)} =\Vec{J}^{(1)} + \Vec{J}^{(2)}$.

        \smallskip
        Queremos descomponer los vectores $\ket{ j _1 \ m_1 , j_2 \ m_2}$ en la base de autovectores de $\Vec{J}^2=(\Vec{J}^{(1)} + \Vec{J}^{(2)})^2$ y $J_z=J_{1z}+J_{2z}$. Como $\left ( \Vec{J}^{(1)}\right)^2$ y $\left ( \Vec{J}^{(2)}\right)^2$ conmutan entre sí y con $\Vec{J}^2$ y $J_z$ tenemos autovalores comunes a estos 4 operadores que llamamos $\ket{j_1 \ j_2 , J \ M}$ con $j_1$ y $j_2$ fijos.

        \bigskip
        \textbf{¿Qué valores pueden tomar J y M?}\textbf{¿Cuál es la matriz de cambio de base?}

        \smallskip
        Para M tenemos que:

        $$J_z\ket{j_1 \ j_2 , J \ M} = M \ket{j_1 \ j_2 , J \ M}$$

        $$J_z\ket{j_1 \ m_1 , j_2 \ m_2}=(m_1+m_2)\ket{j_1 \ m_1 , j_2 \ m_2}$$

        Se puede tomar complejo conjugado en la segunda ecuación para obtener lo mismo que en la primera, obtenemos que M=$m_1+m_2$.

        \smallskip
        Para J tenemos que, dado que $m_1+m_2 \in (-j_1-j_2,j_1+j_2)$ no hay autovectores con M tal que $|M|>j_1+j_2$, entonces, llamando $n(M)$ al número de vectores que hay para un determinado M podemos hallar la multiplicidad $m_S^{j_1j_2}$, el número de veces que la representación de espín J aparece en la descomposición del producto directo de $j_1$ con $j_2$.

        \smallskip
        Los n(M) vectores de autovalor M de $J_z$ vienen de los $m_S^{j_1j_2}$ vectores para los diferentes valores de J comparibles con M.

        $$n(M)= \sum _{J\geq |M|}m_S^{j_1j_2}$$

        Entonces $m_S^{j_1j_2} = n(J)-n(J+1)$ es 1 si no estamos en J=$J_{max}$ o bien 0 en ese caso. En resumen, los ($2j_2 + 1$)($2j_1+1$) vectores de la base $\ket{j_1 \ m_1,j_2 \ m_2}$ se pueden expresar en términos de vectores $\ket{j_1 \ j_2, J \ M}$ a través de unos coeficientes que los relacionan.

        \subsubsection{Coeficientes de Clebsch-Gordan para SU(2)}

        Se trata de los elementos de la matriz ortonormal del cambio de base $\ket{j_1 \ m_1,j_2 \ m_2}$ a la $\ket{j_1 \ j_2, J \ M}$. Usando la notación del tema 3 tenemos:

        $$C_{j_1 \ m_1 \ j_2 \ m_2 | \ J \ M}=\bra{j_1 \ j_2 , J \ M}\ket{j_1 \ m_1, j_2 \m_2}$$

        Luego pueden escribirse los cambios de base:

        $$\ket{j_1 \ m_1, \ j_2 \ m_2}=\sum _{|j_1-j_2|}^{j_1+j_2} C_{j_1 \ m_1 \ j_2 \ m_2 | \ J \ M} \ket{j_1 \ j_2 , J \ M}$$

        $$\ket{j_1 \ j_2, \ J \ M}=\sum _{-j_1}^{j_1} C_{j_1 \ m_1 \ j_2 \ m_2=M-m_1 | \ J \ M} \ket{j_1 \ m_1 , j_2 \ m_2=M-m_1}$$

        El valor de estos coeficientes de pueden deducir (salvo fase relativa). El convenio usual es que para valor de J se elige $\bra{j_1 \ j_2 , J \ M}\ket{j_1 \ m_1, j_2 \m_2} \in \mathds{R}$.

        El resto de vectores quedan entonces determinados unívocamente por como actúan $J_\pm$ sobre estos estados (todos los coeficientes de C-G son reales).

        \textbf{Ejercicio:} calcular la descomposición y coeficientes de CG de $\frac{1}{2}\otimes \frac{1}{2}$.

        \subsection{Medida invariante de SU(2)}

        Tenemos que si U,V son dos elementos de SU(2) es posible definir una medida invariante por la derecha e invariante por la izquierda además de por inversión. Fijada la normalización, esta medida es única y recibe el nombre de medida de Haar. La forma explícita depende de la parametrización utilizada. En parametrización ángulo-eje:

        $$d_\mu (U)=\frac{1}{2}sen^2(\frac{\uppsi}{2})sen(\theta) d\uppsi d\theta d\phi$$

        Esta normalizada tal que su volumen sea el de una esfera unidad.

        $$V(SU(2))=2\pi ^2$$

        Para SO(3) en el que $\uppsi \in [0,\pi )$ (recorre la mitad del intervalo) tenemos que el volumen $V(SO(3))=\pi ^2$.

        \smallskip
        En ángulos de euler la medida es:

        $$d_\mu (U)=\frac{1}{8}sen \beta d\alpha d\beta d\gamma$$

        donde $\gamma \in [0,4\pi], \ \alpha \in [0,2\pi], \ \beta \in [0,\pi]$.

        \subsection{Ortogonalidad, completitud y caracteres}

        $$(2j+1)\int \frac{d_\mu (U)}{2\pi^2} D^j_{mn}(U)\bar{D}_{m'n'}^{j'}(U)=\delta _{j,j'}\delta _{m,m'}\delta _{n,n'}$$

        $$\sum _{j,m,n} (2j+1)D^j_{mn}(U)\bar{D}_{m'n'}^{j'}(U')=2\pi ^2 \delta (U,U')$$

        donde esa delta es $\delta (U,U')=\int d_\mu (U')\delta (U,U') f(U') =f(U)$.

        Se puede escribir explícitamente en ángulos de euler:

        $$\delta (U,U')=\delta (\alpha - \alpha ')\delta (cos \beta - cos \beta ')\delta (\gamma - \gamma ')$$


        Estas relaciones implican que las $D_{m,n}^j$ forman una base ortogonal y completa del espacio de funciones de cuadrado integrable en SU(2), que es el conocido teorema de Peter-Weyl.

        \smallskip
        Los caracteres de las representaciones de SU(2) son

        $$\mathcal{X}_j(U)=\mathcal{X}_j(\uppsi)=trD^j(\Vec{n},\uppsi)=trD^j(\Vec{z},\uppsi)=\sum _{m=-j}^j e^{im\uppsi}=\frac{sin (\frac{2j+1}{2}\uppsi)}{sin \frac{\uppsi}{2}}$$

        Estos caracteres son los llamados polinomios de Chebyshev de segunda clase de la variable $2cos\frac{\uppsi}{2}$, en particular:

        $$\mathcal{X}_0(\uppsi)=1, \hspace{0.2cm} \mathcal{X}_{1/2}(\uppsi)=2cos \frac{\uppsi}{2}, \hspace{0.2cm} \mathcal{X}_1=1+2cos\uppsi , \ ...$$

        Estos caracteres tienen propiedades de unitariedad y realidad ($\mathcal{X}_i(U^{-1}=\bar{\mathds{X}}_i(U)=\mathcal{X}_i(U)$), paridad y periodicidad ($\mathcal{X}_j(-U)=\mathcal{X}_j(2\pi + \uppsi) =(-1)^{2j}\mathcal{X}_j(U)$), ortogonalidad y completitud (que no la pongo porque me suda los cojones).

        \subsection{Teorema de Wigner-Eckart}

        Si un sistema fisico admite el grupo de simetría SU(2) las transformaciones de simetría implican relaciones entre los observables físicos (operadores) que pertenecen a la misma representación irreducible. En otras palabras, cantidades físicas se corresponden con tensores irreducibles.

        \smallskip
        Sea un conjunto de operadores tensoriales irreducibles que se transforman de acuerdo a la representación de espín j.

        $$D^j(g) Q^{\Tilde{j}}_{\Tilde{m}} D^{j'}(g^{-1})=\sum _{m'}Q^\Tilde{j}_{m'} D^{\Tilde{j}}_{m'\Tilde{m}}(g)$$

        Entonces sus elementos de matriz entre estados físicos (vectores) irreducibles satisfacen el teorema de W-E.

        $$\bra{j' \ m'}Q^{\Tilde{j}}_{\Tilde{m}} \ket{j \ m}=C_{j \ m, \ \Tilde{j} \ \Tilde{m} \ | \ j' \ m'}(j'||\Tilde{Q}'||j)$$

        Sin ningún conocimiento específico de la dinámica del sistema concluimos:

        \begin{enumerate}
            \item Reglas de selección, el elemento de matriz reducida se anula a menos que se cumplan las relglas de selección.


        \end{enumerate}

        \subsection{Aplicación física: isospín}

        Los hadrones forman multipletes (tienen masas muy parecidas pero distintas cargas eléctricas). Si se ignora la interacción electromagnética cada multiplete está formado por estados degenerados y se corresponde con una representación irreducible del grupo de simetría.

        \smallskip
        Aquí el grupo de simetría es SU(2) ya que solo hay un observable cuántico (la carga eléctrica) que distinga los elementos de cada multiplete. El candidato a la simetría es un grupo compacto con un casimir (de rango 1) con representaciones unitarias y finitas.

        \smallskip
        A la cantidad conservada bajo la acción de SU(2) se le llama espín isotópico o isospín. Llamamos $\Vec{I}^2$ al casimir e $I_z$ al generador infinitesimal en la dirección z. Sabemos que dada una representación irreducible de SU(2) de espín I tengo una base de autoestados $\ket{I \ I_z}$ tal que la acción del casimir y el generador es:

        $$\Vec{I}^2 \ket{I \ I_z} = I(I+1)\ket{I \ I_z}$$

        $$I_z \ket{I \ I_z} = I_z \ket{I \ I_z}$$

        Se dice que este isospín es un buen número cuántico dado que se conserva en las interacciones que estudiamos (al conmutar con los generadores de la representación de estas interacciones). $I_z$ se corresponde con las diferentes cargas eléctricas dentro del multiplete.

        \smallskip
        Los nucleones forman un doblete luego viven en la representación de espín 1/2 que es la representación irreducible de dimensión 2. Los piones forman un triplete y por tanto serán la representación irreducible de espín 1 (de dimensión 3).

        $$p=\ket{1/2 \ 1/2} \hspace{1cm} n=\ket{1/2 \ -1/2}$$

        $$\pi^+ = \ket{1 \ 1} \hspace{0.5cm} \pi^0 = \ket{1 \ 0} \hspace{0.5cm} \pi^- = \ket{1 \ -1} $$

        Podemos ahora analizar procesos nucleares a través de las reglas que habíamos desarrollado para esta representación de SU(2). Veamos por ejemplo la desintegración:

        $$N \to N + \pi$$
        $$\ket{1/2 \ -1/2} \to \ket{1/2 \ \pm 1/2} + \ket{1 \ \pm 1,0}$$

        En principio está permitido por las reglas de selección aunque solo para algunos valores de la carga.

        \smallskip
        Análogamente para procesos de scattering del tipo $N+\pi \to N + \pi$ se pueden aplicar estas reglas, por ejemplo sería posible tener:

        $$p + \pi ^+ \to \pi ^+ + p$$

        Un proceso que conserva la carga 3/2. Un proceso menos trivial (de $I_z=$1/2) es:

        $$p + \pi ^0 \to n + \pi ^+$$

        Llamamos T al operador responsable de estas transiciones, se puede deducir a través del teorema de Wigner-Eckart más información de esta simetría.

        \smallskip
        La composición $\frac{1}{2}\otimes 1$ da lugar a la siguiente descomposición de Clebsch-Gordan:

        $$\ket{\left ( \frac{1}{2},1\right) \ \frac{3}{2} \ \frac{3}{2}}=\ket{\frac{1}{2} \ \frac{1}{2} , \ 1 \ 1}$$

        $$\ket{\left ( \frac{1}{2},1\right) \ \frac{3}{2}, \frac{1}{2}}= \frac{1}{\sqrt{3}}\left ( \sqrt{2} \ket{\frac{1}{2} \ \frac{1}{2}, \ 1 \ 0 } + \ket{\frac{1}{2} \ \frac{-1}{2} , \ 1 \ 1}\right)$$

        $$\ket{\left ( \frac{1}{2},1\right) \ \frac{3}{2} \ -\frac{1}{2}}= \frac{1}{\sqrt{3}}\left ( \sqrt{2} \ket{\frac{1}{2} \ \frac{1}{2}, \ 1 \ -1} + \ket{\frac{1}{2} \ \frac{-1}{2} , \ 1 \ 0}\right)$$

        $$\ket{\left ( \frac{1}{2},1\right) \ \frac{3}{2} \ -\frac{3}{2}}=\ket{\frac{1}{2} \ -\frac{1}{2} , \ 1 \ -1}$$

        Y por ortonormalidad sacamos los de isospín 1/2:

        $$\ket{\left ( \frac{1}{2},1\right) \ \frac{1}{2} \ \frac{1}{2}}= \frac{1}{\sqrt{3}}(-\ket{\frac{1}{2} \ \frac{1}{2} ,\ 1 \ 0} + \sqrt{2} \ket{\frac{1}{2} \ -\frac{1}{2} ,\ 1 \ 1})$$

        $$\ket{\left ( \frac{1}{2},1\right) \ \frac{1}{2} \ -\frac{1}{2}}= \frac{1}{\sqrt{3}}(-\sqrt{2}\ket{\frac{1}{2} \ \frac{1}{2} ,\ 1 \ -1} + \ket{\frac{1}{2} \ -\frac{1}{2} ,\ 1 \ 0})$$

        Invirtiendo estas relaciones llegamos a:

        $$\ket{p, \ \pi ^-}=\frac{1}{\sqrt{3}}(\ket{\frac{3}{2},\ -\frac{1}{2}} - \sqrt{2} \ket{\frac{1}{2}, \ -\frac{1}{2}})$$

        $$\ket{n, \ \pi ^0}=\frac{1}{\sqrt{3}}(\sqrt{2}\ket{\frac{3}{2},\ -\frac{1}{2}} -\ket{\frac{1}{2}, \ -\frac{1}{2}})$$

        El teorema de Wigner-Eckart nos dice que esta simetría, al ser isomorfa a SU(2) obliga a una invarianza de isoespín que se traduce en:

        $$\bra{I \ I_z}\mathcal{T}\ket{I' \ I'_z}=\mathcal{T}_I \delta _{I, I'}\delta _{I_z \I_z'}$$

        Entonces los elementos de matriz del operador de transición entre los diferentes estados solo depende del isospín:

        $$\bra{p \ \pi ^+}\mathcal{T}\ket{p \ \pi ^+}=\mathcal{T}_{3/2}$$

        $$\bra{p \ \pi ^-}\mathcal{T}\ket{p \ \pi ^-}$$


        $$\text{Aqui falta algo }\bra{n \ \pi ^0}\mathcal{T}\ket{p \ \pi ^-}$$

        Encontramos que las amplitudes de transición satisfacen

        $$\sqrt{2} \bra{n \ \pi ^0}\mathcal{T}\ket{p \ \pi ^-} + \bra{p \ \pi}\mathcal{T}\ket{p \ \pi}=\bra{p \ \pi ^+} \mathcal{T} \ket{p \pi ^+} = T_{3/2}$$

        Además implica desigualdades triangulares entre los módulos al cuadrado de las amplitudes de transición (o secciones eficaces $\sigma$).

        $$[\sqrt{\sigma (\pi ^-p \to \pi ^- \ p)} - \sqrt{2\sigma (\pi ^- \ p \to \pi ^0 n)}]^2\leq \sigma (\pi ^+ \ p \to \pi ^+ p) \leq [\sqrt{\sigma (\pi ^- p \to \pi ^- \ p)} + \sqrt{2\sigma (\pi ^- \ p \to \pi ^0 n)}]^2$$

        Que se ha verificado experimentalmente. Es más, se ha encontrado que experimentalmente a una energía de 180MeV las secciones eficaces verifican:

        $$\frac{\sigma (\pi ^+ p \to \pi ^+ \ p)}{\sigma (\pi ^-p \to \pi ^- \ p)}=9 \hspace{1cm} \frac{\sigma (\pi ^-p \to \pi ^0 \ p)}{\sigma (\pi ^-p \to \pi ^- \ p)}=2$$

        Que sería lo que obtendríamos de los elementos de matriz $\braket{\mathcal{T}}$ si $\mathcal{T}_{1/2}=0$. Esto indica que a dicha energía el canal $I=\frac{3}{2}$ domina y que existe un estado intermedio $\pi N$ de isospín 3/2 correspondiente a una partícula muy inestable (resonancia) y que se denota por $\Delta$ con cuatro estados de carga $\Delta ^{++}, \Delta ^{+}, \Delta ^{0}$ y $\Delta ^{-}$ cuya contribución domina esta amplitud de scattering.

        \subsection{Rotación de funciones de onda (en representación de posiciones)}

        veamos con más detenimiento las propiedades de transformación inducidas sobre funciones, no solo escalares si no multicomponentes.

        $$[f'(\Vec{x}')=f(\Vec{x}) \leftrightarrow f'(\Vec{x})=f(R^{-1}\Vec{x})]$$

        Aquí vamos a entender estas funciones como las funciones de onda de un sistema cuántico, luego el espacio vectorial es un espacio de Hilbert.

        \smallskip
        Sea el espacio de representación $\mathcal{H}=L^2(\mathds{R}^3,d\Vec{x})$ de vectores (estados) $\ket{\uppsi}=\int d^3\Vec{x} \uppsi (\Vec{x})\ket{\Vec{x}}$

        La transformación $\Vec{x} \to \Vec{x}'=R\Vec{x}$ induce la transformación a los vectores de la base:

        $$\ket{\Vec{x}'}=\ket{R\Vec{x}}$$

        Y a los estados:

        $$\ket{\uppsi '}=U(R)\ket{\uppsi}=\int d^3\Vec{x} \uppsi (\Vec{x})(R^{-1}\Vec{x})\ket{\Vec{x}}$$

        Se ve que para funciones escalares:

        $$\uppsi '(\Vec{x})=\uppsi (R^{-1}\Vec{x})$$


         Veamos como generalizar eso a multicomponentes en los espinores de Pauli (etiquetados por un número cuántico discreto, es decir espín). Los espinores de Pauli son objetos con espín $\pm \frac{1}{2}$. La base no es solo los autovectores del operador posición si no que se le añade la etiqueta del espín.

         $$\{ \ket{\Vec{x} \ \sigma},  \ \sigma = \pm \frac{1}{2}\}$$

        Se transforma según (metiendo la D que transforma el espín):

        $$U(R)\ket{\Vec{x}, \ \sigma} = \ket{R\Vec{x}, \ \lambda} \ D_{\lambda \ \sigma}^{1/2}(R)$$

        Entonces un estado arbitrario se transformará según:

        $$\ket{\uppsi} = \sum _\sigma \int d^3\Vec{x} \uppsi _\sigma (\Vec{x}) \ket{\Vec{x}, \ \sigma} \to \ket{\uppsi '}$$

        $$\ket{\uppsi '}= \sum _\sigma \int d^3\Vec{x} \uppsi _\sigma (\Vec{x})\ket{R\Vec{x}, \ \lambda} D_{\lambda \ \sigma}^{1/2} (R)$$

        Luego la función de onda transformada:

        $$\uppsi '_\lambda (\Vec{x})= D^{1/2}_{\lambda \ \sigma}(R) \uppsi _\sigma (R^{-1}\Vec{x})$$

        Que se puede generalizar a otro espín sin más que cambiar el superínidice por el espín que toque en cada caso.

        \smallskip
        \textbf{Definición:} un conjunto de funciones multicomponente de un vector real se dice que forma una función de onda irreducible o bien un campo irreducible de espín j si esta se transforma bajo rotaciones como hemos expuesto antes. Serán escalares si j=0 y vectoriales si j=1, el espinor de Dirac es la suma de dos de Pauli.


        \newpage

        \section{El grupo de Lorentz}

        \subsection{Propiedades básicas}

        El espacio de Minkowski es un espacio de $\mathds{R}^4$ con una métrica pseudoeuclídea de signatura (+,-,-,-). En una base ortonormal en la que los vectores contravariante $x^mu$ tienen coordenadas $x^\mu =(x^0,x^1,x^2,x^3)=(ct,x,y,z)=(x^0,\Vec{x})$ la métrica es diagonal.

        $$\eta = \left ( \begin{array}{cccc}
             1& 0 & 0 & 0  \\
               0& -1 & 0 & 0  \\
                 0& 0 & -1 & 0  \\
                   0& 0 & 0 & -1
        \end{array}\right)}$$

        La norma al cuadrado del 4-vector se puede construir a través de la métrica como:

        $$x^\mu \cdot x_\mu = x^\mu \eta _{\mu \nu} x_\nu= (x^0)^2 - (\Vec{x})^2$$

        El grupo de Lorentz es el grupo de isometría de esta forma cuadrática (producto escalar), es decir, la norma en el espacio de Minkowski ha de ser invariante bajo transformaciones del grupo de Lorentz.

        $$\Lambda \in O(1,3): \ \ \ x \ \longrightarrow x'=\Lambda x$$

        $$x'\cdot x' =x\cdot x$$

        La condición de invarianza de la métrica puede traducirse en $\Lambda ^\mu _\rho \eta _{\mu \nu} \Lambda ^\mu _\sigma =\eta _{\rho \sigma}$. O bien en forma matricial:

        $$\Lambda ^t \eta \Lambda =\eta$$


        Se puede demostrar que esas matrices $\Lambda$ forman un grupo bajo la multiplicación de matrices 4 $\times$ 4.

        \smallskip
        El grupo de Lorentz es un grupo de Lie lineal de dimensión 6 (6gdl). Es un grupo no conexo formado por 4 conjuntos disjuntos (componentes u hojas). Nos restringiremos a la hoja con determinante positivo y componente $\Lambda _0^0 \geq 0$ que es el llamado subgrupo ortocrono propio (o grupo de Lorentz restringido) $L__+^\uparrow \simeq$ SO(1,3).

        \smallskip
        \textbf{Ejercicio:} demuestra que el subgrupo de Lorentz ortocrono propio no solo es subgrupo si no que es subgrupo normal.

        Otros subgrupos del grupo de Lorentz son el grupo propio $L_+^\uparrow U L_+^\downarrow$ y el subgrupo ortocrono $L_+^\uparrow U L_-^\uparrow$.

        \subsection{Grupo de Lorentz ortocrono propio}

        Todas las matrices $\Lambda \in L_+^\uparrow$ pueden escribirse como el producto de una rotación SO(3) por una transformación de Lorentz pura o boost. Veámoslas:

        \begin{itemize}
            \item \textbf{Rotaciones espaciales (R):} las rotaciones de ángulo $\phi$ respecto al eje $\Vec{n}$:

            $$R_{\Vec{n}}(\phi) = \left( \begin{array}{c|ccc}
                 1& 0 & 0 & 0  \\
                 \hline
                   0& - & - & -  \\
                     0& - & R_\Vec{n}(\phi) & -  \\
                       0& - & - & -
            \end{array}\right)$$

            En esta forma es evidente ver que SO(3) es subgrupo de $L_+^\uparrow$.

            \newpage

            \item \textbf{Transformación de Lorentz pura o boost  ($L$):} las trnasformaciones puras se hacen de velocidad v en la dirección de $\Vec{v}$. Por ejemplo, en la dirección x:

            $$L_1=\left ( \begin{array}{cccc}
                   \gamma & -\beta \gamma & 0 & 0  \\
                    -\beta \gamma & \gamma & 0 & 0  \\
                      0& 0 & 1 & 0  \\
                        0& 0 & 0 & 1
            \end{array}\right)$$

            Utilizando el parámetro de boost $\uppsi$ se puede escribir $\gamma =cosh(\uppsi)$, $\beta = tanh(\uppsi)$ y queda:

            $$L_1=\left ( \begin{array}{cccc}
                   cosh(\uppsi) & -senh(\uppsi) & 0 & 0  \\
                    -senh(\uppsi) & cosh(\uppsi) & 0 & 0  \\
                      0& 0 & 1 & 0  \\
                        0& 0 & 0 & 1
            \end{array}\right)$$

            En general, un boost en una dirección arbitraria podría escribirse de forma parecida aunque es bastante más sencillo obtenerlas a partir de las transformaciones infinitesimales así que no voy a copiar esto.

            \smallskip
            Se puede demostrar que cualquier transformación de Lorentz se puede obtener como una transformación de Lorentz en una dirección concreta, p.e $L_3$ con una rotación $R_{\Vec{n}}(\phi)$.

            \smallskip
            Además, en general, los boosts no forman subgrupo (solo las uniparamétricas entre ellas lo forman, por ejemplo $L_1$ consigo misma, $L_2$ consigo misma, etc).

            \item \textbf{Parametrización:} tenemos que una transformación genérica se puede obtener de forma $\Lambda =LR$, de los 6 parámetros que forman $\Lambda$ se asocian 3 a las rotaciones y otras 3 a los boosts.

            \smallskip
            El espacio de parámetros correspondiente a las transformaciones L se puede tomar como un hiperboloide en el espacio euclídeo de 4 dimensiones (pues la norma de Minkowski es la ecuación de un hiperboloide). Solo tomamos la rama de arriba que es la que incluye el subgrupo ortocrono propio.

            \smallskip
            Es decir, los boosts nos desplazan por la rama del hiperboloide, entre dos puntos de esta. El subgrupo no es compacto (pues es un hiperboloide) y tampoco es simplemente conexo (será doblemente conexo) pues el subconjunto de transformaciones L si que es simplemente conexo (cualquier camino cerrado en el hiperboloide se puede encoger de forma continua a un punto) pero hereda de R, cuyas transformaciones son doblemente conexas su no conexión.

            \item\textbf{Recubridor universal:} por definición, es recubridor universal es el grupo que siendo simplemente conexo sea homeomórfico a $L_+^\uparrow$. Asociamos a cada 4-vector $x=(x^0,x^1,x^2,x^3)$ la matriz hermítica 2 $\times$ 2:

            $$X=\sigma _\mu x^\mu =\left ( \begin{array}{cc}
                 x^0+x^3 & x^1-ix^2 \\
                 x^1+ix^2 & x^0-x^3
            \end{array}\right)$$

            siendo $\sigma _\mu =(\mathds{1},\sigma _i)$ un vector con la matriz identidad y las matrices de pauli que cumple $\sigma ^\mu =\bar{\sigma}_\mu =(\sigma _0 , -\Vec{\sigma})$ y se verifica que Tr($\bar{\sigma}_\mu \sigma _\nu$)=$2\eta _{\mu \nu}$, que permite expresar $x^\mu =\frac{1}{2}Tr(\sigma ^\mu X)$.

            Notemos que DetX=$x\cdot x$. Introduzcamos ahora una matriz compleja $A=\left ( \begin{array}{cc}
                 \alpha & \beta \\
                \gamma & \delta
            \end{array}\right)$ unimodular (det A=1) y transformemos X según:

            $$X'=AXA^+$$

            De modo que x'x'=xx preserve la norma. Esto demuestra que A describe una transformación lineal de $x^\mu$ que deja xx invariante (se corresponde con una transformación de Lorentz).

            \newpage
            La relación entre las matrices A y $\Lambda$ está dada por:

            $$\Lambda _\nu ^\mu =\frac{1}{2}Tr(\bar{\sigma}^mu A \sigma _\nu A^+)$$

            El conjunto de matrices A de componentes complejas y determinante 1 forman el grupo SL(2, $\mathds{C}$). Por cada matriz A hay una transformación de Lorentz $\Lambda$ y por cada transformación de Lorentz hay dos matrices de SL(2, $\mathds{C}$), A y -A. Luego es un homeomorfismo de grupos, en particular, tanto $\mathds{A}$ como $-\mathds{1}$ en SL(2, $\mathds{C}$) se corresponden con la identidad en $L_+^\uparrow$.

            Esta correspondencia entre SL y $L_+^\uparrow$ es un homomorfismo ya que preserva la estructura de grupo.

            $$S=S_1+S_2$$

            $$S_1=-i\frac{\phi}{2}\Vec{n}\cdot \Vec{\sigma}, \ \ \ S_2=-\frac{\uppsi}{2}\Vec{u}\cdot \Vec{\sigma}$$

            La exponenciación de $S_1$ nos da matrices unitarias (rotaciones) y la de $S_2$ nos da matrices hermíticas (boosts).

            \smallskip
            Hay un teorema del álgebra que garantiza que toda matriz de SL(2, $\mathds{C}$) admite descomposición polar (unitaria por hermítica $A=H\cdot U$ de manera única).

            $$H=\sqrt{AA^+} \ \ \ U=H^{-1}A$$

            El homomorfismo entre SL(2, $\mathds{C}$) y el grupo de Lorentz ortocrono propio nos garantiza que cualquier matriz del grupo $\lambda$ se puede descomponer en una rotación pura (unitaria) por un boost puro (hermítica).

         \end{itemize}

        \subsection{Álgebra de Lie de $L^\uparrow_+$}

        Recordamos que las transformaciones infinitesimales eran los generadores del álgebra. Primeramente, el subgrupo de rotaciones espaciales puras generadas por tres elementos independientes es tomado como matrices ahora 4 $\times$ 4 de la forma:

        $$\left (\begin{array}{cccc}
            1 & 0 \\
            0 & R_{i(3\times 3)}(\phi)
        \end{array}\right )$$

    Y tendrá tres generadores de las rotaciones puras, equivalentemente a lo que hemos visto en electromagnetismo:


    $$J_1= i\left (\begin{array}{cccc}
    0 & 0 &  0 &  0\\
    0 & 0 &  0 & 0 \\
     0 & 0 & 0 & -1\\
       0 & 0 & 1 & 0
\end{array} \right )\ \ \ J_2= i\left (\begin{array}{cccc}
    0 & 0 &  0 &  0\\
    0 & 0 &  0 & 1 \\
     0 & 0 & 0 & 0\\
       0 & -1 & 0 & 0
\end{array} \right )\ \ \ J_3= i\left (\begin{array}{cccc}
    0 & 0 &  0 &  0\\
    0 & 0 &  -1 & 0 \\
     0 & 1 & 0 & 0\\
       0 & 0 & 0 & 0
\end{array} \right )$$


    Una rotación infinitesimal de ángulo $\delta \phi$ en torno a un eje dado por $\Vec{n}$ es:

    $$R=\mathds{1}-i\delta \phi \Vec{n}\cdot \Vec{J}$$

    Análogamente para los boosts:

    $$K_1= -i\left (\begin{array}{cccc}
    0 & 1 &  0 &  0\\
    1 & 0 &  0 & 0 \\
     0 & 0 & 0 & 0\\
       0 & 0 & 0 & 0
\end{array} \right ) \ \ \ K_2= -i\left (\begin{array}{cccc}
    0 & 0 &  1 &  0\\
    0 & 0 &  0 & 0 \\
     1 & 0 & 0 & 0\\
       0 & 0 & 0 & 0
\end{array} \right ) \ \ \ K_3= -i\left (\begin{array}{cccc}
    0 & 0 &  0 &  1\\
    0 & 0 &  0 & 0 \\
     0 & 0 & 0 & 0\\
       1 & 0 & 0 & 0
\end{array} \right )$$


        Y por tanto el boost infinitesimal es:

        $$L=\mathds{1}-i\delta \uppsi \Vec{u} \cdot \Vec{K}$$

        \newpage

        Las transformaciones finitas se obtienen, como ya deberíamos saber, de exponenciar el álgebra.

        $$L=e^{i\uppsi \Vec{u} \cdot \Vec{K}}, \ \ \ R=e^{-i \phi \Vec{n} \cdot \Vec{J}} \longrightarrow \Lambda =e^{-i(\phi \Vec{n}\cdot \Vec{J} + \uppsi \Vec{u}\cdot \Vec{K})}$$

        Lo que caracteriza estos generadores son sus conmutadores que son idénticos en todas sus representaciones. El álgebra está definida pues por los siguientes conmutadores:


        $$[J_i,J_j]=i\epsilon _{ijk}J_k$$
        $$[K_i,K_j]=-i\epsilon_{ijk}J_k$$

        Es conveniente definir el tensor antisimétrico $M_{\mu \nu}$ de componentes:

        $$(M_{12},M_{23},M_{31})=(J_3,J_1,J_2)$$
        $$(M_{01},M_{02},M_{03})=(K_1,K_2,K_3)$$

        Y se pueden agrupar las relaciones de conmutación:

        $$[M_{\lambda \rho},M_{\mu \nu}]=-i[\eta_{\lambda \mu}M_{\rho \nu} + 2\eta _{\rho \nu} M_{\lambda \mu}-\eta _{\lambda \nu}M_{\rho \mu} - \eta_{\rho \mu}-\eta _{\rho \mu}M_{\lambda \nu} ]$$

        Y en términos de este tensor se pueden escribir los generadores:

        $$\Lambda = e^{\frac{-i}{2}\omega ^{\mu \nu} M_{\mu \nu}}$$

        Los casimires también se pueden obtener en términos de este tensor:

        $$\Vec{J}^2-\Vec{K}^2 = \frac{1}{2}M^{\mu \nu} M_{\mu \nu}, \ \ \ \frac{1}{2}\epsilon ^{\mu \nu \alpha \beta }M_{\mu \nu}M_{\alpha \beta}=-\Vec{J}\cdot \Vec{K}$$


        \subsection{Representaciones irreducibles de $L^\uparrow _+$}

        Queremos hallar las representaciones irreducibles de $L^\uparrow _+$ de dimensión finita. Dado que este grupo no es compacto, las representaciones no pueden ser unitarias ($\Lambda$ es combinación de unitaria y hermítica luego no es unitaria).

        \smallskip
        Para clasificar las representaciones irreducibles conviene introducir las siguientes combinaciones de generadores que son hermíticas.

        $$\Vec{M}=\frac{\Vec{J+i\Vec{K}}}{2}, \ \ \ \Vec{N}=\frac{\Vec{J}-i\Vec{K}}{2}$$

        Cumplen:

        $$[M_i,M_j]=i\epsilon _{ijk}M_k, \ \ \ [N_i,N_j]=i\epsilon _{ijk}N_k, \ \ \ [M_i,N_j]=0$$

        $\Vec{M}$ y $\Vec{N}$ son copias del álgebra de SU(2) solo que compleja. Dado que se etiquetar las representaciones irreducibles de SU(2) pues lo hemos hecho en el tema anterior, ahora es posible construir las de Lorentz a través de las de SU(2) (mediante estas combinaciones de generadores).

        \smallskip
        Denotando $M_\mathds{C}$ y $N_\mathds{C}$ a la envolvente lineal compleja de $\Vec{M}$ y $\Vec{N}$, tenemos el isomorfismo:

        $$so(1,3)_\mathds{C}\simeq M_\mathds{C}\otimes N_\mathds{C} \equiv su(2)_\mathds{C}\otimes su(2)_\mathds{C} \equiv sl(2,\mathds{C}) \oplus sl(2,\mathds{C})$$

        Concretamente mediante las relaciones de conmutación siguientes:

        $$[\Vec{M}^2,M_i]=0, \ \ \ [\Vec{N}^2,N_i]=0$$

        Podemos etiquetar las representaciones irreducibles de $L^\uparrow _+$ con los autovalores j(j+1) y j'(j'+1) de los operadores de casimir $M^2$ y $N^2$ respectivamente y construir exactamente la misma vaina que en SU(2).

        Restringiéndonos solo a rotaciones puras, SO(3), las representaciones dejan de ser irreducibles y podrán descomponerse en términos de las representaciones irreducibles de SO(3).

        $$D^{(j,j')}(R)=D^{(j)}(R)\otimesD^{(j')}(R)=D^{(j+j')}\oplus ... \oplus D^{|j-j'|}(R)$$

        Dado que para SO(3) hay dos clases de representaciones irreducibles: tensoriales (con j+j' entero) y espinoriales (de j+j' semientero).

        Las representaciones espinoriales se obtienen de su recubridor universal SL(2,$\mathds{C}$) y desde el punto de vista de $L^\uparrow _+$ son bivaluadas (por ser $L^\uparrow _+$ doblemente conexo, sus elementos admiten dos representaciones de SL(2,$\mathds{C}$)).

        \smallskip
        De acuerdo con esto, mientras que campos tensoriales se transforman bien bajo SO(1,3) los campos espinoriales se transforman bajo SL(2, $\mathds{C}$), análogamente a tensores y espinores de SO(3) y SU(2).

        \subsubsection{Representaciones de espín más bajo}

        Hay dos representaciones en la dimensión más baja j=0,j'=1/2 (recordar que la dimensión es (2j+1)(2j'+1)), o bien al contrario aunque no son equivalentes, veamos.

        $$D^{(1/2,0)} \longrightarrow j=1/2, \ \ \ j'=0$$
         $$D^{(0, 1/2)} \longrightarrow j=0, \ \ \ j'=1/2$$

         Entendamos el origen de esta inequivalencia. Mientras que tenemos que $\Lambda (A)=\Lambda (-A)$ es la misma representación esto no ocurre así en general para A y $\bar{A}$ (A y su conjugado). Como en general A no es unitaria no existe una transformación de similaridad que relaciones A con $\bar{A}$ pero esto no pasa para las rotaciones para las que una representación y su conjugada sí que son equivalentes.

         \smallskip
         Por tanto, las matrices A y $\bar{A}$ constituyen de forma general representaciones irreducibles no equivalentes de $L^\uparrow _+$, que actúan en dos espacios vectoriales bidimensionales diferentes.

         Tenemos entonces dos bases no equivalentes y en general dos clases de bi-espinores contravariantes que denotaremos $\xi$ y $\bar{\xi}$. Estos bi-espinores se transforman según:

         $$\xi '=A\xi, \ \ \text{en componentes:} \ \ \ \xi =\left ( \begin{array}{c}
              \xi ^1  \\
             \xi ^2
         \end{array}\right), \ \ \ \xi ^{\alpha '}=A ^{\alpha '}_\beta \xi ^\beta$$

         $$\xi '=\bar{A}\xi, \ \ \text{en componentes:} \ \ \ \xi =\left ( \begin{array}{c}
              \xi ^\dot{1}  \\
             \xi ^\dot{2}
         \end{array}\right), \ \ \ \xi ^{\dot{\alpha} '}=\bar{A} ^{\dot{\alpha} '}_{\dot{\beta}} \xi ^\dot{\beta}$$

        En términos de componentes covariantes $\eta =(\eta _1 , \eta _2), \ \ \bar{\eta}=(\eta _{\dot{1}},\eta _{\dot{2}})$ se puede escribir:

        $$\eta _{\alpha }' = \eta _{\beta} (A^{-1})^\beta _\alpha , \ \ \ \eta '_{\dot{\alpha}}=\eta _{\dot{ \beta}}(\bar{A}^{-1})^{\dot{\beta}}_{\dot{ \alpha}} \leftrightarrow \eta '=\eta A^{-1}, \ \ \ \bar{\eta}'=\bar{\eta}A^+$$

        Y además sabemos que $\eta \xi$ y $ \bar{\eta}\bar{\xi}$ son invariantes.

        \smallskip
        \textbf{Ejercicio:} Una transformación de Lorentz general infinitesimal se puede escribir de la forma $\Lambda _\sigma ^\rho = \eta ^\rho _\sigma + \delta \omega ^\rho _\sigma$ con esta delta real infinitesimal. Hallar:

        \begin{itemize}
            \item Demostrar que $ \delta \omega ^\rho _\sigma$ es antisimétrica.

            \item A partir de esta relación y de $\Lambda =e^{\frac{-1}{2}\omega ^{\mu \nu}M_{\mu \nu}}$ determinar los elementos de matriz de los generadores.

            \item Verifica que se obtiene $M_{ij}=\epsilon _{ijk}J_k$ y $M_{0i}=K_i$.
        \end{itemize}


        \end{document}
